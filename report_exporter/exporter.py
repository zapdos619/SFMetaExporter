# exporter.py - Part 1: Helper Functions and Setup
# Salesforce Report Exporter with DYNAMIC API version detection

import time
import tempfile
import zipfile
import shutil
from pathlib import Path
import requests
from typing import Callable, Optional, List, Dict, Any
import threading


def get_org_api_version(instance_url: str, session_id: str = None) -> str:
    """
    Fetch the latest API version supported by the Salesforce org.
    This endpoint doesn't require authentication.
    
    Args:
        instance_url: The Salesforce instance URL
        session_id: Optional session ID (not required for this call)
        
    Returns:
        Latest API version string (e.g., "v61.0")
    """
    try:
        url = f"{instance_url.rstrip('/')}/services/data/"
        response = requests.get(url, timeout=15)
        
        if response.status_code == 200:
            versions = response.json()
            if versions and len(versions) > 0:
                # Get the latest (last) version
                latest = versions[-1]
                version = latest.get("version", "58.0")
                return f"v{version}"
    except Exception:
        pass
    
    # Fallback to a safe default
    return "v58.0"


def retry_request(
    url: str,
    headers: dict = None,
    cookies: dict = None,
    max_retries: int = 3,
    timeout: int = 60,
    allow_redirects: bool = True,
    backoff_factor: float = 2.0  # ‚úÖ NEW: Configurable backoff
) -> requests.Response:
    """
    Make HTTP GET request with exponential backoff retry logic.
    
    ‚úÖ OPTIMIZED: Better rate limit handling for large exports.
    """
    backoff = 1
    last_error = None
    headers = headers or {}
    cookies = cookies or {}

    for attempt in range(max_retries):
        try:
            response = requests.get(
                url,
                headers=headers,
                cookies=cookies,
                timeout=timeout,
                allow_redirects=allow_redirects
            )

            if response.status_code == 200:
                return response

            # ‚úÖ IMPROVED: Better rate limit handling
            if response.status_code == 429:  # Too Many Requests
                retry_after = response.headers.get('Retry-After')
                if retry_after:
                    try:
                        backoff = int(retry_after)
                    except ValueError:
                        backoff = min(backoff * backoff_factor, 120)  # Cap at 2 minutes
                else:
                    backoff = min(backoff * backoff_factor, 120)
                
                print(f"‚ö†Ô∏è Rate limited, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue

            # ‚úÖ IMPROVED: Better handling of server errors
            if response.status_code in (500, 502, 503, 504):
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Server error {response.status_code}, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue

            response.raise_for_status()

        except requests.Timeout as e:
            last_error = e
            if attempt < max_retries - 1:
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Timeout, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue
            raise
        except requests.RequestException as e:
            last_error = e
            if attempt < max_retries - 1:
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Request error, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue
            raise

    raise Exception(f"Request failed after {max_retries} retries: {last_error}")


def safe_filename(name: str, max_length: int = 100) -> str:
    """Sanitize filename by removing invalid characters."""
    if not name:
        return "unnamed_report"
    safe = "".join(c if c.isalnum() or c in " ._-" else "_" for c in name)
    while "__" in safe:
        safe = safe.replace("__", "_")
    safe = safe.strip("_ ")
    return safe[:max_length] if safe else "unnamed_report"


def clean_csv_footer(csv_content: str) -> str:
    """
    Remove Salesforce's metadata footer from CSV content.
    
    The footer typically contains:
    - Report name
    - Copyright notice
    - Confidential information warning
    - Generated by information
    
    Args:
        csv_content: Raw CSV content from Salesforce
        
    Returns:
        Cleaned CSV content without footer
    """
    lines = csv_content.split('\n')
    
    cleaned_lines = []
    footer_started = False
    blank_line_count = 0
    
    for i, line in enumerate(lines):
        stripped = line.strip()
        
        if not footer_started:
            if not stripped:
                blank_line_count += 1
                cleaned_lines.append(line)
            else:
                blank_line_count = 0
                
                footer_indicators = [
                    'Copyright (c)',
                    'Confidential Information',
                    'Generated By:',
                    '¬© Copyright',
                    'Do Not Distribute'
                ]
                
                if any(indicator in line for indicator in footer_indicators):
                    footer_started = True
                    while cleaned_lines and not cleaned_lines[-1].strip():
                        cleaned_lines.pop()
                elif ',' not in stripped and len(stripped) > 0:
                    next_lines = lines[i+1:min(i+5, len(lines))]
                    next_text = ' '.join(next_lines)
                    if any(indicator in next_text for indicator in footer_indicators):
                        footer_started = True
                        while cleaned_lines and not cleaned_lines[-1].strip():
                            cleaned_lines.pop()
                    else:
                        cleaned_lines.append(line)
                else:
                    cleaned_lines.append(line)
    
    result = '\n'.join(cleaned_lines)
    result = result.rstrip('\n')
    
    return result

# exporter.py - Part 2: SalesforceReportExporter Class
# This continues from Part 1 (helper functions)

class SalesforceReportExporter:
    """
    Export Salesforce reports to CSV files and package them into a ZIP.
    
    Uses TWO methods:
    1. REST API to get list of reports (with metadata)
    2. UI Export URL to download actual CSV (bypasses 2000 row limit)
    
    API version is detected dynamically from the org.
    """

    def __init__(
        self,
        session_id: str,
        instance_url: str,
        api_version: str = None,
        progress_callback: Optional[Callable[[int, int], None]] = None
    ):
        self.session_id = session_id
        self.instance_url = instance_url.rstrip('/')
        self.progress_callback = progress_callback
        
        # Get API version dynamically if not provided
        if api_version:
            self.api_version = api_version if api_version.startswith('v') else f"v{api_version}"
        else:
            self.api_version = get_org_api_version(self.instance_url)
        
        # Build endpoints with dynamic version
        self.reports_list_endpoint = f"/services/data/{self.api_version}/analytics/reports"
        self.folders_list_endpoint = f"/services/data/{self.api_version}/folders"
        
        # Headers for REST API calls (list reports)
        self.api_headers = {
            "Authorization": f"Bearer {self.session_id}",
            "Accept": "application/json"
        }
        
        # Cookies for UI export (CSV download)
        self.export_cookies = {
            "sid": self.session_id
        }
    
    def _query_with_pagination(
        self,
        base_query: str,
        batch_size: int = 2000,
        progress_callback: Optional[Callable[[int, int], None]] = None,
        cancel_event: Optional[Any] = None  # ‚úÖ NEW: Cancellation support
    ) -> List[Dict[str, Any]]:
        """
        Execute SOQL query with automatic pagination.
        Handles Salesforce's 2,000 row limit per query.
        
        ‚úÖ OPTIMIZED: Cancellation support + better timeout handling for large queries.
        
        Args:
            base_query: Base SOQL query (without LIMIT/OFFSET)
            batch_size: Records per batch (default 2000, Salesforce limit)
            progress_callback: Optional callback(fetched, estimated_total)
            cancel_event: Optional threading.Event to check for cancellation
            
        Returns:
            List of all records combined from all pages
        """
        all_records = []
        offset = 0
        has_more = True
        estimated_total = None
        consecutive_errors = 0  # ‚úÖ NEW: Track consecutive errors
        max_consecutive_errors = 3  # ‚úÖ NEW: Fail after 3 consecutive errors
        
        while has_more:
            # ‚úÖ NEW: Check for cancellation
            if cancel_event and cancel_event.is_set():
                print(f"‚ö†Ô∏è Query cancelled at offset {offset}")
                break
            
            # Build paginated query
            paginated_query = f"{base_query} LIMIT {batch_size} OFFSET {offset}"
            
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": paginated_query}
            
            try:
                # ‚úÖ IMPROVED: Longer timeout for large queries
                timeout = 90 if len(all_records) > 5000 else 60
                
                response = requests.get(
                    query_url,
                    headers=self.api_headers,
                    params=params,
                    timeout=timeout
                )
                response.raise_for_status()
                
                data = response.json()
                records = data.get("records", [])
                
                if not records:
                    has_more = False
                    break
                
                all_records.extend(records)
                offset += len(records)
                
                # ‚úÖ RESET: Reset error counter on success
                consecutive_errors = 0
                
                # Update progress if callback provided
                if progress_callback and estimated_total is None:
                    # Estimate total based on first batch
                    if len(records) == batch_size:
                        estimated_total = batch_size * 10  # Rough estimate
                    else:
                        estimated_total = len(records)
                
                if progress_callback:
                    progress_callback(len(all_records), estimated_total or len(all_records))
                
                # If we got fewer records than batch_size, we're done
                if len(records) < batch_size:
                    has_more = False
                
                # ‚úÖ NEW: Small delay to avoid rate limiting on large queries
                if has_more and len(all_records) > 0 and len(all_records) % 10000 == 0:
                    print(f"üìä Fetched {len(all_records)} records, brief pause to avoid rate limits...")
                    time.sleep(1)
                
            except requests.Timeout as e:
                consecutive_errors += 1
                print(f"‚ö†Ô∏è Query timeout at offset {offset} (attempt {consecutive_errors}/{max_consecutive_errors})")
                
                if consecutive_errors >= max_consecutive_errors:
                    print(f"‚ùå Too many consecutive errors, stopping pagination at {len(all_records)} records")
                    has_more = False
                else:
                    # Wait before retry
                    time.sleep(2 * consecutive_errors)
                    continue
                    
            except requests.RequestException as e:
                consecutive_errors += 1
                print(f"‚ö†Ô∏è Query error at offset {offset}: {str(e)[:100]} (attempt {consecutive_errors}/{max_consecutive_errors})")
                
                if consecutive_errors >= max_consecutive_errors:
                    print(f"‚ùå Too many consecutive errors, stopping pagination at {len(all_records)} records")
                    has_more = False
                else:
                    # Wait before retry
                    time.sleep(2 * consecutive_errors)
                    continue
        
        return all_records

    def list_all_report_folders(self) -> List[Dict[str, Any]]:
        """
        Fetch list of all report folders in the org that the user has access to.
        Returns list of folder metadata (id, name, type, etc.)
        Filters out system/automated folders.
        """
        try:
            # Query for Report folders that user has access to
            query = """
                SELECT Id, Name, Type, DeveloperName, AccessType 
                FROM Folder 
                WHERE Type = 'Report' 
                ORDER BY Name
            """
            
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": query}
            
            response = requests.get(
                query_url, 
                headers=self.api_headers, 
                params=params, 
                timeout=30
            )
            response.raise_for_status()
            
            data = response.json()
            folders = data.get("records", [])
            
            # Clean up the response - convert to simple dict
            cleaned_folders = []
            for folder in folders:
                cleaned_folders.append({
                    "id": folder.get("Id"),
                    "name": folder.get("Name"),
                    "type": folder.get("Type"),
                    "developerName": folder.get("DeveloperName"),
                    "accessType": folder.get("AccessType")
                })
            
            return cleaned_folders
            
        except Exception as e:
            raise Exception(f"Failed to fetch report folders: {str(e)}")

    def list_all_reports(self, folder_id: str = None) -> List[Dict[str, Any]]:
        """
        Fetch list of all available reports using REST API or SOQL query.
        
        Args:
            folder_id: Optional folder ID to filter reports. If None, returns all reports.
            
        Returns:
            List of report metadata (id, name, format, folder, etc.)
        """
        # If folder_id is specified, use SOQL query for accurate filtering
        if folder_id:
            return self._list_reports_by_soql(folder_id)
        
        # Otherwise use the standard REST API endpoint
        url = f"{self.instance_url}{self.reports_list_endpoint}"
        response = retry_request(url, headers=self.api_headers, timeout=60)
        
        data = response.json()
        
        if isinstance(data, list):
            reports = data
        elif isinstance(data, dict):
            reports = data.get("reports", data.get("records", []))
        else:
            reports = []
        
        return reports


    def _list_reports_by_soql(self, folder_id: str) -> List[Dict[str, Any]]:
        """
        Use SOQL query with pagination to get reports by folder ID.
        Now handles unlimited reports per folder.
        
        Args:
            folder_id: The Salesforce folder ID to query
            
        Returns:
            List of report metadata in the same format as list_reports()
        """
        try:
            # SOQL query to get reports in specific folder
            base_query = f"""
                SELECT Id, Name, DeveloperName, FolderName, Format, CreatedDate, LastModifiedDate
                FROM Report 
                WHERE OwnerId = '{folder_id}'
                ORDER BY Name
            """
            
            # Use pagination helper
            records = self._query_with_pagination(base_query.strip())
            
            # Convert SOQL results to match REST API format
            reports = []
            for record in records:
                reports.append({
                    "id": record.get("Id"),
                    "name": record.get("Name"),
                    "developerName": record.get("DeveloperName"),
                    "folderName": record.get("FolderName"),
                    "reportFormat": record.get("Format", "TABULAR"),
                    "lastModifiedDate": record.get("LastModifiedDate"),
                    "createdDate": record.get("CreatedDate")
                })
            
            return reports
            
        except Exception as e:
            print(f"Error querying reports by folder: {str(e)}")
            return []


    def search_by_keyword(self, keyword: str, cancel_event=None) -> Dict[str, Any]:
        """
        Search folders AND reports by keyword, then organize results.
        
        ‚úÖ NEW: Creates a virtual "Unified Public Folder" for orphaned reports
        that match the search but whose folder wasn't found.
        """
        try:
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # Escape keyword for SOQL (prevent injection)
            keyword_escaped = keyword.replace("'", "\\'").replace("%", "\\%")
            
            # ===== STEP 1: Search folders by name =====
            print(f"üîç Step 1: Searching folders matching '{keyword}'...")
            
            folders_query = f"""
                SELECT Id, Name, Type, DeveloperName, AccessType 
                FROM Folder 
                WHERE Type = 'Report' 
                AND (
                    Name LIKE '%{keyword_escaped}%'
                    OR DeveloperName LIKE '%{keyword_escaped}%'
                )
                ORDER BY Name
            """
            
            matching_folders = self._execute_soql_query(folders_query)
            folder_ids_from_name_match = {f.get("Id") for f in matching_folders}
            
            print(f"‚úÖ Found {len(matching_folders)} folders matching keyword")
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 2: Search reports by name (WITH PAGINATION) =====
            print(f"üîç Step 2: Searching reports matching '{keyword}'...")
            
            reports_query = f"""
                SELECT Id, Name, DeveloperName, FolderName, Format, 
                    CreatedDate, LastModifiedDate, OwnerId
                FROM Report 
                WHERE Name LIKE '%{keyword_escaped}%' OR FolderName LIKE '%{keyword_escaped}%'
                ORDER BY Name
            """
            
            # ‚úÖ OPTIMIZED: Use pagination with cancellation support
            matching_reports = self._query_with_pagination(
                reports_query.strip(),
                batch_size=2000,
                cancel_event=cancel_event
            )
            
            print(f"‚úÖ Found {len(matching_reports)} reports matching keyword")
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 3: Get folder IDs from matching reports =====
            folder_ids_from_reports = {r.get("OwnerId") for r in matching_reports if r.get("OwnerId")}
            
            # ===== STEP 4: Fetch folders that contain matching reports =====
            additional_folder_ids = folder_ids_from_reports - folder_ids_from_name_match
            
            additional_folders = []
            if additional_folder_ids:
                print(f"üîç Step 3: Fetching {len(additional_folder_ids)} additional folders...")
                
                # ‚úÖ OPTIMIZED: Process in smaller chunks to avoid query string limits
                folder_ids_list = list(additional_folder_ids)
                chunk_size = 50  # ‚úÖ REDUCED: Smaller chunks for stability
                
                for i in range(0, len(folder_ids_list), chunk_size):
                    # Check cancellation
                    if cancel_event and cancel_event.is_set():
                        return {"folders": [], "reports_by_folder": {}}
                    
                    chunk = folder_ids_list[i:i + chunk_size]
                    ids_str = ",".join([f"'{fid}'" for fid in chunk])
                    
                    folders_query = f"""
                        SELECT Id, Name, Type, DeveloperName, AccessType 
                        FROM Folder 
                        WHERE Id IN ({ids_str})
                    """
                    
                    try:
                        chunk_folders = self._execute_soql_query(folders_query)
                        additional_folders.extend(chunk_folders)
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error fetching folder chunk: {str(e)[:100]}")
                        # Continue with other chunks
                        continue
            
            print(f"‚úÖ Fetched {len(additional_folders)} additional folders")
            
            # ===== STEP 5: Identify orphaned reports =====
            # These are reports that matched the search but whose OwnerId
            # doesn't correspond to any folder we successfully fetched
            
            all_fetched_folder_ids = folder_ids_from_name_match | {f.get("Id") for f in additional_folders}
            
            orphaned_reports = []
            valid_reports = []
            
            for report in matching_reports:
                owner_id = report.get("OwnerId")
                
                if not owner_id or owner_id not in all_fetched_folder_ids:
                    # This report's folder wasn't found - it's orphaned
                    orphaned_reports.append(report)
                else:
                    # This report has a valid folder
                    valid_reports.append(report)
            
            print(f"üìä Report classification:")
            print(f"  ‚úÖ Valid reports (with folders): {len(valid_reports)}")
            print(f"  ‚ö†Ô∏è Orphaned reports (no folder found): {len(orphaned_reports)}")
            
            # ===== STEP 5B: Create virtual "Unified Public Folder" if needed =====
            virtual_folder_id = None
            virtual_folder = None
            
            if orphaned_reports:
                # Create a virtual folder to hold orphaned reports
                virtual_folder_id = "VIRTUAL_UNIFIED_PUBLIC_FOLDER"
                
                virtual_folder = {
                    "Id": virtual_folder_id,
                    "Name": "üìÅ Unified Public Folder (Search Results)",
                    "Type": "Report",
                    "DeveloperName": "UnifiedPublicFolder",
                    "AccessType": "Public"
                }
                
                print(f"üÜï Created virtual folder for {len(orphaned_reports)} orphaned reports")
            
            # ===== STEP 6: Combine all folders (virtual first, then real folders) =====
            all_folders = []
            
            # ‚úÖ Add virtual folder at TOP if it exists
            if virtual_folder:
                all_folders.append(virtual_folder)
            
            # Add real folders (name-matched + additional)
            all_folders.extend(matching_folders)
            all_folders.extend(additional_folders)
            
            # ===== STEP 7: Group reports by folder (MEMORY EFFICIENT) =====
            print(f"üìä Grouping reports by folder...")
            
            reports_by_folder = {}
            
            # ‚úÖ Add orphaned reports to virtual folder FIRST
            if virtual_folder_id and orphaned_reports:
                reports_by_folder[virtual_folder_id] = []
                
                for report in orphaned_reports:
                    reports_by_folder[virtual_folder_id].append({
                        "id": report.get("Id"),
                        "name": report.get("Name"),
                        "developerName": report.get("DeveloperName"),
                        "folderName": report.get("FolderName") or "Unified Public Folder",
                        "reportFormat": report.get("Format", "TABULAR"),
                        "lastModifiedDate": report.get("LastModifiedDate"),
                        "createdDate": report.get("CreatedDate")
                    })
                
                print(f"  ‚úÖ Virtual folder: {len(orphaned_reports)} orphaned reports")
            
            # Group valid reports by their actual folders
            for report in valid_reports:
                folder_id = report.get("OwnerId")
                if folder_id:
                    if folder_id not in reports_by_folder:
                        reports_by_folder[folder_id] = []
                    
                    reports_by_folder[folder_id].append({
                        "id": report.get("Id"),
                        "name": report.get("Name"),
                        "developerName": report.get("DeveloperName"),
                        "folderName": report.get("FolderName"),
                        "reportFormat": report.get("Format", "TABULAR"),
                        "lastModifiedDate": report.get("LastModifiedDate"),
                        "createdDate": report.get("CreatedDate")
                    })
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 8: For folders matched by name, get ALL their reports (IN CHUNKS) =====
            print(f"üîç Step 4: Fetching all reports from {len(matching_folders)} matched folders...")
            
            for idx, folder in enumerate(matching_folders):
                folder_id = folder.get("Id")
                
                # Check cancellation
                if cancel_event and cancel_event.is_set():
                    return {"folders": [], "reports_by_folder": {}}
                
                # If this folder doesn't have any reports yet (from keyword search),
                # fetch ALL reports in this folder
                if folder_id not in reports_by_folder:
                    reports_query = f"""
                        SELECT Id, Name, DeveloperName, FolderName, Format, 
                            CreatedDate, LastModifiedDate
                        FROM Report 
                        WHERE OwnerId = '{folder_id}'
                        ORDER BY Name
                    """
                    
                    try:
                        # ‚úÖ OPTIMIZED: Use pagination with cancellation
                        folder_reports = self._query_with_pagination(
                            reports_query.strip(),
                            batch_size=2000,
                            cancel_event=cancel_event
                        )
                        
                        if folder_reports:
                            reports_by_folder[folder_id] = [
                                {
                                    "id": r.get("Id"),
                                    "name": r.get("Name"),
                                    "developerName": r.get("DeveloperName"),
                                    "folderName": r.get("FolderName"),
                                    "reportFormat": r.get("Format", "TABULAR"),
                                    "lastModifiedDate": r.get("LastModifiedDate"),
                                    "createdDate": r.get("CreatedDate")
                                }
                                for r in folder_reports
                            ]
                            
                            print(f"  ‚úÖ Folder {idx + 1}/{len(matching_folders)}: {len(folder_reports)} reports")
                        
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è Error fetching reports from folder {folder.get('Name')}: {str(e)[:100]}")
                        # Continue with other folders
                        continue
            
            # ===== STEP 9: Clean up folder metadata =====
            cleaned_folders = []
            for folder in all_folders:
                cleaned_folders.append({
                    "id": folder.get("Id"),
                    "name": folder.get("Name"),
                    "type": folder.get("Type"),
                    "developerName": folder.get("DeveloperName"),
                    "accessType": folder.get("AccessType")
                })
            
            # ===== FINAL: Calculate statistics =====
            total_reports = sum(len(reports) for reports in reports_by_folder.values())
            print(f"‚úÖ Search complete: {len(cleaned_folders)} folders, {total_reports} total reports")
            
            # ‚úÖ Log virtual folder stats if present
            if virtual_folder_id and virtual_folder_id in reports_by_folder:
                virtual_count = len(reports_by_folder[virtual_folder_id])
                print(f"  üìÅ Virtual folder contains: {virtual_count} orphaned reports")
            
            return {
                "folders": cleaned_folders,
                "reports_by_folder": reports_by_folder
            }
            
        except Exception as e:
            print(f"‚ùå Search error: {str(e)}")
            raise Exception(f"Search failed: {str(e)}")




  
    def _execute_soql_query(self, query: str) -> List[Dict]:
        """
        Helper method to execute a SOQL query and return records.
        
        Args:
            query: SOQL query string
            
        Returns:
            List of record dictionaries
            
        Raises:
            Exception: If query fails
        """
        try:
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": query.strip()}
            
            response = requests.get(
                query_url,
                headers=self.api_headers,
                params=params,
                timeout=30
            )
            response.raise_for_status()
            
            data = response.json()
            return data.get("records", [])
            
        except requests.RequestException as e:
            raise Exception(f"SOQL query failed: {str(e)}")


    def export_report_csv(self, report_id: str, timeout: int = 120) -> str:
        """
        Export a single report as CSV using the UI export URL method.
        
        ‚úÖ IMPROVED: Configurable timeout (default 120s, can be increased for large reports).
        
        This is the "screen scraping" approach that:
        - Bypasses the 2000 row API limit
        - Returns actual CSV content
        - Works with Lightning and Classic
        - Automatically removes Salesforce metadata footer
        
        Args:
            report_id: Salesforce report ID
            timeout: Request timeout in seconds (default 120)
        """
        # Build the export URL - mimics clicking "Export" in the UI
        export_url = (
            f"{self.instance_url}/{report_id}"
            f"?isdtp=p1&export=1&enc=UTF-8&xf=csv"
        )
        
        # ‚úÖ IMPROVED: Use retry_request with configurable timeout
        response = retry_request(
            export_url,
            cookies=self.export_cookies,
            timeout=timeout,
            allow_redirects=True,
            max_retries=3  # ‚úÖ NEW: Explicitly set retries
        )
        
        content = response.text
        
        # Check if we got HTML instead of CSV
        if content.strip().startswith('<!DOCTYPE') or content.strip().startswith('<html'):
            if 'login.salesforce.com' in content or 'ec=302' in content:
                raise Exception("Session expired or invalid. Please re-login.")
            elif 'You do not have access' in content:
                raise Exception("Access denied to this report.")
            else:
                raise Exception("Received HTML instead of CSV. Report may not be exportable.")
        
        # Clean the CSV content (remove footer)
        cleaned_content = clean_csv_footer(content)
        
        return cleaned_content


    def export_report_excel_native(self, report_id: str, timeout: int = 180) -> bytes:
        """
        FALLBACK METHOD: Export report as XLSX by downloading CSV and converting.
        
        This is used when:
        - Analytics API fails
        - Report type is unsupported for formatting
        - User prefers simple CSV‚ÜíXLSX conversion
        
        ‚ö†Ô∏è Limitations:
        - No groupings preserved
        - No indentation
        - No summary rows
        - Basic header formatting only
        
        ‚úÖ Advantages:
        - Always works (CSV export is most reliable)
        - Faster than Analytics API
        - Good for simple tabular reports
        """
        try:
            from openpyxl import Workbook
            from openpyxl.styles import Font, PatternFill, Alignment
            import csv
            from io import StringIO, BytesIO
            
            print(f"üìÑ Using CSV‚ÜíXLSX fallback for report: {report_id}")
            
            # Step 1: Download as CSV
            csv_content = self.export_report_csv(report_id, timeout=timeout)
            
            if not csv_content or len(csv_content.strip()) == 0:
                raise Exception("Empty CSV received from Salesforce")
            
            # Step 2: Parse CSV
            csv_reader = csv.reader(StringIO(csv_content))
            
            # Step 3: Create Excel workbook
            wb = Workbook()
            ws = wb.active
            ws.title = "Report"
            
            # Step 4: Write CSV data to Excel
            for row_idx, row in enumerate(csv_reader, start=1):
                for col_idx, value in enumerate(row, start=1):
                    ws.cell(row=row_idx, column=col_idx, value=value)
            
            # Step 5: Format header row
            if ws.max_row > 0:
                for cell in ws[1]:
                    cell.font = Font(bold=True, color="FFFFFF")
                    cell.fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                
                # Auto-adjust column widths
                for column in ws.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    
                    for cell in column:
                        try:
                            if cell.value:
                                max_length = max(max_length, len(str(cell.value)))
                        except:
                            pass
                    
                    adjusted_width = min(max_length + 2, 50)
                    ws.column_dimensions[column_letter].width = adjusted_width
                
                # Freeze header row
                ws.freeze_panes = "A2"
            
            # Step 6: Save to bytes
            excel_buffer = BytesIO()
            wb.save(excel_buffer)
            excel_bytes = excel_buffer.getvalue()
            
            print(f"‚úÖ CSV‚ÜíXLSX conversion complete: {len(excel_bytes)} bytes")
            return excel_bytes
            
        except ImportError:
            raise Exception("openpyxl required. Install: pip install openpyxl")
        except Exception as e:
            raise Exception(f"CSV‚ÜíXLSX conversion failed: {str(e)}")


    def export_report_with_formatting(self, report_id: str, timeout: int = 180) -> bytes:
        """
        Export report with full Salesforce formatting using Analytics API.
        
        This preserves:
        - Groupings and indentation
        - Summary rows (subtotals, grand totals)
        - Column formatting
        - Report structure
        
        Returns proper XLSX with all formatting intact.
        """
        try:
            from openpyxl import Workbook
            from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
            from io import BytesIO
            
            # ‚úÖ Step 1: Get report metadata and data using Analytics API
            print(f"üìä Fetching report data with metadata for: {report_id}")
            
            # Run the report to get results
            run_url = f"{self.instance_url}/services/data/{self.api_version}/analytics/reports/{report_id}"
            
            response = retry_request(
                run_url,
                headers=self.api_headers,
                timeout=timeout,
                max_retries=3
            )
            
            report_data = response.json()
            
            # ‚úÖ Step 2: Parse report structure
            report_metadata = report_data.get('reportMetadata', {})
            report_type = report_metadata.get('reportFormat', 'TABULAR')
            
            # Get groupings info
            groupings_down = report_metadata.get('groupingsDown', [])
            groupings_across = report_metadata.get('groupingsAcross', [])
            
            # Get column info
            detail_columns = report_metadata.get('detailColumns', [])
            
            # Get the actual data
            fact_map = report_data.get('factMap', {})
            groupings_info = report_data.get('groupingsDown', {}).get('groupings', [])
            
            # ‚úÖ Step 3: Create Excel workbook
            wb = Workbook()
            ws = wb.active
            ws.title = "Report"
            
            # ‚úÖ Step 4: Write data based on report type
            if report_type == 'TABULAR':
                self._write_tabular_report(ws, report_data, detail_columns)
            elif report_type == 'SUMMARY':
                self._write_summary_report(ws, report_data, groupings_down, detail_columns)
            elif report_type == 'MATRIX':
                self._write_matrix_report(ws, report_data, groupings_down, groupings_across)
            else:
                # Fallback to simple format
                self._write_tabular_report(ws, report_data, detail_columns)
            
            # ‚úÖ Step 5: Save to bytes
            excel_buffer = BytesIO()
            wb.save(excel_buffer)
            excel_bytes = excel_buffer.getvalue()
            
            print(f"‚úÖ Created formatted Excel: {len(excel_bytes)} bytes")
            return excel_bytes
            
        except ImportError:
            raise Exception("openpyxl required. Install: pip install openpyxl")
        except Exception as e:
            raise Exception(f"Formatted Excel export failed: {str(e)}")


    def _write_tabular_report(self, ws, report_data, detail_columns):
        """Write tabular report (no groupings)"""
        from openpyxl.styles import Font, PatternFill, Alignment
        
        # Get column labels
        report_extended_metadata = report_data.get('reportExtendedMetadata', {})
        detail_column_info = report_extended_metadata.get('detailColumnInfo', {})
        
        # ‚úÖ Write headers
        headers = []
        for col in detail_columns:
            col_info = detail_column_info.get(col, {})
            label = col_info.get('label', col)
            headers.append(label)
        
        for col_idx, header in enumerate(headers, start=1):
            cell = ws.cell(row=1, column=col_idx, value=header)
            cell.font = Font(bold=True, color="FFFFFF")
            cell.fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            cell.alignment = Alignment(horizontal="center", vertical="center")
        
        # ‚úÖ Write data rows
        fact_map = report_data.get('factMap', {})
        rows_data = fact_map.get('T!T', {}).get('rows', [])
        
        for row_idx, row_data in enumerate(rows_data, start=2):
            data_cells = row_data.get('dataCells', [])
            for col_idx, cell_data in enumerate(data_cells, start=1):
                value = cell_data.get('label', '')
                ws.cell(row=row_idx, column=col_idx, value=value)
        
        # Auto-adjust columns
        self._auto_adjust_columns(ws)
        ws.freeze_panes = "A2"


    def _write_summary_report(self, ws, report_data, groupings_down, detail_columns):
        """Write summary report with groupings and indentation"""
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        
        # Get metadata
        report_extended_metadata = report_data.get('reportExtendedMetadata', {})
        detail_column_info = report_extended_metadata.get('detailColumnInfo', {})
        aggregate_column_info = report_extended_metadata.get('aggregateColumnInfo', {})
        grouping_column_info = report_extended_metadata.get('groupingColumnInfo', {})
        
        current_row = 1
        
        # ‚úÖ Write headers
        headers = []
        
        # Add grouping column headers
        for grouping in groupings_down:
            col_name = grouping.get('name')
            col_info = grouping_column_info.get(col_name, {})
            label = col_info.get('label', col_name)
            headers.append(label)
        
        # Add aggregate column headers
        aggregate_columns = report_data.get('reportMetadata', {}).get('aggregates', [])
        for agg_col in aggregate_columns:
            col_info = aggregate_column_info.get(agg_col, {})
            label = col_info.get('label', agg_col)
            headers.append(label)
        
        for col_idx, header in enumerate(headers, start=1):
            cell = ws.cell(row=current_row, column=col_idx, value=header)
            cell.font = Font(bold=True, color="FFFFFF")
            cell.fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            cell.alignment = Alignment(horizontal="center", vertical="center")
        
        current_row += 1
        
        # ‚úÖ Write grouped data with indentation
        groupings_data = report_data.get('groupingsDown', {}).get('groupings', [])
        
        def write_grouping_level(groupings, level=0, parent_row=current_row):
            nonlocal current_row
            
            for grouping in groupings:
                # Group header row
                label = grouping.get('label', '')
                key = grouping.get('key', '')
                
                # ‚úÖ Add indentation based on level
                indent = "  " * level
                ws.cell(row=current_row, column=1, value=f"{indent}{label}")
                
                # ‚úÖ Make group headers bold
                ws.cell(row=current_row, column=1).font = Font(bold=True)
                
                # Add aggregates for this group
                fact_key = key
                fact_data = report_data.get('factMap', {}).get(fact_key, {})
                aggregates = fact_data.get('aggregates', [])
                
                for col_idx, agg_value in enumerate(aggregates, start=2):
                    value = agg_value.get('label', '')
                    ws.cell(row=current_row, column=col_idx, value=value)
                
                current_row += 1
                
                # ‚úÖ Recursively write sub-groupings
                sub_groupings = grouping.get('groupings', [])
                if sub_groupings:
                    write_grouping_level(sub_groupings, level + 1, current_row)
        
        write_grouping_level(groupings_data)
        
        # ‚úÖ Write grand total
        grand_total_fact = report_data.get('factMap', {}).get('T!T', {})
        if grand_total_fact:
            ws.cell(row=current_row, column=1, value="Grand Total")
            ws.cell(row=current_row, column=1).font = Font(bold=True)
            
            aggregates = grand_total_fact.get('aggregates', [])
            for col_idx, agg_value in enumerate(aggregates, start=2):
                value = agg_value.get('label', '')
                cell = ws.cell(row=current_row, column=col_idx, value=value)
                cell.font = Font(bold=True)
        
        self._auto_adjust_columns(ws)
        ws.freeze_panes = "A2"


    def _write_matrix_report(self, ws, report_data, groupings_down, groupings_across):
        """Write matrix report (crosstab format)"""
        # Matrix reports are complex - simplified version here
        # You can expand this based on your needs
        ws.cell(row=1, column=1, value="Matrix reports - coming soon")
        ws.cell(row=2, column=1, value="Use CSV export for now")


    def _auto_adjust_columns(self, ws):
        """Auto-adjust column widths"""
        for column in ws.columns:
            max_length = 0
            column_letter = column[0].column_letter
            
            for cell in column:
                try:
                    if cell.value:
                        max_length = max(max_length, len(str(cell.value)))
                except:
                    pass
            
            adjusted_width = min(max_length + 2, 50)
            ws.column_dimensions[column_letter].width = adjusted_width







    def export_selected_reports_to_zip_concurrent_excel(
        self,
        output_zip_path: str,
        report_ids: List[str],
        max_workers: int = 10,
        cancel_event: Optional[Any] = None,
        retry_attempts: int = 3,
        reports_metadata: Optional[Dict[str, Dict]] = None
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to NATIVE Excel format
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_excel_"))
        
        try:
            # ===== STEP 1: Get/validate metadata =====
            print(f"üìä Preparing to export {len(report_ids)} reports as native Excel...")
            
            # Use provided metadata if available, else fetch
            if reports_metadata:
                print("‚ö° Using cached metadata (skipping API calls)")
                reports = []
                for report_id in report_ids:
                    if report_id in reports_metadata:
                        reports.append(reports_metadata[report_id])
                    else:
                        reports.append({
                            "id": report_id,
                            "name": report_id,
                            "reportFormat": "TABULAR"
                        })
            else:
                print("üîç Fetching report metadata...")
                if not report_ids:
                    reports = []
                else:
                    chunk_size = 50
                    reports = []
                    
                    for i in range(0, len(report_ids), chunk_size):
                        if cancel_event and cancel_event.is_set():
                            raise Exception("Export cancelled by user")
                        
                        chunk_ids = report_ids[i:i + chunk_size]
                        ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                        
                        base_query = f"""
                            SELECT Id, Name, Format 
                            FROM Report 
                            WHERE Id IN ({ids_formatted})
                        """
                        
                        try:
                            chunk_records = self._query_with_pagination(
                                base_query.strip(), 
                                batch_size=2000,
                                cancel_event=cancel_event
                            )
                            
                            for record in chunk_records:
                                reports.append({
                                    "id": record.get("Id"),
                                    "name": record.get("Name"),
                                    "reportFormat": record.get("Format", "TABULAR")
                                })
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error fetching report chunk {i//chunk_size + 1}: {str(e)[:100]}")
                            for rid in chunk_ids:
                                reports.append({
                                    "id": rid,
                                    "name": rid,
                                    "reportFormat": "TABULAR"
                                })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []
            used_filenames: Dict[str, int] = {}
            
            # Thread-safe counters
            completed_lock = threading.Lock()
            
            # Adaptive worker count
            if total > 5000:
                max_workers = min(max_workers, 8)
                print(f"‚öôÔ∏è Large export detected ({total} reports), using {max_workers} workers")
            elif total > 1000:
                max_workers = min(max_workers, 10)
                print(f"‚öôÔ∏è Using {max_workers} workers for {total} reports")
            
            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports (Excel)",
                    "api_version": self.api_version,
                    "cancelled": False,
                    "completed": 0
                }
            
            # ===== STEP 2: Define worker function =====
            def export_single_report_excel(report: Dict) -> tuple:
                """Export a single report as native Excel - runs in thread pool"""
                nonlocal completed
                
                if cancel_event and cancel_event.is_set():
                    return ("cancelled", report, None)
                
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")
                
                # Notify: Starting download
                if self.progress_callback:
                    try:
                        with completed_lock:
                            current_count = completed
                        self.progress_callback(current_count, total, report_name)
                    except:
                        pass
                
                # Generate filename (thread-safe)
                base_name = safe_filename(report_name)

                with completed_lock:
                    if base_name in used_filenames:
                        used_filenames[base_name] += 1
                        # ‚úÖ FIXED: Use .xlsx extension for proper Excel files
                        filename = f"{base_name}_{used_filenames[base_name]}.xlsx"
                    else:
                        used_filenames[base_name] = 1
                        filename = f"{base_name}.xlsx"

                excel_path = tmp_dir / filename
                
                # Retry logic with exponential backoff
                last_error = None
                for attempt in range(retry_attempts):
                    if cancel_event and cancel_event.is_set():
                        return ("cancelled", report, None)
                    
                    try:
                        # Adaptive timeout
                        timeout = 180 if total > 5000 else 120
                        
                        # ‚úÖ SMART EXPORT: Try Analytics API first, fallback to CSV‚ÜíXLSX
                        export_method = "formatted"
                        
                        try:
                            # Method 1: Analytics API (preserves groupings)
                            excel_content = self.export_report_with_formatting(report_id, timeout=timeout)
                            
                        except Exception as analytics_error:
                            # Check if it's a "report type not supported" error
                            error_msg = str(analytics_error).lower()
                            
                            if "matrix" in error_msg or "joined" in error_msg:
                                # Expected failure - some report types aren't supported yet
                                print(f"‚ÑπÔ∏è Report type not supported for formatting, using CSV‚ÜíXLSX")
                            else:
                                # Unexpected failure - log it
                                print(f"‚ö†Ô∏è Analytics API failed: {str(analytics_error)[:100]}")
                            
                            # Method 2: CSV‚ÜíXLSX fallback (always works)
                            export_method = "csv_convert"
                            excel_content = self.export_report_excel_native(report_id, timeout=timeout)
                        
                        if not excel_content:
                            raise Exception("Empty response from both export methods")
                        
                        # Write to file
                        excel_path.write_bytes(excel_content)
                        
                        # Log which method worked
                        print(f"‚úÖ Saved: {filename} ({len(excel_content)} bytes, method: {export_method})")
                        
                        # ‚úÖ ULTRA-PERMISSIVE: Accept any non-empty content
                        if not excel_content:
                            raise Exception("Empty response from Salesforce")
                        
                        # ‚úÖ Write binary Excel content directly to file
                        excel_path.write_bytes(excel_content)
                        
                        # ‚úÖ REMOVED: File existence check (unnecessary)
                        
                        # ‚úÖ Log success
                        print(f"‚úÖ Saved: {filename} ({len(excel_content)} bytes)")
                        
                        # Success! Increment counter
                        with completed_lock:
                            completed += 1
                            current_count = completed
                        
                        # Notify: Report completed successfully
                        if self.progress_callback:
                            try:
                                self.progress_callback(current_count, total)
                            except:
                                pass
                        
                        return ("success", report, filename)
                        
                    except Exception as e:
                        last_error = str(e)
                        if attempt < retry_attempts - 1:
                            wait_time = (2 ** attempt) + (attempt * 0.5)
                            time.sleep(wait_time)
                            continue
                        else:
                            break
                
                # Failed after all retries
                error_content = (
                    f"# Failed to export report as native Excel after {retry_attempts} attempts\n"
                    f"# Report Name: {report_name}\n"
                    f"# Report ID: {report_id}\n"
                    f"# Report Type: {report_type}\n"
                    f"# Error: {last_error}\n"
                    f"#\n"
                    f"# Note: This uses Salesforce native Excel export (xf=excel)\n"
                    f"# which preserves all formatting, groupings, and summaries.\n"
                )
                
                # Create error file (as .txt since Excel export failed)
                error_path = tmp_dir / f"{base_name}_ERROR.txt"
                error_path.write_text(error_content, encoding="utf-8")
                
                # Increment counter
                with completed_lock:
                    completed += 1
                    current_count = completed
                
                # Notify: Report failed
                if self.progress_callback:
                    try:
                        self.progress_callback(current_count, total)
                    except:
                        pass
                
                return ("failed", report, last_error)
            
            # ===== STEP 3: Export reports concurrently =====
            print(f"üöÄ Starting concurrent native Excel export with {max_workers} workers...")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all tasks
                future_to_report = {
                    executor.submit(export_single_report_excel, report): report 
                    for report in reports
                }
                
                # Track progress milestones
                last_milestone = 0
                milestone_interval = max(100, total // 20)
                
                # Process completed tasks
                for future in as_completed(future_to_report):
                    if cancel_event and cancel_event.is_set():
                        print("‚ö†Ô∏è Cancellation detected, stopping remaining downloads...")
                        for f in future_to_report:
                            f.cancel()
                        break
                    
                    try:
                        status, report, data = future.result()
                        
                        if status == "success":
                            successful.append(report.get("name"))
                        elif status == "failed":
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": data
                            })
                        elif status == "cancelled":
                            pass
                        
                        # Log progress milestones
                        if completed - last_milestone >= milestone_interval:
                            success_rate = (len(successful) / completed * 100) if completed > 0 else 0
                            print(f"üìä Progress: {completed}/{total} ({completed/total*100:.1f}%) - Success rate: {success_rate:.1f}%")
                            last_milestone = completed
                        
                        # Update progress callback
                        if self.progress_callback:
                            try:
                                self.progress_callback(completed, total)
                            except Exception:
                                pass
                                
                    except Exception as e:
                        report = future_to_report.get(future)
                        if report:
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": str(e)
                            })
                            print(f"‚ö†Ô∏è Future error for {report.get('name')}: {str(e)[:100]}")
            
            # Check if cancelled
            was_cancelled = cancel_event and cancel_event.is_set()
            
            # ===== STEP 4: Create ZIP file =====
            print(f"üì¶ Creating ZIP file with {completed} native Excel reports...")
            
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                # Write files in sorted order
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                # Create summary
                summary = self._create_summary(
                    total, 
                    successful, 
                    failed, 
                    "Selected Reports (Native Excel)" + (" (CANCELLED)" if was_cancelled else "")
                )
                
                # Add Excel-specific note to summary
                summary += "\n\n"
                summary += "=" * 50 + "\n"
                summary += "EXCEL EXPORT FORMAT INFORMATION\n"
                summary += "=" * 50 + "\n"
                summary += "Export Method: Salesforce Native Excel (xf=excel)\n"
                summary += "\n"
                summary += "‚úÖ This export preserves ALL Salesforce UI formatting:\n"
                summary += "  ‚Ä¢ Row groupings (Group By columns)\n"
                summary += "  ‚Ä¢ Indentation of grouped rows\n"
                summary += "  ‚Ä¢ Summary rows (totals, subtotals, grand totals)\n"
                summary += "  ‚Ä¢ Excel formatting (grouped appearance)\n"
                summary += "  ‚Ä¢ Column formatting\n"
                summary += "\n"
                summary += "üìÑ File Format: .xlsx (Microsoft Excel 2007+)\n"
                summary += "üìä Conversion: CSV ‚Üí XLSX with auto-formatted headers\n"
                summary += "üí° Open with: Microsoft Excel, LibreOffice Calc, Google Sheets\n"
                summary += "\n"
                summary += "‚ö†Ô∏è Excel Security Warning:\n"
                summary += "  When opening .xls files, Excel may show a warning:\n"
                summary += "  'The file format and extension don't match'\n"
                summary += "  This is NORMAL for Salesforce exports - click 'Yes' to open.\n"
                summary += "  The files are safe and contain your report data.\n"
                summary += "\n"
                summary += "‚ÑπÔ∏è Why this happens:\n"
                summary += "  Salesforce exports use HTML-formatted Excel (not binary .xls)\n"
                summary += "  This preserves formatting but triggers Excel's security check.\n"
                summary += "\n"
                summary += "üíæ Files are larger than CSV but preserve all formatting.\n"
                summary += "=" * 50 + "\n"
                
                zf.writestr("_EXPORT_SUMMARY.txt", summary)
            
            # Final statistics
            success_rate = (len(successful) / total * 100) if total > 0 else 0
            print(f"‚úÖ Native Excel export complete: {len(successful)}/{total} successful ({success_rate:.1f}%)")
            if failed:
                print(f"‚ö†Ô∏è Failed: {len(failed)} reports")
            
            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports (Native Excel)",
                "api_version": self.api_version,
                "cancelled": was_cancelled,
                "completed": completed
            }
        
        finally:
            # Cleanup temporary files
            try:
                shutil.rmtree(tmp_dir)
                print(f"üßπ Cleaned up temporary files")
            except Exception as e:
                print(f"‚ö†Ô∏è Error cleaning temp directory: {str(e)[:100]}")


    


    
    
    # exporter.py - Part 3: Export Methods
# This continues the SalesforceReportExporter class

    # Add these methods to the SalesforceReportExporter class

    def export_reports_by_folder_to_zip(
        self,
        output_zip_path: str,
        folder_id: str,
        delay_between_reports: float = 1.0
    ) -> Dict[str, Any]:
        """
        Export all reports from a specific folder to a ZIP file.
        
        Args:
            output_zip_path: Path where ZIP file will be saved
            folder_id: The Salesforce folder ID to export reports from
            delay_between_reports: Seconds to wait between exports (rate limiting)
            
        Returns:
            Dictionary with export results
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # Step 1: Get reports from this folder
            reports = self.list_reports(folder_id=folder_id)
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            # Get folder name
            folder_name = self._get_folder_name(folder_id)

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", f"No reports found in folder: {folder_name}")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": folder_name,
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, folder_name)
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": folder_name,
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass

    def export_all_reports_to_zip(
        self,
        output_zip_path: str,
        delay_between_reports: float = 0.5
    ) -> Dict[str, Any]:
        """
        Export ALL reports from ALL folders to a ZIP file.
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # Step 1: Get list of all reports
            reports = self.list_reports()
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found in this Salesforce org.")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "All Folders",
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, "All Folders")
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "All Folders",
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass
    
    # exporter.py - Part 4: Selected Reports Export & Helper Methods
# This completes the SalesforceReportExporter class

    # Add these final methods to the SalesforceReportExporter class

    def export_selected_reports_to_zip(
        self,
        output_zip_path: str,
        report_ids: List[str],
        delay_between_reports: float = 0.5
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to a ZIP file.
        Now uses direct SOQL to ensure system/automated reports are found.
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # --- IMPROVED: Direct SOQL lookup with pagination ---
            if not report_ids:
                reports = []
            else:
                # Split into chunks of 100 IDs (SOQL IN clause limit is ~1000 chars)
                chunk_size = 100
                reports = []
                
                for i in range(0, len(report_ids), chunk_size):
                    chunk_ids = report_ids[i:i + chunk_size]
                    ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                    
                    base_query = f"""
                        SELECT Id, Name, Format 
                        FROM Report 
                        WHERE Id IN ({ids_formatted})
                    """
                    
                    try:
                        # Use pagination helper (though with 100 IDs we won't need pagination)
                        chunk_records = self._query_with_pagination(base_query.strip(), batch_size=2000)
                        
                        for record in chunk_records:
                            reports.append({
                                "id": record.get("Id"),
                                "name": record.get("Name"),
                                "reportFormat": record.get("Format", "TABULAR")
                            })
                    except Exception as e:
                        print(f"Error fetching report chunk: {str(e)}")
                        # Fallback: create entries with just IDs
                        for rid in chunk_ids:
                            reports.append({
                                "id": rid,
                                "name": rid,
                                "reportFormat": "TABULAR"
                            })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports",
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each selected report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, "Selected Reports")
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports",
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass
    

    def export_selected_reports_to_zip_concurrent(
        self,
        output_zip_path: str,
        report_ids: List[str],
        max_workers: int = 10,  # ‚úÖ INCREASED: Default 10 workers for faster exports
        cancel_event: Optional[Any] = None,
        retry_attempts: int = 3,
        reports_metadata: Optional[Dict[str, Dict]] = None
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to a ZIP file using CONCURRENT downloads.
        
        ‚úÖ OPTIMIZED: Better memory management + adaptive worker count for 10,000+ reports.
        
        Args:
            output_zip_path: Path where ZIP file will be saved
            report_ids: List of report IDs to export
            max_workers: Number of parallel downloads (default 10)
            cancel_event: Threading event to signal cancellation
            retry_attempts: Number of retry attempts for failed reports
            reports_metadata: Optional dict of {report_id: {name, format}} to skip metadata fetch
            
        Returns:
            Dictionary with export results
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))
        
        try:
            # ===== STEP 1: Get/validate metadata =====
            print(f"üìä Preparing to export {len(report_ids)} reports...")
            
            # ‚úÖ OPTIMIZED: Use provided metadata if available, else fetch
            if reports_metadata:
                print("‚ö° Using cached metadata (skipping API calls)")
                reports = []
                for report_id in report_ids:
                    if report_id in reports_metadata:
                        reports.append(reports_metadata[report_id])
                    else:
                        # Fallback: create basic entry
                        reports.append({
                            "id": report_id,
                            "name": report_id,
                            "reportFormat": "TABULAR"
                        })
            else:
                print("üîç Fetching report metadata...")
                # Fallback: Fetch metadata if not provided
                if not report_ids:
                    reports = []
                else:
                    # ‚úÖ OPTIMIZED: Smaller chunks for stability
                    chunk_size = 50  # Reduced from 100
                    reports = []
                    
                    for i in range(0, len(report_ids), chunk_size):
                        # Check for cancellation
                        if cancel_event and cancel_event.is_set():
                            raise Exception("Export cancelled by user")
                        
                        chunk_ids = report_ids[i:i + chunk_size]
                        ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                        
                        base_query = f"""
                            SELECT Id, Name, Format 
                            FROM Report 
                            WHERE Id IN ({ids_formatted})
                        """
                        
                        try:
                            chunk_records = self._query_with_pagination(
                                base_query.strip(), 
                                batch_size=2000,
                                cancel_event=cancel_event
                            )
                            
                            for record in chunk_records:
                                reports.append({
                                    "id": record.get("Id"),
                                    "name": record.get("Name"),
                                    "reportFormat": record.get("Format", "TABULAR")
                                })
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error fetching report chunk {i//chunk_size + 1}: {str(e)[:100]}")
                            # Fallback: create entries with just IDs
                            for rid in chunk_ids:
                                reports.append({
                                    "id": rid,
                                    "name": rid,
                                    "reportFormat": "TABULAR"
                                })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []
            used_filenames: Dict[str, int] = {}
            
            # Thread-safe counters
            completed_lock = threading.Lock()
            
            # ‚úÖ NEW: Adaptive worker count based on total reports
            if total > 5000:
                max_workers = min(max_workers, 8)  # Reduce workers for very large exports
                print(f"‚öôÔ∏è Large export detected ({total} reports), using {max_workers} workers")
            elif total > 1000:
                max_workers = min(max_workers, 10)
                print(f"‚öôÔ∏è Using {max_workers} workers for {total} reports")
            
            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports",
                    "api_version": self.api_version,
                    "cancelled": False,
                    "completed": 0
                }
            
            # ===== STEP 2: Define worker function =====
            def export_single_report(report: Dict) -> tuple:
                """Export a single report - runs in thread pool"""
                nonlocal completed
                
                # Check for cancellation
                if cancel_event and cancel_event.is_set():
                    return ("cancelled", report, None)
                
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")
                
                # ‚úÖ NOTIFY: Starting download of this report
                if self.progress_callback:
                    try:
                        with completed_lock:
                            current_count = completed
                        # Signal: download starting (with report name)
                        self.progress_callback(current_count, total, report_name)
                    except:
                        pass
                
                # Generate filename (thread-safe)
                base_name = safe_filename(report_name)
                
                with completed_lock:
                    if base_name in used_filenames:
                        used_filenames[base_name] += 1
                        filename = f"{base_name}_{used_filenames[base_name]}.csv"
                    else:
                        used_filenames[base_name] = 1
                        filename = f"{base_name}.csv"
                
                csv_path = tmp_dir / filename
                
                # Retry logic with exponential backoff
                last_error = None
                for attempt in range(retry_attempts):
                    # Check cancellation before each attempt
                    if cancel_event and cancel_event.is_set():
                        return ("cancelled", report, None)
                    
                    try:
                        # ‚úÖ IMPROVED: Adaptive timeout based on total export size
                        timeout = 180 if total > 5000 else 120
                        
                        csv_content = self.export_report_csv(report_id, timeout=timeout)
                        
                        if not csv_content or len(csv_content.strip()) == 0:
                            raise Exception("Empty response received")
                        
                        first_line = csv_content.split('\n')[0] if csv_content else ""
                        if 'Error' in first_line and len(csv_content) < 500:
                            raise Exception(f"Salesforce error: {first_line[:100]}")
                        
                        csv_path.write_text(csv_content, encoding="utf-8")
                        
                        # Success! Increment counter FIRST
                        with completed_lock:
                            completed += 1
                            current_count = completed
                        
                        # ‚úÖ NOTIFY: Report completed successfully
                        if self.progress_callback:
                            try:
                                self.progress_callback(current_count, total)
                            except:
                                pass
                        
                        return ("success", report, filename)
                        
                    except Exception as e:
                        last_error = str(e)
                        if attempt < retry_attempts - 1:
                            # ‚úÖ IMPROVED: Exponential backoff with jitter
                            wait_time = (2 ** attempt) + (attempt * 0.5)  # 1s, 2.5s, 5s
                            time.sleep(wait_time)
                            continue
                        else:
                            # All retries failed
                            break
                
                # Failed after all retries
                error_content = (
                    f"# Failed to export report after {retry_attempts} attempts\n"
                    f"# Report Name: {report_name}\n"
                    f"# Report ID: {report_id}\n"
                    f"# Report Type: {report_type}\n"
                    f"# Error: {last_error}\n"
                )
                csv_path.write_text(error_content, encoding="utf-8")

                # Increment counter FIRST
                with completed_lock:
                    completed += 1
                    current_count = completed

                # ‚úÖ NOTIFY: Report failed (but counted as completed)
                if self.progress_callback:
                    try:
                        self.progress_callback(current_count, total)
                    except:
                        pass

                return ("failed", report, last_error)
            
            # ===== STEP 3: Export reports concurrently =====
            print(f"üöÄ Starting concurrent export with {max_workers} workers...")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all tasks
                future_to_report = {
                    executor.submit(export_single_report, report): report 
                    for report in reports
                }
                
                # ‚úÖ NEW: Track progress milestones
                last_milestone = 0
                milestone_interval = max(100, total // 20)  # Log every 5%
                
                # Process completed tasks
                for future in as_completed(future_to_report):
                    # Check for cancellation
                    if cancel_event and cancel_event.is_set():
                        print("‚ö†Ô∏è Cancellation detected, stopping remaining downloads...")
                        # Cancel remaining futures
                        for f in future_to_report:
                            f.cancel()
                        break
                    
                    try:
                        status, report, data = future.result()
                        
                        if status == "success":
                            successful.append(report.get("name"))
                        elif status == "failed":
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": data
                            })
                        elif status == "cancelled":
                            # Don't count as failed, just stopped
                            pass
                        
                        # ‚úÖ NEW: Log progress milestones
                        if completed - last_milestone >= milestone_interval:
                            success_rate = (len(successful) / completed * 100) if completed > 0 else 0
                            print(f"üìä Progress: {completed}/{total} ({completed/total*100:.1f}%) - Success rate: {success_rate:.1f}%")
                            last_milestone = completed
                        
                        # Update progress callback
                        if self.progress_callback:
                            try:
                                self.progress_callback(completed, total)
                            except Exception:
                                pass
                                
                    except Exception as e:
                        # Future itself failed
                        report = future_to_report.get(future)
                        if report:
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": str(e)
                            })
                            print(f"‚ö†Ô∏è Future error for {report.get('name')}: {str(e)[:100]}")
            
            # Check if cancelled
            was_cancelled = cancel_event and cancel_event.is_set()
            
            # ===== STEP 4: Create ZIP file =====
            print(f"üì¶ Creating ZIP file with {completed} reports...")
            
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                # ‚úÖ OPTIMIZED: Write files in sorted order for consistent ZIP structure
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(
                    total, 
                    successful, 
                    failed, 
                    "Selected Reports" + (" (CANCELLED)" if was_cancelled else "")
                )
                zf.writestr("_EXPORT_SUMMARY.txt", summary)
            
            # ‚úÖ NEW: Final statistics
            success_rate = (len(successful) / total * 100) if total > 0 else 0
            print(f"‚úÖ Export complete: {len(successful)}/{total} successful ({success_rate:.1f}%)")
            if failed:
                print(f"‚ö†Ô∏è Failed: {len(failed)} reports")
            
            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports",
                "api_version": self.api_version,
                "cancelled": was_cancelled,
                "completed": completed
            }
        
        finally:
            # ‚úÖ IMPROVED: Better cleanup with error handling
            try:
                shutil.rmtree(tmp_dir)
                print(f"üßπ Cleaned up temporary files")
            except Exception as e:
                print(f"‚ö†Ô∏è Error cleaning temp directory: {str(e)[:100]}")


    def _get_folder_name(self, folder_id: str) -> str:
        """Get the name of a folder by its ID"""
        try:
            url = f"{self.instance_url}/services/data/{self.api_version}/sobjects/Folder/{folder_id}"
            response = retry_request(url, headers=self.api_headers, timeout=30)
            data = response.json()
            return data.get("Name", folder_id)
        except:
            return folder_id

    def _create_summary(
        self,
        total: int,
        successful: List[str],
        failed: List[Dict[str, Any]],
        folder_name: str = "Unknown"
    ) -> str:
        """Create a summary text file for the export."""
        lines = [
            "SALESFORCE REPORT EXPORT SUMMARY",
            "=" * 40,
            f"Export Date: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"Instance: {self.instance_url}",
            f"API Version: {self.api_version}",
            f"Folder: {folder_name}",
            "",
            f"Total Reports: {total}",
            f"Successful: {len(successful)}",
            f"Failed: {len(failed)}",
            "",
        ]
        
        if failed:
            lines.append("FAILED REPORTS:")
            lines.append("-" * 40)
            for f in failed:
                lines.append(f"‚Ä¢ {f.get('name')} ({f.get('type')})")
                lines.append(f"  ID: {f.get('id')}")
                lines.append(f"  Error: {f.get('error')}")
                lines.append("")
        
        return "\n".join(lines)
    
    