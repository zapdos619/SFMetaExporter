# exporter.py - Part 1: Helper Functions and Setup
# Salesforce Report Exporter with DYNAMIC API version detection

import time
import tempfile
import zipfile
import shutil
from pathlib import Path
import requests
from typing import Callable, Optional, List, Dict, Any
import threading


def get_org_api_version(instance_url: str, session_id: str = None) -> str:
    """
    Fetch the latest API version supported by the Salesforce org.
    This endpoint doesn't require authentication.
    
    Args:
        instance_url: The Salesforce instance URL
        session_id: Optional session ID (not required for this call)
        
    Returns:
        Latest API version string (e.g., "v61.0")
    """
    try:
        url = f"{instance_url.rstrip('/')}/services/data/"
        response = requests.get(url, timeout=15)
        
        if response.status_code == 200:
            versions = response.json()
            if versions and len(versions) > 0:
                # Get the latest (last) version
                latest = versions[-1]
                version = latest.get("version", "58.0")
                return f"v{version}"
    except Exception:
        pass
    
    # Fallback to a safe default
    return "v58.0"


def retry_request(
    url: str,
    headers: dict = None,
    cookies: dict = None,
    max_retries: int = 3,
    timeout: int = 60,
    allow_redirects: bool = True,
    backoff_factor: float = 2.0  # ‚úÖ NEW: Configurable backoff
) -> requests.Response:
    """
    Make HTTP GET request with exponential backoff retry logic.
    
    ‚úÖ OPTIMIZED: Better rate limit handling for large exports.
    """
    backoff = 1
    last_error = None
    headers = headers or {}
    cookies = cookies or {}

    for attempt in range(max_retries):
        try:
            response = requests.get(
                url,
                headers=headers,
                cookies=cookies,
                timeout=timeout,
                allow_redirects=allow_redirects
            )

            if response.status_code == 200:
                return response

            # ‚úÖ IMPROVED: Better rate limit handling
            if response.status_code == 429:  # Too Many Requests
                retry_after = response.headers.get('Retry-After')
                if retry_after:
                    try:
                        backoff = int(retry_after)
                    except ValueError:
                        backoff = min(backoff * backoff_factor, 120)  # Cap at 2 minutes
                else:
                    backoff = min(backoff * backoff_factor, 120)
                
                print(f"‚ö†Ô∏è Rate limited, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue

            # ‚úÖ IMPROVED: Better handling of server errors
            if response.status_code in (500, 502, 503, 504):
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Server error {response.status_code}, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue

            response.raise_for_status()

        except requests.Timeout as e:
            last_error = e
            if attempt < max_retries - 1:
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Timeout, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue
            raise
        except requests.RequestException as e:
            last_error = e
            if attempt < max_retries - 1:
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Request error, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue
            raise

    raise Exception(f"Request failed after {max_retries} retries: {last_error}")


def safe_filename(name: str, max_length: int = 100) -> str:
    """Sanitize filename by removing invalid characters."""
    if not name:
        return "unnamed_report"
    safe = "".join(c if c.isalnum() or c in " ._-" else "_" for c in name)
    while "__" in safe:
        safe = safe.replace("__", "_")
    safe = safe.strip("_ ")
    return safe[:max_length] if safe else "unnamed_report"


def clean_csv_footer(csv_content: str) -> str:
    """
    Remove Salesforce's metadata footer from CSV content.
    
    The footer typically contains:
    - Report name
    - Copyright notice
    - Confidential information warning
    - Generated by information
    
    Args:
        csv_content: Raw CSV content from Salesforce
        
    Returns:
        Cleaned CSV content without footer
    """
    lines = csv_content.split('\n')
    
    cleaned_lines = []
    footer_started = False
    blank_line_count = 0
    
    for i, line in enumerate(lines):
        stripped = line.strip()
        
        if not footer_started:
            if not stripped:
                blank_line_count += 1
                cleaned_lines.append(line)
            else:
                blank_line_count = 0
                
                footer_indicators = [
                    'Copyright (c)',
                    'Confidential Information',
                    'Generated By:',
                    '¬© Copyright',
                    'Do Not Distribute'
                ]
                
                if any(indicator in line for indicator in footer_indicators):
                    footer_started = True
                    while cleaned_lines and not cleaned_lines[-1].strip():
                        cleaned_lines.pop()
                elif ',' not in stripped and len(stripped) > 0:
                    next_lines = lines[i+1:min(i+5, len(lines))]
                    next_text = ' '.join(next_lines)
                    if any(indicator in next_text for indicator in footer_indicators):
                        footer_started = True
                        while cleaned_lines and not cleaned_lines[-1].strip():
                            cleaned_lines.pop()
                    else:
                        cleaned_lines.append(line)
                else:
                    cleaned_lines.append(line)
    
    result = '\n'.join(cleaned_lines)
    result = result.rstrip('\n')
    
    return result

# exporter.py - Part 2: SalesforceReportExporter Class
# This continues from Part 1 (helper functions)

class SalesforceReportExporter:
    """
    Export Salesforce reports to formatted Excel files and package them into a ZIP.
    
    Uses THREE methods:
    1. REST API to get list of reports (with metadata)
    2. Analytics API to export reports with full formatting (groupings, summaries)
    3. UI Export URL to download CSV (for CSV format exports only)
    
    API version is detected dynamically from the org.
    """

    def __init__(
        self,
        session_id: str,
        instance_url: str,
        api_version: str = None,
        progress_callback: Optional[Callable[[int, int], None]] = None
    ):
        self.session_id = session_id
        self.instance_url = instance_url.rstrip('/')
        self.progress_callback = progress_callback
        
        # Get API version dynamically if not provided
        if api_version:
            self.api_version = api_version if api_version.startswith('v') else f"v{api_version}"
        else:
            self.api_version = get_org_api_version(self.instance_url)
        
        # Build endpoints with dynamic version
        self.reports_list_endpoint = f"/services/data/{self.api_version}/analytics/reports"
        self.folders_list_endpoint = f"/services/data/{self.api_version}/folders"
        
        # Headers for REST API calls (list reports)
        self.api_headers = {
            "Authorization": f"Bearer {self.session_id}",
            "Accept": "application/json"
        }
        
        # Cookies for UI export (CSV download)
        self.export_cookies = {
            "sid": self.session_id
        }
    
    def _query_with_pagination(
        self,
        base_query: str,
        batch_size: int = 2000,
        progress_callback: Optional[Callable[[int, int], None]] = None,
        cancel_event: Optional[Any] = None  # ‚úÖ NEW: Cancellation support
    ) -> List[Dict[str, Any]]:
        """
        Execute SOQL query with automatic pagination.
        Handles Salesforce's 2,000 row limit per query.
        
        ‚úÖ OPTIMIZED: Cancellation support + better timeout handling for large queries.
        
        Args:
            base_query: Base SOQL query (without LIMIT/OFFSET)
            batch_size: Records per batch (default 2000, Salesforce limit)
            progress_callback: Optional callback(fetched, estimated_total)
            cancel_event: Optional threading.Event to check for cancellation
            
        Returns:
            List of all records combined from all pages
        """
        all_records = []
        offset = 0
        has_more = True
        estimated_total = None
        consecutive_errors = 0  # ‚úÖ NEW: Track consecutive errors
        max_consecutive_errors = 3  # ‚úÖ NEW: Fail after 3 consecutive errors
        
        while has_more:
            # ‚úÖ NEW: Check for cancellation
            if cancel_event and cancel_event.is_set():
                print(f"‚ö†Ô∏è Query cancelled at offset {offset}")
                break
            
            # Build paginated query
            paginated_query = f"{base_query} LIMIT {batch_size} OFFSET {offset}"
            
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": paginated_query}
            
            try:
                # ‚úÖ IMPROVED: Longer timeout for large queries
                timeout = 90 if len(all_records) > 5000 else 60
                
                response = requests.get(
                    query_url,
                    headers=self.api_headers,
                    params=params,
                    timeout=timeout
                )
                response.raise_for_status()
                
                data = response.json()
                records = data.get("records", [])
                
                if not records:
                    has_more = False
                    break
                
                all_records.extend(records)
                offset += len(records)
                
                # ‚úÖ RESET: Reset error counter on success
                consecutive_errors = 0
                
                # Update progress if callback provided
                if progress_callback and estimated_total is None:
                    # Estimate total based on first batch
                    if len(records) == batch_size:
                        estimated_total = batch_size * 10  # Rough estimate
                    else:
                        estimated_total = len(records)
                
                if progress_callback:
                    progress_callback(len(all_records), estimated_total or len(all_records))
                
                # If we got fewer records than batch_size, we're done
                if len(records) < batch_size:
                    has_more = False
                
                # ‚úÖ NEW: Small delay to avoid rate limiting on large queries
                if has_more and len(all_records) > 0 and len(all_records) % 10000 == 0:
                    print(f"üìä Fetched {len(all_records)} records, brief pause to avoid rate limits...")
                    time.sleep(1)
                
            except requests.Timeout as e:
                consecutive_errors += 1
                print(f"‚ö†Ô∏è Query timeout at offset {offset} (attempt {consecutive_errors}/{max_consecutive_errors})")
                
                if consecutive_errors >= max_consecutive_errors:
                    print(f"‚ùå Too many consecutive errors, stopping pagination at {len(all_records)} records")
                    has_more = False
                else:
                    # Wait before retry
                    time.sleep(2 * consecutive_errors)
                    continue
                    
            except requests.RequestException as e:
                consecutive_errors += 1
                print(f"‚ö†Ô∏è Query error at offset {offset}: {str(e)[:100]} (attempt {consecutive_errors}/{max_consecutive_errors})")
                
                if consecutive_errors >= max_consecutive_errors:
                    print(f"‚ùå Too many consecutive errors, stopping pagination at {len(all_records)} records")
                    has_more = False
                else:
                    # Wait before retry
                    time.sleep(2 * consecutive_errors)
                    continue
        
        return all_records

    def list_all_report_folders(self) -> List[Dict[str, Any]]:
        """
        Fetch list of all report folders in the org that the user has access to.
        Returns list of folder metadata (id, name, type, etc.)
        Filters out system/automated folders.
        """
        try:
            # Query for Report folders that user has access to
            query = """
                SELECT Id, Name, Type, DeveloperName, AccessType 
                FROM Folder 
                WHERE Type = 'Report' 
                ORDER BY Name
            """
            
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": query}
            
            response = requests.get(
                query_url, 
                headers=self.api_headers, 
                params=params, 
                timeout=30
            )
            response.raise_for_status()
            
            data = response.json()
            folders = data.get("records", [])
            
            # Clean up the response - convert to simple dict
            cleaned_folders = []
            for folder in folders:
                cleaned_folders.append({
                    "id": folder.get("Id"),
                    "name": folder.get("Name"),
                    "type": folder.get("Type"),
                    "developerName": folder.get("DeveloperName"),
                    "accessType": folder.get("AccessType")
                })
            
            return cleaned_folders
            
        except Exception as e:
            raise Exception(f"Failed to fetch report folders: {str(e)}")

    def list_all_reports(self, folder_id: str = None) -> List[Dict[str, Any]]:
        """
        Fetch list of all available reports using REST API or SOQL query.
        
        Args:
            folder_id: Optional folder ID to filter reports. If None, returns all reports.
            
        Returns:
            List of report metadata (id, name, format, folder, etc.)
        """
        # If folder_id is specified, use SOQL query for accurate filtering
        if folder_id:
            return self._list_reports_by_soql(folder_id)
        
        # Otherwise use the standard REST API endpoint
        url = f"{self.instance_url}{self.reports_list_endpoint}"
        response = retry_request(url, headers=self.api_headers, timeout=60)
        
        data = response.json()
        
        if isinstance(data, list):
            reports = data
        elif isinstance(data, dict):
            reports = data.get("reports", data.get("records", []))
        else:
            reports = []
        
        return reports


    def _list_reports_by_soql(self, folder_id: str) -> List[Dict[str, Any]]:
        """
        Use SOQL query with pagination to get reports by folder ID.
        Now handles unlimited reports per folder.
        
        Args:
            folder_id: The Salesforce folder ID to query
            
        Returns:
            List of report metadata in the same format as list_reports()
        """
        try:
            # SOQL query to get reports in specific folder
            base_query = f"""
                SELECT Id, Name, DeveloperName, FolderName, Format, CreatedDate, LastModifiedDate
                FROM Report 
                WHERE OwnerId = '{folder_id}'
                ORDER BY Name
            """
            
            # Use pagination helper
            records = self._query_with_pagination(base_query.strip())
            
            # Convert SOQL results to match REST API format
            reports = []
            for record in records:
                reports.append({
                    "id": record.get("Id"),
                    "name": record.get("Name"),
                    "developerName": record.get("DeveloperName"),
                    "folderName": record.get("FolderName"),
                    "reportFormat": record.get("Format", "TABULAR"),
                    "lastModifiedDate": record.get("LastModifiedDate"),
                    "createdDate": record.get("CreatedDate")
                })
            
            return reports
            
        except Exception as e:
            print(f"Error querying reports by folder: {str(e)}")
            return []


    def search_by_keyword(self, keyword: str, cancel_event=None) -> Dict[str, Any]:
        """
        Search folders AND reports by keyword, then organize results.
        
        ‚úÖ NEW: Creates a virtual "Unified Public Folder" for orphaned reports
        that match the search but whose folder wasn't found.
        """
        try:
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # Escape keyword for SOQL (prevent injection)
            keyword_escaped = keyword.replace("'", "\\'").replace("%", "\\%")
            
            # ===== STEP 1: Search folders by name =====
            print(f"üîç Step 1: Searching folders matching '{keyword}'...")
            
            folders_query = f"""
                SELECT Id, Name, Type, DeveloperName, AccessType 
                FROM Folder 
                WHERE Type = 'Report' 
                AND (
                    Name LIKE '%{keyword_escaped}%'
                    OR DeveloperName LIKE '%{keyword_escaped}%'
                )
                ORDER BY Name
            """
            
            matching_folders = self._execute_soql_query(folders_query)
            folder_ids_from_name_match = {f.get("Id") for f in matching_folders}
            
            print(f"‚úÖ Found {len(matching_folders)} folders matching keyword")
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 2: Search reports by name (WITH PAGINATION) =====
            print(f"üîç Step 2: Searching reports matching '{keyword}'...")
            
            reports_query = f"""
                SELECT Id, Name, DeveloperName, FolderName, Format, 
                    CreatedDate, LastModifiedDate, OwnerId
                FROM Report 
                WHERE Name LIKE '%{keyword_escaped}%' OR FolderName LIKE '%{keyword_escaped}%'
                ORDER BY Name
            """
            
            # ‚úÖ OPTIMIZED: Use pagination with cancellation support
            matching_reports = self._query_with_pagination(
                reports_query.strip(),
                batch_size=2000,
                cancel_event=cancel_event
            )
            
            print(f"‚úÖ Found {len(matching_reports)} reports matching keyword")
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 3: Get folder IDs from matching reports =====
            folder_ids_from_reports = {r.get("OwnerId") for r in matching_reports if r.get("OwnerId")}
            
            # ===== STEP 4: Fetch folders that contain matching reports =====
            additional_folder_ids = folder_ids_from_reports - folder_ids_from_name_match
            
            additional_folders = []
            if additional_folder_ids:
                print(f"üîç Step 3: Fetching {len(additional_folder_ids)} additional folders...")
                
                # ‚úÖ OPTIMIZED: Process in smaller chunks to avoid query string limits
                folder_ids_list = list(additional_folder_ids)
                chunk_size = 50  # ‚úÖ REDUCED: Smaller chunks for stability
                
                for i in range(0, len(folder_ids_list), chunk_size):
                    # Check cancellation
                    if cancel_event and cancel_event.is_set():
                        return {"folders": [], "reports_by_folder": {}}
                    
                    chunk = folder_ids_list[i:i + chunk_size]
                    ids_str = ",".join([f"'{fid}'" for fid in chunk])
                    
                    folders_query = f"""
                        SELECT Id, Name, Type, DeveloperName, AccessType 
                        FROM Folder 
                        WHERE Id IN ({ids_str})
                    """
                    
                    try:
                        chunk_folders = self._execute_soql_query(folders_query)
                        additional_folders.extend(chunk_folders)
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error fetching folder chunk: {str(e)[:100]}")
                        # Continue with other chunks
                        continue
            
            print(f"‚úÖ Fetched {len(additional_folders)} additional folders")
            
            # ===== STEP 5: Identify orphaned reports =====
            # These are reports that matched the search but whose OwnerId
            # doesn't correspond to any folder we successfully fetched
            
            all_fetched_folder_ids = folder_ids_from_name_match | {f.get("Id") for f in additional_folders}
            
            orphaned_reports = []
            valid_reports = []
            
            for report in matching_reports:
                owner_id = report.get("OwnerId")
                
                if not owner_id or owner_id not in all_fetched_folder_ids:
                    # This report's folder wasn't found - it's orphaned
                    orphaned_reports.append(report)
                else:
                    # This report has a valid folder
                    valid_reports.append(report)
            
            print(f"üìä Report classification:")
            print(f"  ‚úÖ Valid reports (with folders): {len(valid_reports)}")
            print(f"  ‚ö†Ô∏è Orphaned reports (no folder found): {len(orphaned_reports)}")
            
            # ===== STEP 5B: Create virtual "Unified Public Folder" if needed =====
            virtual_folder_id = None
            virtual_folder = None
            
            if orphaned_reports:
                # Create a virtual folder to hold orphaned reports
                virtual_folder_id = "VIRTUAL_UNIFIED_PUBLIC_FOLDER"
                
                virtual_folder = {
                    "Id": virtual_folder_id,
                    "Name": "üìÅ Unified Public Folder (Search Results)",
                    "Type": "Report",
                    "DeveloperName": "UnifiedPublicFolder",
                    "AccessType": "Public"
                }
                
                print(f"üÜï Created virtual folder for {len(orphaned_reports)} orphaned reports")
            
            # ===== STEP 6: Combine all folders (virtual first, then real folders) =====
            all_folders = []
            
            # ‚úÖ Add virtual folder at TOP if it exists
            if virtual_folder:
                all_folders.append(virtual_folder)
            
            # Add real folders (name-matched + additional)
            all_folders.extend(matching_folders)
            all_folders.extend(additional_folders)
            
            # ===== STEP 7: Group reports by folder (MEMORY EFFICIENT) =====
            print(f"üìä Grouping reports by folder...")
            
            reports_by_folder = {}
            
            # ‚úÖ Add orphaned reports to virtual folder FIRST
            if virtual_folder_id and orphaned_reports:
                reports_by_folder[virtual_folder_id] = []
                
                for report in orphaned_reports:
                    reports_by_folder[virtual_folder_id].append({
                        "id": report.get("Id"),
                        "name": report.get("Name"),
                        "developerName": report.get("DeveloperName"),
                        "folderName": report.get("FolderName") or "Unified Public Folder",
                        "reportFormat": report.get("Format", "TABULAR"),
                        "lastModifiedDate": report.get("LastModifiedDate"),
                        "createdDate": report.get("CreatedDate")
                    })
                
                print(f"  ‚úÖ Virtual folder: {len(orphaned_reports)} orphaned reports")
            
            # Group valid reports by their actual folders
            for report in valid_reports:
                folder_id = report.get("OwnerId")
                if folder_id:
                    if folder_id not in reports_by_folder:
                        reports_by_folder[folder_id] = []
                    
                    reports_by_folder[folder_id].append({
                        "id": report.get("Id"),
                        "name": report.get("Name"),
                        "developerName": report.get("DeveloperName"),
                        "folderName": report.get("FolderName"),
                        "reportFormat": report.get("Format", "TABULAR"),
                        "lastModifiedDate": report.get("LastModifiedDate"),
                        "createdDate": report.get("CreatedDate")
                    })
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 8: For folders matched by name, get ALL their reports (IN CHUNKS) =====
            print(f"üîç Step 4: Fetching all reports from {len(matching_folders)} matched folders...")
            
            for idx, folder in enumerate(matching_folders):
                folder_id = folder.get("Id")
                
                # Check cancellation
                if cancel_event and cancel_event.is_set():
                    return {"folders": [], "reports_by_folder": {}}
                
                # If this folder doesn't have any reports yet (from keyword search),
                # fetch ALL reports in this folder
                if folder_id not in reports_by_folder:
                    reports_query = f"""
                        SELECT Id, Name, DeveloperName, FolderName, Format, 
                            CreatedDate, LastModifiedDate
                        FROM Report 
                        WHERE OwnerId = '{folder_id}'
                        ORDER BY Name
                    """
                    
                    try:
                        # ‚úÖ OPTIMIZED: Use pagination with cancellation
                        folder_reports = self._query_with_pagination(
                            reports_query.strip(),
                            batch_size=2000,
                            cancel_event=cancel_event
                        )
                        
                        if folder_reports:
                            reports_by_folder[folder_id] = [
                                {
                                    "id": r.get("Id"),
                                    "name": r.get("Name"),
                                    "developerName": r.get("DeveloperName"),
                                    "folderName": r.get("FolderName"),
                                    "reportFormat": r.get("Format", "TABULAR"),
                                    "lastModifiedDate": r.get("LastModifiedDate"),
                                    "createdDate": r.get("CreatedDate")
                                }
                                for r in folder_reports
                            ]
                            
                            print(f"  ‚úÖ Folder {idx + 1}/{len(matching_folders)}: {len(folder_reports)} reports")
                        
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è Error fetching reports from folder {folder.get('Name')}: {str(e)[:100]}")
                        # Continue with other folders
                        continue
            
            # ===== STEP 9: Clean up folder metadata =====
            cleaned_folders = []
            for folder in all_folders:
                cleaned_folders.append({
                    "id": folder.get("Id"),
                    "name": folder.get("Name"),
                    "type": folder.get("Type"),
                    "developerName": folder.get("DeveloperName"),
                    "accessType": folder.get("AccessType")
                })
            
            # ===== FINAL: Calculate statistics =====
            total_reports = sum(len(reports) for reports in reports_by_folder.values())
            print(f"‚úÖ Search complete: {len(cleaned_folders)} folders, {total_reports} total reports")
            
            # ‚úÖ Log virtual folder stats if present
            if virtual_folder_id and virtual_folder_id in reports_by_folder:
                virtual_count = len(reports_by_folder[virtual_folder_id])
                print(f"  üìÅ Virtual folder contains: {virtual_count} orphaned reports")
            
            return {
                "folders": cleaned_folders,
                "reports_by_folder": reports_by_folder
            }
            
        except Exception as e:
            print(f"‚ùå Search error: {str(e)}")
            raise Exception(f"Search failed: {str(e)}")




  
    def _execute_soql_query(self, query: str) -> List[Dict]:
        """
        Helper method to execute a SOQL query and return records.
        
        Args:
            query: SOQL query string
            
        Returns:
            List of record dictionaries
            
        Raises:
            Exception: If query fails
        """
        try:
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": query.strip()}
            
            response = requests.get(
                query_url,
                headers=self.api_headers,
                params=params,
                timeout=30
            )
            response.raise_for_status()
            
            data = response.json()
            return data.get("records", [])
            
        except requests.RequestException as e:
            raise Exception(f"SOQL query failed: {str(e)}")


    def export_report_csv(self, report_id: str, timeout: int = 120) -> str:
        """
        Export a single report as CSV using the UI export URL method.
        
        ‚úÖ IMPROVED: Configurable timeout (default 120s, can be increased for large reports).
        
        This is the "screen scraping" approach that:
        - Bypasses the 2000 row API limit
        - Returns actual CSV content
        - Works with Lightning and Classic
        - Automatically removes Salesforce metadata footer
        
        Args:
            report_id: Salesforce report ID
            timeout: Request timeout in seconds (default 120)
        """
        # Build the export URL - mimics clicking "Export" in the UI
        export_url = (
            f"{self.instance_url}/{report_id}"
            f"?isdtp=p1&export=1&enc=UTF-8&xf=csv"
        )
        
        # ‚úÖ IMPROVED: Use retry_request with configurable timeout
        response = retry_request(
            export_url,
            cookies=self.export_cookies,
            timeout=timeout,
            allow_redirects=True,
            max_retries=3  # ‚úÖ NEW: Explicitly set retries
        )
        
        content = response.text
        
        # Check if we got HTML instead of CSV
        if content.strip().startswith('<!DOCTYPE') or content.strip().startswith('<html'):
            if 'login.salesforce.com' in content or 'ec=302' in content:
                raise Exception("Session expired or invalid. Please re-login.")
            elif 'You do not have access' in content:
                raise Exception("Access denied to this report.")
            else:
                raise Exception("Received HTML instead of CSV. Report may not be exportable.")
        
        # Clean the CSV content (remove footer)
        cleaned_content = clean_csv_footer(content)
        
        return cleaned_content


    def export_report_with_formatting(self, report_id: str, timeout: int = 180) -> bytes:
        """
        Export report with full Salesforce formatting using Analytics API + SOQL.
        
        ‚úÖ COMPLETE SOLUTION:
        - Uses Analytics API for report structure, groupings, and aggregates
        - Uses SOQL to fetch detail rows if API doesn't provide them
        - Preserves exact Salesforce UI layout
        
        This preserves:
        - Groupings and indentation
        - Detail rows (individual records)
        - Summary rows (subtotals, grand totals)
        - Column formatting
        - Report structure
        
        Returns proper XLSX with all formatting intact.
        """
        try:
            from openpyxl import Workbook
            from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
            from io import BytesIO
            
            print(f"üìä Fetching report data for: {report_id}")
            
            # ===== STEP 1: Get report data from Analytics API =====
            run_url = f"{self.instance_url}/services/data/{self.api_version}/analytics/reports/{report_id}?includeDetails=true"
            
            print(f"   üîó Analytics API Request: {run_url}")
            
            response = retry_request(
                run_url,
                headers=self.api_headers,
                timeout=timeout,
                max_retries=3
            )
            
            report_data = response.json()
            
            # ===== STEP 2: Analyze what we got from API =====
            print(f"   üîç API Response Analysis:")
            
            has_details = report_data.get('hasDetailRows', False)
            all_data = report_data.get('allData', False)
            
            print(f"      hasDetailRows: {has_details}")
            print(f"      allData: {all_data}")
            
            # Count detail rows in response
            fact_map = report_data.get('factMap', {})
            total_detail_rows = sum(len(fact.get('rows', [])) for fact in fact_map.values())
            
            print(f"      Detail rows in API response: {total_detail_rows}")
            
            # ===== STEP 3: Parse report structure =====
            report_metadata = report_data.get('reportMetadata', {})
            report_type = report_metadata.get('reportFormat', 'TABULAR')
            
            print(f"   üìã Report Type: {report_type}")
            
            # Get groupings and columns
            groupings_down = report_metadata.get('groupingsDown', [])
            groupings_across = report_metadata.get('groupingsAcross', [])
            detail_columns = report_metadata.get('detailColumns', [])
            aggregate_columns = report_metadata.get('aggregates', [])
            
            print(f"   üìä Report Structure:")
            print(f"      Groupings Down: {len(groupings_down)}")
            print(f"      Groupings Across: {len(groupings_across)}")
            print(f"      Detail Columns: {len(detail_columns)}")
            print(f"      Aggregate Columns: {len(aggregate_columns)}")
            
            # ===== STEP 4: Decision Point - Do we need SOQL? =====
            needs_soql = (
                report_type == 'SUMMARY' and 
                len(groupings_down) > 0 and 
                len(detail_columns) > 0 and 
                total_detail_rows == 0
            )
            
            if needs_soql:
                print(f"   ‚ö†Ô∏è Summary report with no detail rows detected")
                print(f"   üîÑ Will fetch detail rows via SOQL...")
            else:
                print(f"   ‚úÖ API response is complete, no SOQL needed")
            
            # ===== STEP 5: Create Excel workbook =====
            wb = Workbook()
            ws = wb.active
            ws.title = "Report"
            
            # ===== STEP 6: Write data based on report type =====
            if report_type == 'SUMMARY' and len(groupings_down) > 0:
                print(f"   üìù Writing SUMMARY report with {len(groupings_down)} groupings")
                
                # The _write_summary_report method will automatically fetch SOQL if needed
                self._write_summary_report(ws, report_data, groupings_down, aggregate_columns)
            
            elif report_type == 'MATRIX':
                print(f"   ‚ö†Ô∏è MATRIX reports: Using simplified layout")
                print(f"   üí° For full MATRIX support, export as CSV or use Salesforce UI")
                
                # Matrix reports are complex - provide a notice
                self._write_matrix_report_notice(ws, report_data, groupings_down, groupings_across)
            
            else:
                # TABULAR or SUMMARY with no groupings
                print(f"   üìù Writing TABULAR report")
                self._write_tabular_report(ws, report_data, detail_columns)
            
            # ===== STEP 7: Save to bytes =====
            excel_buffer = BytesIO()
            wb.save(excel_buffer)
            excel_bytes = excel_buffer.getvalue()
            
            print(f"‚úÖ Created formatted Excel: {len(excel_bytes)} bytes")
            print(f"   üìÑ Total rows written: {ws.max_row}")
            print(f"   üìä Total columns: {ws.max_column}")
            
            return excel_bytes
            
        except ImportError:
            raise Exception("openpyxl required. Install: pip install openpyxl")
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"‚ùå Formatted Excel export failed:")
            print(error_details)
            raise Exception(f"Formatted Excel export failed: {str(e)}")


    def _write_matrix_report_notice(self, ws, report_data, groupings_down, groupings_across):
        """
        Write a notice for MATRIX reports with basic information.
        
        MATRIX reports have complex cross-tab layouts that are difficult to 
        replicate in Excel via API. We provide a simplified view with a notice.
        """
        from openpyxl.styles import Font, PatternFill, Alignment
        
        current_row = 1
        
        # Title
        title_cell = ws.cell(row=current_row, column=1, value="MATRIX Report Export")
        title_cell.font = Font(bold=True, size=16, color="C00000")
        current_row += 2
        
        # Notice
        notice_lines = [
            "‚ö†Ô∏è MATRIX Report Limitation",
            "",
            "This report uses MATRIX format (cross-tab layout) which is not fully",
            "supported for Excel export via the Analytics API.",
            "",
            "To get the full formatted report:",
            "  1. Open this report in Salesforce",
            "  2. Click 'Export' ‚Üí 'Formatted Report'",
            "  3. Choose Excel format",
            "",
            "Alternatively:",
            "  ‚Ä¢ Export as CSV format (loses grouping but includes all data)",
            "  ‚Ä¢ Use Salesforce Classic export feature",
            "",
            "This export shows basic aggregated data only:",
        ]
        
        for line in notice_lines:
            ws.cell(row=current_row, column=1, value=line)
            current_row += 1
        
        current_row += 1
        
        # Try to write basic aggregate data
        try:
            fact_map = report_data.get('factMap', {})
            grand_total = fact_map.get('T!T', {})
            
            if grand_total:
                aggregates = grand_total.get('aggregates', [])
                
                # Write aggregate headers and values
                ws.cell(row=current_row, column=1, value="Aggregated Totals:")
                ws.cell(row=current_row, column=1).font = Font(bold=True)
                current_row += 1
                
                report_metadata = report_data.get('reportMetadata', {})
                aggregate_columns = report_metadata.get('aggregates', [])
                report_extended_metadata = report_data.get('reportExtendedMetadata', {})
                aggregate_column_info = report_extended_metadata.get('aggregateColumnInfo', {})
                
                for idx, agg_col in enumerate(aggregate_columns):
                    col_info = aggregate_column_info.get(agg_col, {})
                    label = col_info.get('label', agg_col)
                    value = aggregates[idx].get('label', '') if idx < len(aggregates) else ''
                    
                    ws.cell(row=current_row, column=1, value=label)
                    ws.cell(row=current_row, column=2, value=value)
                    current_row += 1
        except Exception as e:
            print(f"   ‚ö†Ô∏è Could not write aggregate data: {e}")
        
        # Auto-adjust columns
        for col in ws.columns:
            max_length = 0
            column_letter = col[0].column_letter
            for cell in col:
                try:
                    if cell.value:
                        max_length = max(max_length, len(str(cell.value)))
                except:
                    pass
            adjusted_width = min(max_length + 2, 80)
            ws.column_dimensions[column_letter].width = adjusted_width
 
 
 
 
 
    def _get_report_describe(self, report_id: str) -> dict:
        """
        Get report describe metadata which includes filter details and column mappings.
        
        This gives us the information needed to reconstruct the SOQL query.
        
        Returns:
            Report describe metadata
        """
        try:
            describe_url = f"{self.instance_url}/services/data/{self.api_version}/analytics/reports/{report_id}/describe"
            
            response = requests.get(
                describe_url,
                headers=self.api_headers,
                timeout=30
            )
            response.raise_for_status()
            
            return response.json()
        except Exception as e:
            print(f"      ‚ö†Ô∏è Failed to get report describe: {str(e)}")
            return {}


    def _build_field_mappings(self, report_extended_metadata: dict) -> dict:
        """
        Build mapping from report column names to Salesforce field API names.
        
        Uses the reportExtendedMetadata.detailColumnInfo which contains the actual field names.
        
        Args:
            report_extended_metadata: Extended metadata from report response
            
        Returns:
            Dictionary mapping report column names to field API names
        """
        field_mappings = {}
        
        detail_column_info = report_extended_metadata.get('detailColumnInfo', {})
        grouping_column_info = report_extended_metadata.get('groupingColumnInfo', {})
        
        # Map detail columns
        for col_name, col_info in detail_column_info.items():
            # The column name itself often IS the field API name
            # But check if there's a dataType or entityColumnName
            entity_col = col_info.get('entityColumnName', col_name)
            field_mappings[col_name] = entity_col
        
        # Map grouping columns
        for col_name, col_info in grouping_column_info.items():
            entity_col = col_info.get('entityColumnName', col_name)
            field_mappings[col_name] = entity_col
        
        return field_mappings


    def _get_sobject_from_report_type(self, report_type_name: str) -> str:
        """
        Extract the primary SObject from report type name.
        
        Args:
            report_type_name: Report type like "Opportunity" or "OpportunityReport"
            
        Returns:
            SObject API name like "Opportunity"
        """
        # Common report type to SObject mappings
        mappings = {
            'Opportunity': 'Opportunity',
            'OpportunityReport': 'Opportunity',
            'Account': 'Account',
            'AccountReport': 'Account',
            'Contact': 'Contact',
            'ContactReport': 'Contact',
            'Lead': 'Lead',
            'LeadReport': 'Lead',
            'Case': 'Case',
            'CaseReport': 'Case',
            'Task': 'Task',
            'TaskReport': 'Task',
            'Event': 'Event',
            'EventReport': 'Event',
        }
        
        # Try direct lookup
        if report_type_name in mappings:
            return mappings[report_type_name]
        
        # Try to extract base name
        for key, value in mappings.items():
            if key.lower() in report_type_name.lower():
                return value
        
        # Default fallback
        print(f"      ‚ö†Ô∏è Unknown report type: {report_type_name}, defaulting to Opportunity")
        return 'Opportunity'


    def _parse_report_filters(self, report_metadata: dict) -> str:
        """
        Parse report filters and build WHERE clause.
        
        Args:
            report_metadata: Report metadata containing filters
            
        Returns:
            WHERE clause string (without "WHERE" keyword), empty if no filters
        """
        try:
            report_filters = report_metadata.get('reportFilters', [])
            
            if not report_filters:
                return ""
            
            filter_clauses = []
            
            for filter_item in report_filters:
                column = filter_item.get('column')
                operator = filter_item.get('operator')
                value = filter_item.get('value')
                
                if not column or not operator:
                    continue
                
                # Map report operators to SOQL operators
                operator_map = {
                    'equals': '=',
                    'notEqual': '!=',
                    'lessThan': '<',
                    'greaterThan': '>',
                    'lessOrEqual': '<=',
                    'greaterOrEqual': '>=',
                    'contains': 'LIKE',
                    'notContain': 'NOT LIKE',
                    'startsWith': 'LIKE',
                }
                
                soql_operator = operator_map.get(operator, '=')
                
                # Handle different value types
                if value is None or value == '':
                    if operator == 'equals':
                        filter_clauses.append(f"{column} = NULL")
                    elif operator == 'notEqual':
                        filter_clauses.append(f"{column} != NULL")
                    continue
                
                # Handle LIKE operators
                if operator == 'contains':
                    filter_clauses.append(f"{column} LIKE '%{value}%'")
                elif operator == 'startsWith':
                    filter_clauses.append(f"{column} LIKE '{value}%'")
                elif operator == 'notContain':
                    filter_clauses.append(f"{column} NOT LIKE '%{value}%'")
                else:
                    # Quote strings
                    if isinstance(value, str):
                        filter_clauses.append(f"{column} {soql_operator} '{value}'")
                    else:
                        filter_clauses.append(f"{column} {soql_operator} {value}")
            
            return " AND ".join(filter_clauses)
        
        except Exception as e:
            print(f"      ‚ö†Ô∏è Error parsing filters: {str(e)}")
            return ""


    def _fetch_detail_rows_via_soql(self, report_id: str, report_data: dict) -> dict:
        """
        Fetch detail rows for summary report using SOQL query.
        
        This reconstructs the report's underlying query to get individual records.
        
        Args:
            report_id: Report ID
            report_data: Full report data from Analytics API
            
        Returns:
            Dictionary with detail rows grouped by report grouping values
        """
        try:
            print(f"   üîç Fetching detail rows via SOQL...")
            
            report_metadata = report_data.get('reportMetadata', {})
            report_extended_metadata = report_data.get('reportExtendedMetadata', {})
            
            # Get report type and extract SObject
            report_type_metadata = report_metadata.get('reportType', {})
            report_type_name = report_type_metadata.get('type', 'Opportunity')
            
            sobject = self._get_sobject_from_report_type(report_type_name)
            
            print(f"      SObject: {sobject}")
            
            # Get field mappings
            field_mappings = self._build_field_mappings(report_extended_metadata)
            
            # Build SELECT clause
            detail_columns = report_metadata.get('detailColumns', [])
            groupings = report_metadata.get('groupingsDown', [])
            
            select_fields = []
            
            # Add grouping fields first
            for grouping in groupings:
                field_name = grouping.get('name')
                if field_name:
                    mapped_field = field_mappings.get(field_name, field_name)
                    if mapped_field not in select_fields:
                        select_fields.append(mapped_field)
            
            # Add detail columns
            for col in detail_columns:
                mapped_field = field_mappings.get(col, col)
                if mapped_field not in select_fields:
                    select_fields.append(mapped_field)
            
            if not select_fields:
                print(f"      ‚ö†Ô∏è No fields to query")
                return {}
            
            print(f"      Fields to query: {select_fields[:5]}... ({len(select_fields)} total)")
            
            # Build SOQL query
            soql = f"SELECT {', '.join(select_fields)} FROM {sobject}"
            
            # Add WHERE clause from filters
            where_clause = self._parse_report_filters(report_metadata)
            if where_clause:
                soql += f" WHERE {where_clause}"
            
            # Add ORDER BY for grouping
            if groupings:
                group_field = field_mappings.get(groupings[0].get('name'), select_fields[0])
                soql += f" ORDER BY {group_field}"
            
            # Add LIMIT to prevent massive queries
            soql += " LIMIT 2000"
            
            print(f"      SOQL Query:")
            print(f"         {soql[:200]}...")
            
            # Execute query
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": soql}
            
            response = requests.get(
                query_url,
                headers=self.api_headers,
                params=params,
                timeout=90
            )
            response.raise_for_status()
            
            query_data = response.json()
            records = query_data.get('records', [])
            
            print(f"      ‚úÖ Fetched {len(records)} records")
            
            # Group records by grouping field value
            if not groupings or not records:
                return {'__all__': records}
            
            grouped = {}
            grouping_field = field_mappings.get(groupings[0].get('name'))
            
            for record in records:
                # Get grouping value (handle nested fields like Account.Name)
                group_value = record
                for field_part in grouping_field.split('.'):
                    if isinstance(group_value, dict):
                        group_value = group_value.get(field_part)
                    else:
                        break
                
                group_key = str(group_value) if group_value is not None else 'NULL'
                
                if group_key not in grouped:
                    grouped[group_key] = []
                
                grouped[group_key].append(record)
            
            print(f"      ‚úÖ Grouped into {len(grouped)} groups")
            
            return grouped
            
        except Exception as e:
            print(f"      ‚ùå SOQL fetch failed: {str(e)}")
            import traceback
            traceback.print_exc()
            return {}
 
 
 
 
    def _write_tabular_report(self, ws, report_data, detail_columns):
        """Write tabular report (no groupings) - simple table format"""
        from openpyxl.styles import Font, PatternFill, Alignment
        
        print(f"   üî® Building tabular report...")
        
        # Get column labels
        report_extended_metadata = report_data.get('reportExtendedMetadata', {})
        detail_column_info = report_extended_metadata.get('detailColumnInfo', {})
        
        # Write headers
        headers = []
        for col in detail_columns:
            col_info = detail_column_info.get(col, {})
            label = col_info.get('label', col)
            headers.append(label)
        
        for col_idx, header in enumerate(headers, start=1):
            cell = ws.cell(row=1, column=col_idx, value=header)
            cell.font = Font(bold=True, color="FFFFFF")
            cell.fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            cell.alignment = Alignment(horizontal="center", vertical="center")
        
        # Write data rows
        fact_map = report_data.get('factMap', {})
        rows_data = fact_map.get('T!T', {}).get('rows', [])
        
        for row_idx, row_data in enumerate(rows_data, start=2):
            data_cells = row_data.get('dataCells', [])
            for col_idx, cell_data in enumerate(data_cells, start=1):
                value = cell_data.get('label', '')
                ws.cell(row=row_idx, column=col_idx, value=value)
        
        # Auto-adjust columns
        self._auto_adjust_columns(ws)
        ws.freeze_panes = "A2"
        
        print(f"   ‚úÖ Tabular report complete: {len(rows_data)} data rows")


 
    def _write_summary_report(self, ws, report_data, groupings_down, aggregate_columns):
        """
        Write summary report with groupings, detail rows, and indentation.
        
        ‚úÖ DEBUG VERSION: Prints detailed information about what's happening
        """
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        
        print(f"\n{'='*70}")
        print(f"üî® _WRITE_SUMMARY_REPORT CALLED")
        print(f"{'='*70}")
        
        # Get metadata
        report_metadata = report_data.get('reportMetadata', {})
        report_extended_metadata = report_data.get('reportExtendedMetadata', {})
        grouping_column_info = report_extended_metadata.get('groupingColumnInfo', {})
        aggregate_column_info = report_extended_metadata.get('aggregateColumnInfo', {})
        detail_column_info = report_extended_metadata.get('detailColumnInfo', {})
        
        # Get detail columns
        detail_columns = report_metadata.get('detailColumns', [])
        
        print(f"üìã Report Metadata:")
        print(f"   Detail columns: {detail_columns}")
        print(f"   Groupings: {[g.get('name') for g in groupings_down]}")
        print(f"   Aggregates: {aggregate_columns}")
        
        # Check if API returned detail rows
        fact_map = report_data.get('factMap', {})
        
        print(f"\nüîç Checking factMap for detail rows...")
        api_has_details = False
        for key, fact in list(fact_map.items())[:5]:  # Check first 5
            rows = fact.get('rows', [])
            if rows:
                api_has_details = True
                print(f"   ‚úÖ {key}: {len(rows)} rows")
            else:
                print(f"   ‚ùå {key}: 0 rows")
        
        print(f"\nüìä API has detail rows: {api_has_details}")
        
        # ‚úÖ CRITICAL: Fetch detail rows via SOQL if API didn't provide them
        soql_records = {}
        
        if not api_has_details and detail_columns:
            print(f"\n{'='*70}")
            print(f"‚ö†Ô∏è API DIDN'T RETURN DETAILS - FETCHING VIA SOQL")
            print(f"{'='*70}")
            
            try:
                # Get report ID
                report_id = report_metadata.get('id', '')
                print(f"   Report ID: {report_id}")
                
                if not report_id:
                    print(f"   ‚ùå ERROR: No report ID found in metadata!")
                    print(f"   Available metadata keys: {list(report_metadata.keys())}")
                else:
                    print(f"   üìû Calling _fetch_detail_rows_via_soql()...")
                    
                    soql_records = self._fetch_detail_rows_via_soql(report_id, report_data)
                    
                    print(f"\n   üì• SOQL Results:")
                    print(f"      Keys returned: {list(soql_records.keys())}")
                    
                    total_records = sum(len(recs) for recs in soql_records.values())
                    print(f"      Total records: {total_records}")
                    
                    if total_records == 0:
                        print(f"      ‚ùå WARNING: SOQL returned 0 records!")
                    else:
                        print(f"      ‚úÖ SOQL returned {total_records} records")
                        
                        # Show sample
                        for key, records in list(soql_records.items())[:3]:
                            print(f"         {key}: {len(records)} records")
                            if records:
                                print(f"            Sample: {list(records[0].keys())[:5]}")
            
            except Exception as e:
                print(f"\n   ‚ùå EXCEPTION during SOQL fetch:")
                print(f"      {type(e).__name__}: {str(e)}")
                import traceback
                traceback.print_exc()
        else:
            print(f"\n‚úÖ Using API-provided detail rows (no SOQL needed)")
        
        print(f"\n{'='*70}")
        print(f"üìù Starting Excel Write")
        print(f"{'='*70}\n")
        
        current_row = 1
        
        # ===== WRITE HEADERS =====
        headers = []
        
        # Add grouping column headers
        for grouping in groupings_down:
            col_name = grouping.get('name')
            col_info = grouping_column_info.get(col_name, {})
            label = col_info.get('label', col_name)
            headers.append(label)
        
        # Add detail column headers
        has_detail_columns = len(detail_columns) > 0
        if has_detail_columns:
            for detail_col in detail_columns:
                col_info = detail_column_info.get(detail_col, {})
                label = col_info.get('label', detail_col)
                headers.append(label)
        
        # Add aggregate column headers
        for agg_col in aggregate_columns:
            col_info = aggregate_column_info.get(agg_col, {})
            label = col_info.get('label', agg_col)
            headers.append(label)
        
        # Write header row
        for col_idx, header in enumerate(headers, start=1):
            cell = ws.cell(row=current_row, column=col_idx, value=header)
            cell.font = Font(bold=True, color="FFFFFF")
            cell.fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            cell.alignment = Alignment(horizontal="center", vertical="center")
        
        current_row += 1
        
        print(f"‚úÖ Headers written: {headers}\n")
        
        # ===== WRITE GROUPED DATA WITH DETAIL ROWS =====
        groupings_data = report_data.get('groupingsDown', {}).get('groupings', [])
        
        if not groupings_data:
            print(f"‚ùå No grouping data found!")
            return
        
        print(f"üìä Processing {len(groupings_data)} top-level groups...\n")
        
        # Build field mappings for SOQL records
        field_mappings = {}
        if soql_records:
            print(f"üó∫Ô∏è Building field mappings...")
            field_mappings = self._build_field_mappings(report_extended_metadata)
            print(f"   Mappings: {field_mappings}\n")
        
        def write_grouping_level(groupings, level=0):
            """Recursively write groupings with detail rows and summaries"""
            nonlocal current_row
            
            for group_idx, grouping in enumerate(groupings):
                label = grouping.get('label', '')
                key = grouping.get('key', '')
                
                print(f"{'  '*level}üìÅ Group: {label} (key: {key})")
                
                # Create indentation
                indent = "  " * level
                display_label = f"{indent}{label}"
                
                # ===== WRITE GROUP HEADER =====
                cell = ws.cell(row=current_row, column=1, value=display_label)
                cell.font = Font(bold=True, color="1F4E78")
                cell.fill = PatternFill(start_color="E7E6E6", end_color="E7E6E6", fill_type="solid")
                
                current_row += 1
                
                # ===== WRITE DETAIL ROWS =====
                detail_rows = []

                # ‚úÖ FIX: Salesforce factMap uses "key!T" format, not just "key"
                # Groups have keys like "0", but factMap stores them as "0!T"
                if key:
                    fact_map_key = f"{key}!T"
                else:
                    fact_map_key = key

                # Try to get from API first
                fact_data = fact_map.get(fact_map_key, {})
                api_rows = fact_data.get('rows', [])
                
                print(f"{'  '*(level+1)}üîç DEBUG: fact_map_key='{fact_map_key}', fact_data keys={list(fact_data.keys())}, rows={len(api_rows)}")
                
                if api_rows:
                    # API provided rows
                    detail_rows = api_rows
                    print(f"{'  '*(level+1)}‚úÖ Using {len(detail_rows)} API rows")
                
                elif soql_records:
                    # Use SOQL-fetched records
                    print(f"{'  '*(level+1)}üîç Looking for SOQL records...")
                    print(f"{'  '*(level+1)}   Available keys: {list(soql_records.keys())}")
                    
                    # Match by group label
                    soql_group_records = soql_records.get(label, [])
                    
                    if not soql_group_records and '__all__' in soql_records:
                        print(f"{'  '*(level+1)}   Trying to filter from '__all__'")
                        
                        # Filter from all records
                        grouping_field = field_mappings.get(groupings_down[0].get('name'))
                        print(f"{'  '*(level+1)}   Grouping field: {grouping_field}")
                        
                        soql_group_records = [
                            r for r in soql_records['__all__']
                            if self._get_nested_value(r, grouping_field) == label
                        ]
                        
                        print(f"{'  '*(level+1)}   Filtered: {len(soql_group_records)} records")
                    
                    if soql_group_records:
                        print(f"{'  '*(level+1)}‚úÖ Using {len(soql_group_records)} SOQL rows")
                        
                        # Convert SOQL records to row format
                        detail_rows = self._convert_soql_to_rows(
                            soql_group_records,
                            detail_columns,
                            field_mappings
                        )
                        
                        print(f"{'  '*(level+1)}   Converted to {len(detail_rows)} rows")
                    else:
                        print(f"{'  '*(level+1)}‚ùå No SOQL records found for this group")
                
                # Write detail rows
                if detail_rows:
                    print(f"{'  '*(level+1)}üìù Writing {len(detail_rows)} detail rows...")
                    
                    for row_idx, row_data in enumerate(detail_rows):
                        # Handle both API format and converted SOQL format
                        if isinstance(row_data, dict) and 'dataCells' in row_data:
                            # API format
                            data_cells = row_data.get('dataCells', [])
                            values = [cell.get('label', '') for cell in data_cells]
                        else:
                            # Converted SOQL format (list of values)
                            values = row_data
                        
                        # Write row with indentation
                        col_idx = 1
                        
                        # First column: indent
                        indent_spaces = "    " * (level + 1)
                        ws.cell(row=current_row, column=col_idx, value=indent_spaces)
                        col_idx += 1
                        
                        # Detail columns
                        for value in values:
                            ws.cell(row=current_row, column=col_idx, value=value)
                            col_idx += 1
                        
                        current_row += 1
                    
                    print(f"{'  '*(level+1)}‚úÖ Wrote {len(detail_rows)} rows")
                else:
                    print(f"{'  '*(level+1)}‚ö†Ô∏è No detail rows to write")
                
                # ===== WRITE SUMMARY ROW (SUBTOTAL) =====
                aggregates = fact_data.get('aggregates', [])
                
                if aggregates:
                    # Subtotal label
                    subtotal_label = f"{indent}  Total {label}"
                    cell = ws.cell(row=current_row, column=1, value=subtotal_label)
                    cell.font = Font(bold=True, color="000000")
                    cell.fill = PatternFill(start_color="F2F2F2", end_color="F2F2F2", fill_type="solid")
                    
                    # Calculate column position for aggregates
                    agg_col_start = 1 + len(groupings_down)
                    if has_detail_columns:
                        agg_col_start += len(detail_columns)
                    
                    # Write aggregate values
                    for agg_idx, agg_value in enumerate(aggregates):
                        value = agg_value.get('label', '')
                        cell = ws.cell(row=current_row, column=agg_col_start + agg_idx, value=value)
                        cell.font = Font(bold=True)
                        cell.fill = PatternFill(start_color="F2F2F2", end_color="F2F2F2", fill_type="solid")
                    
                    current_row += 1
                    print(f"{'  '*(level+1)}‚úÖ Wrote subtotal")
                
                # ===== RECURSIVELY WRITE SUB-GROUPINGS =====
                sub_groupings = grouping.get('groupings', [])
                if sub_groupings:
                    print(f"{'  '*(level+1)}‚Üì Processing {len(sub_groupings)} sub-groups...")
                    write_grouping_level(sub_groupings, level + 1)
        
        # Write all groupings
        write_grouping_level(groupings_data)
        
        # ===== WRITE GRAND TOTAL =====
        print(f"\nüìä Writing Grand Total...")
        grand_total_fact = fact_map.get('T!T', {})
        if grand_total_fact:
            ws.cell(row=current_row, column=1, value="Grand Total")
            cell = ws.cell(row=current_row, column=1)
            cell.font = Font(bold=True, size=11)
            cell.fill = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")
            
            aggregates = grand_total_fact.get('aggregates', [])
            
            # Calculate column position
            agg_col_start = 1 + len(groupings_down)
            if has_detail_columns:
                agg_col_start += len(detail_columns)
            
            for agg_idx, agg_value in enumerate(aggregates):
                value = agg_value.get('label', '')
                cell = ws.cell(row=current_row, column=agg_col_start + agg_idx, value=value)
                cell.font = Font(bold=True, size=11)
                cell.fill = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")
            
            print(f"‚úÖ Grand Total written at row {current_row}")
        
        # Auto-adjust columns
        self._auto_adjust_columns(ws)
        
        # Freeze header row
        ws.freeze_panes = "A2"
        
        print(f"\n{'='*70}")
        print(f"‚úÖ SUMMARY REPORT COMPLETE: {current_row} rows written")
        print(f"{'='*70}\n")
        
    

    def _get_nested_value(self, record: dict, field_path: str):
        """
        Get value from nested field path like 'Account.Name'
        
        Args:
            record: Salesforce record
            field_path: Field path (e.g., 'Account.Name' or 'Name')
            
        Returns:
            Field value or None
        """
        if not field_path:
            return None
        
        value = record
        for field_part in field_path.split('.'):
            if isinstance(value, dict):
                value = value.get(field_part)
            else:
                return None
        
        return value


    def _convert_soql_to_rows(self, soql_records: list, detail_columns: list, field_mappings: dict) -> list:
        """
        Convert SOQL records to row format compatible with Excel writer.
        
        Args:
            soql_records: List of Salesforce records from SOQL query
            detail_columns: List of detail column names from report
            field_mappings: Mapping of column names to field API names
            
        Returns:
            List of value lists for each row
        """
        rows = []
        
        for record in soql_records:
            row_values = []
            
            for col_name in detail_columns:
                field_path = field_mappings.get(col_name, col_name)
                value = self._get_nested_value(record, field_path)
                
                # Format value
                if value is None:
                    display_value = ''
                elif isinstance(value, (int, float)):
                    display_value = value
                else:
                    display_value = str(value)
                
                row_values.append(display_value)
            
            rows.append(row_values)
        
        return rows
    
    
    
    def _auto_adjust_columns(self, ws):
        """Auto-adjust column widths based on content"""
        for column in ws.columns:
            max_length = 0
            column_letter = column[0].column_letter
            
            for cell in column:
                try:
                    if cell.value:
                        max_length = max(max_length, len(str(cell.value)))
                except:
                    pass
            
            adjusted_width = min(max_length + 2, 50)
            ws.column_dimensions[column_letter].width = adjusted_width


    def _write_matrix_report(self, ws, report_data, groupings_down, groupings_across):
        """Write matrix report (crosstab format) - NOT FULLY IMPLEMENTED YET"""
        from openpyxl.styles import Font
        
        print(f"   ‚ö†Ô∏è Matrix reports are not fully supported")
        
        ws.cell(row=1, column=1, value="Matrix Report Export")
        ws.cell(row=1, column=1).font = Font(bold=True, size=14)
        
        ws.cell(row=3, column=1, value="‚ö†Ô∏è This report type is not yet supported")
        ws.cell(row=4, column=1, value="Please use CSV export or open the report in Salesforce")
        
        ws.cell(row=6, column=1, value="Report ID:")
        ws.cell(row=6, column=2, value=report_data.get('reportMetadata', {}).get('id', 'Unknown'))







    def export_selected_reports_to_zip_concurrent_excel(
        self,
        output_zip_path: str,
        report_ids: List[str],
        max_workers: int = 10,
        cancel_event: Optional[Any] = None,
        retry_attempts: int = 3,
        reports_metadata: Optional[Dict[str, Dict]] = None
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to formatted Excel using Analytics API.
        
        ‚úÖ This method preserves ALL Salesforce UI formatting:
        ‚Ä¢ Report structure (TABULAR, SUMMARY, MATRIX)
        ‚Ä¢ Row groupings and indentation
        ‚Ä¢ Summary rows (subtotals, grand totals)
        ‚Ä¢ Excel formatting (colors, borders, alignment)
        
        Args:
            output_zip_path: Path where ZIP file will be saved
            report_ids: List of report IDs to export
            max_workers: Number of parallel downloads (default 10)
            cancel_event: Threading event to signal cancellation
            retry_attempts: Number of retry attempts for failed reports
            reports_metadata: Optional dict of {report_id: {name, format}} to skip metadata fetch
            
        Returns:
            Dictionary with export results
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_excel_"))
        
        try:
            # ===== STEP 1: Get/validate metadata =====
            print(f"üìä Preparing to export {len(report_ids)} reports as formatted Excel...")
            
            # ‚úÖ Use provided metadata if available, else fetch
            if reports_metadata:
                print("‚ö° Using cached metadata (skipping API calls)")
                reports = []
                for report_id in report_ids:
                    if report_id in reports_metadata:
                        reports.append(reports_metadata[report_id])
                    else:
                        reports.append({
                            "id": report_id,
                            "name": report_id,
                            "reportFormat": "TABULAR"
                        })
            else:
                print("üîç Fetching report metadata...")
                if not report_ids:
                    reports = []
                else:
                    chunk_size = 50
                    reports = []
                    
                    for i in range(0, len(report_ids), chunk_size):
                        if cancel_event and cancel_event.is_set():
                            raise Exception("Export cancelled by user")
                        
                        chunk_ids = report_ids[i:i + chunk_size]
                        ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                        
                        base_query = f"""
                            SELECT Id, Name, Format 
                            FROM Report 
                            WHERE Id IN ({ids_formatted})
                        """
                        
                        try:
                            chunk_records = self._query_with_pagination(
                                base_query.strip(), 
                                batch_size=2000,
                                cancel_event=cancel_event
                            )
                            
                            for record in chunk_records:
                                reports.append({
                                    "id": record.get("Id"),
                                    "name": record.get("Name"),
                                    "reportFormat": record.get("Format", "TABULAR")
                                })
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error fetching report chunk {i//chunk_size + 1}: {str(e)[:100]}")
                            for rid in chunk_ids:
                                reports.append({
                                    "id": rid,
                                    "name": rid,
                                    "reportFormat": "TABULAR"
                                })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []
            used_filenames: Dict[str, int] = {}
            
            # Thread-safe counters
            completed_lock = threading.Lock()
            
            # Adaptive worker count
            if total > 5000:
                max_workers = min(max_workers, 8)
                print(f"‚öôÔ∏è Large export detected ({total} reports), using {max_workers} workers")
            elif total > 1000:
                max_workers = min(max_workers, 10)
                print(f"‚öôÔ∏è Using {max_workers} workers for {total} reports")
            
            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports (Excel)",
                    "api_version": self.api_version,
                    "cancelled": False,
                    "completed": 0
                }
            
            # ===== STEP 2: Define worker function =====
            def export_single_report_excel(report: Dict) -> tuple:
                """Export a single report as formatted Excel - runs in thread pool"""
                nonlocal completed
                
                if cancel_event and cancel_event.is_set():
                    return ("cancelled", report, None)
                
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")
                
                # Notify: Starting download
                if self.progress_callback:
                    try:
                        with completed_lock:
                            current_count = completed
                        self.progress_callback(current_count, total, report_name)
                    except:
                        pass
                
                # Generate filename (thread-safe)
                base_name = safe_filename(report_name)

                with completed_lock:
                    if base_name in used_filenames:
                        used_filenames[base_name] += 1
                        filename = f"{base_name}_{used_filenames[base_name]}.xlsx"
                    else:
                        used_filenames[base_name] = 1
                        filename = f"{base_name}.xlsx"

                excel_path = tmp_dir / filename
                
                # Retry logic with exponential backoff
                last_error = None
                for attempt in range(retry_attempts):
                    if cancel_event and cancel_event.is_set():
                        return ("cancelled", report, None)
                    
                    try:
                        # Adaptive timeout
                        timeout = 180 if total > 5000 else 120
                        
                        # ‚úÖ FIXED: Only use Analytics API (no fallback)
                        print(f"üìä Exporting with formatting: {report_name}")
                        excel_content = self.export_report_with_formatting(report_id, timeout=timeout)
                        
                        if not excel_content:
                            raise Exception("Empty response from Analytics API")
                        
                        # Write to file
                        excel_path.write_bytes(excel_content)
                        
                        # ‚úÖ FIXED: Removed export_method variable
                        print(f"‚úÖ Saved: {filename} ({len(excel_content)} bytes)")
                        
                        # Success! Increment counter
                        with completed_lock:
                            completed += 1
                            current_count = completed
                        
                        # Notify: Report completed successfully
                        if self.progress_callback:
                            try:
                                self.progress_callback(current_count, total)
                            except:
                                pass
                        
                        return ("success", report, filename)
                        
                    except Exception as e:
                        last_error = str(e)
                        
                        # Check if it's a known unsupported report type
                        if "not supported" in last_error.lower() or "matrix" in last_error.lower():
                            print(f"‚ö†Ô∏è Report type not supported: {report_name} ({report_type})")
                            break  # Don't retry for unsupported types
                        
                        if attempt < retry_attempts - 1:
                            wait_time = (2 ** attempt) + (attempt * 0.5)
                            print(f"‚ö†Ô∏è Retry {attempt + 1}/{retry_attempts} for {report_name} after {wait_time}s")
                            time.sleep(wait_time)
                            continue
                        else:
                            break
                
                # Failed after all retries
                error_content = (
                    f"# Failed to export report after {retry_attempts} attempts\n"
                    f"# Report Name: {report_name}\n"
                    f"# Report ID: {report_id}\n"
                    f"# Report Type: {report_type}\n"
                    f"# Error: {last_error}\n"
                    f"#\n"
                    f"# Note: This report may be a MATRIX or JOINED report type\n"
                    f"# which is not yet supported by the Analytics API.\n"
                    f"# Only TABULAR and SUMMARY reports preserve formatting.\n"
                )
                
                # Create error file
                error_path = tmp_dir / f"{base_name}_ERROR.txt"
                error_path.write_text(error_content, encoding="utf-8")
                
                # Increment counter
                with completed_lock:
                    completed += 1
                    current_count = completed
                
                # Notify: Report failed
                if self.progress_callback:
                    try:
                        self.progress_callback(current_count, total)
                    except:
                        pass
                
                return ("failed", report, last_error)
            
            # ===== STEP 3: Export reports concurrently =====
            print(f"üöÄ Starting concurrent export with {max_workers} workers...")
            print(f"üí° Using Salesforce Analytics API")
            print(f"‚úÖ Preserves groupings, summaries, and indentation")
            print(f"üìä Report structure will match Salesforce UI exactly")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all tasks
                future_to_report = {
                    executor.submit(export_single_report_excel, report): report 
                    for report in reports
                }
                
                # Track progress milestones
                last_milestone = 0
                milestone_interval = max(100, total // 20)
                
                # Process completed tasks
                for future in as_completed(future_to_report):
                    if cancel_event and cancel_event.is_set():
                        print("‚ö†Ô∏è Cancellation detected, stopping remaining downloads...")
                        for f in future_to_report:
                            f.cancel()
                        break
                    
                    try:
                        status, report, data = future.result()
                        
                        if status == "success":
                            successful.append(report.get("name"))
                        elif status == "failed":
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": data
                            })
                        elif status == "cancelled":
                            pass
                        
                        # Log progress milestones
                        if completed - last_milestone >= milestone_interval:
                            success_rate = (len(successful) / completed * 100) if completed > 0 else 0
                            print(f"üìä Progress: {completed}/{total} ({completed/total*100:.1f}%) - Success rate: {success_rate:.1f}%")
                            last_milestone = completed
                        
                        # Update progress callback
                        if self.progress_callback:
                            try:
                                self.progress_callback(completed, total)
                            except Exception:
                                pass
                                
                    except Exception as e:
                        report = future_to_report.get(future)
                        if report:
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": str(e)
                            })
                            print(f"‚ö†Ô∏è Future error for {report.get('name')}: {str(e)[:100]}")
            
            # Check if cancelled
            was_cancelled = cancel_event and cancel_event.is_set()
            
            # ===== STEP 4: Create ZIP file =====
            print(f"üì¶ Creating ZIP file with {completed} formatted Excel reports...")
            
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                # Write files in sorted order
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                # Create summary
                summary = self._create_summary(
                    total, 
                    successful, 
                    failed, 
                    "Selected Reports (Formatted Excel)" + (" (CANCELLED)" if was_cancelled else "")
                )
                
                # ‚úÖ FIXED: Updated summary text
                summary += "\n\n"
                summary += "=" * 50 + "\n"
                summary += "EXCEL EXPORT FORMAT INFORMATION\n"
                summary += "=" * 50 + "\n"
                summary += "Export Method: Salesforce Analytics API\n"
                summary += "\n"
                summary += "‚úÖ This export preserves ALL Salesforce UI formatting:\n"
                summary += "  ‚Ä¢ Report structure (TABULAR, SUMMARY, MATRIX)\n"
                summary += "  ‚Ä¢ Row groupings (Group By columns)\n"
                summary += "  ‚Ä¢ Indentation of grouped rows\n"
                summary += "  ‚Ä¢ Summary rows (subtotals, grand totals)\n"
                summary += "  ‚Ä¢ Excel formatting (colors, borders, alignment)\n"
                summary += "  ‚Ä¢ Column formatting and data types\n"
                summary += "\n"
                summary += "üìÑ File Format: .xlsx (Microsoft Excel 2007+)\n"
                summary += "üìä Method: Analytics API ‚Üí Direct XLSX (preserves structure)\n"
                summary += "\n"
                summary += "‚ö†Ô∏è Limitations:\n"
                summary += "  ‚Ä¢ MATRIX and JOINED reports are not yet supported\n"
                summary += "  ‚Ä¢ Only TABULAR and SUMMARY reports preserve formatting\n"
                summary += "  ‚Ä¢ Unsupported reports will show error files\n"
                summary += "\n"
                summary += "üíæ Files are true .xlsx format (no security warnings)\n"
                summary += "=" * 50 + "\n"
                
                zf.writestr("_EXPORT_SUMMARY.txt", summary)
            
            # Final statistics
            success_rate = (len(successful) / total * 100) if total > 0 else 0
            print(f"‚úÖ Formatted Excel export complete: {len(successful)}/{total} successful ({success_rate:.1f}%)")
            if failed:
                print(f"‚ö†Ô∏è Failed: {len(failed)} reports")
            
            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports (Formatted Excel)",
                "api_version": self.api_version,
                "cancelled": was_cancelled,
                "completed": completed
            }
        
        finally:
            # Cleanup temporary files
            try:
                shutil.rmtree(tmp_dir)
                print(f"üßπ Cleaned up temporary files")
            except Exception as e:
                print(f"‚ö†Ô∏è Error cleaning temp directory: {str(e)[:100]}")



    


    
    
    # exporter.py - Part 3: Export Methods
# This continues the SalesforceReportExporter class

    # Add these methods to the SalesforceReportExporter class

    def export_reports_by_folder_to_zip(
        self,
        output_zip_path: str,
        folder_id: str,
        delay_between_reports: float = 1.0
    ) -> Dict[str, Any]:
        """
        Export all reports from a specific folder to a ZIP file.
        
        Args:
            output_zip_path: Path where ZIP file will be saved
            folder_id: The Salesforce folder ID to export reports from
            delay_between_reports: Seconds to wait between exports (rate limiting)
            
        Returns:
            Dictionary with export results
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # Step 1: Get reports from this folder
            reports = self.list_reports(folder_id=folder_id)
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            # Get folder name
            folder_name = self._get_folder_name(folder_id)

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", f"No reports found in folder: {folder_name}")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": folder_name,
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, folder_name)
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": folder_name,
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass

    def export_all_reports_to_zip(
        self,
        output_zip_path: str,
        delay_between_reports: float = 0.5
    ) -> Dict[str, Any]:
        """
        Export ALL reports from ALL folders to a ZIP file.
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # Step 1: Get list of all reports
            reports = self.list_reports()
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found in this Salesforce org.")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "All Folders",
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, "All Folders")
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "All Folders",
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass
    
    # exporter.py - Part 4: Selected Reports Export & Helper Methods
# This completes the SalesforceReportExporter class

    # Add these final methods to the SalesforceReportExporter class

    def export_selected_reports_to_zip(
        self,
        output_zip_path: str,
        report_ids: List[str],
        delay_between_reports: float = 0.5
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to a ZIP file.
        Now uses direct SOQL to ensure system/automated reports are found.
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # --- IMPROVED: Direct SOQL lookup with pagination ---
            if not report_ids:
                reports = []
            else:
                # Split into chunks of 100 IDs (SOQL IN clause limit is ~1000 chars)
                chunk_size = 100
                reports = []
                
                for i in range(0, len(report_ids), chunk_size):
                    chunk_ids = report_ids[i:i + chunk_size]
                    ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                    
                    base_query = f"""
                        SELECT Id, Name, Format 
                        FROM Report 
                        WHERE Id IN ({ids_formatted})
                    """
                    
                    try:
                        # Use pagination helper (though with 100 IDs we won't need pagination)
                        chunk_records = self._query_with_pagination(base_query.strip(), batch_size=2000)
                        
                        for record in chunk_records:
                            reports.append({
                                "id": record.get("Id"),
                                "name": record.get("Name"),
                                "reportFormat": record.get("Format", "TABULAR")
                            })
                    except Exception as e:
                        print(f"Error fetching report chunk: {str(e)}")
                        # Fallback: create entries with just IDs
                        for rid in chunk_ids:
                            reports.append({
                                "id": rid,
                                "name": rid,
                                "reportFormat": "TABULAR"
                            })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports",
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each selected report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, "Selected Reports")
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports",
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass
    

    def export_selected_reports_to_zip_concurrent(
        self,
        output_zip_path: str,
        report_ids: List[str],
        max_workers: int = 10,  # ‚úÖ INCREASED: Default 10 workers for faster exports
        cancel_event: Optional[Any] = None,
        retry_attempts: int = 3,
        reports_metadata: Optional[Dict[str, Dict]] = None
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to a ZIP file using CONCURRENT downloads.
        
        ‚úÖ OPTIMIZED: Better memory management + adaptive worker count for 10,000+ reports.
        
        Args:
            output_zip_path: Path where ZIP file will be saved
            report_ids: List of report IDs to export
            max_workers: Number of parallel downloads (default 10)
            cancel_event: Threading event to signal cancellation
            retry_attempts: Number of retry attempts for failed reports
            reports_metadata: Optional dict of {report_id: {name, format}} to skip metadata fetch
            
        Returns:
            Dictionary with export results
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))
        
        try:
            # ===== STEP 1: Get/validate metadata =====
            print(f"üìä Preparing to export {len(report_ids)} reports...")
            
            # ‚úÖ OPTIMIZED: Use provided metadata if available, else fetch
            if reports_metadata:
                print("‚ö° Using cached metadata (skipping API calls)")
                reports = []
                for report_id in report_ids:
                    if report_id in reports_metadata:
                        reports.append(reports_metadata[report_id])
                    else:
                        # Fallback: create basic entry
                        reports.append({
                            "id": report_id,
                            "name": report_id,
                            "reportFormat": "TABULAR"
                        })
            else:
                print("üîç Fetching report metadata...")
                # Fallback: Fetch metadata if not provided
                if not report_ids:
                    reports = []
                else:
                    # ‚úÖ OPTIMIZED: Smaller chunks for stability
                    chunk_size = 50  # Reduced from 100
                    reports = []
                    
                    for i in range(0, len(report_ids), chunk_size):
                        # Check for cancellation
                        if cancel_event and cancel_event.is_set():
                            raise Exception("Export cancelled by user")
                        
                        chunk_ids = report_ids[i:i + chunk_size]
                        ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                        
                        base_query = f"""
                            SELECT Id, Name, Format 
                            FROM Report 
                            WHERE Id IN ({ids_formatted})
                        """
                        
                        try:
                            chunk_records = self._query_with_pagination(
                                base_query.strip(), 
                                batch_size=2000,
                                cancel_event=cancel_event
                            )
                            
                            for record in chunk_records:
                                reports.append({
                                    "id": record.get("Id"),
                                    "name": record.get("Name"),
                                    "reportFormat": record.get("Format", "TABULAR")
                                })
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error fetching report chunk {i//chunk_size + 1}: {str(e)[:100]}")
                            # Fallback: create entries with just IDs
                            for rid in chunk_ids:
                                reports.append({
                                    "id": rid,
                                    "name": rid,
                                    "reportFormat": "TABULAR"
                                })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []
            used_filenames: Dict[str, int] = {}
            
            # Thread-safe counters
            completed_lock = threading.Lock()
            
            # ‚úÖ NEW: Adaptive worker count based on total reports
            if total > 5000:
                max_workers = min(max_workers, 8)  # Reduce workers for very large exports
                print(f"‚öôÔ∏è Large export detected ({total} reports), using {max_workers} workers")
            elif total > 1000:
                max_workers = min(max_workers, 10)
                print(f"‚öôÔ∏è Using {max_workers} workers for {total} reports")
            
            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports",
                    "api_version": self.api_version,
                    "cancelled": False,
                    "completed": 0
                }
            
            # ===== STEP 2: Define worker function =====
            def export_single_report(report: Dict) -> tuple:
                """Export a single report - runs in thread pool"""
                nonlocal completed
                
                # Check for cancellation
                if cancel_event and cancel_event.is_set():
                    return ("cancelled", report, None)
                
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")
                
                # ‚úÖ NOTIFY: Starting download of this report
                if self.progress_callback:
                    try:
                        with completed_lock:
                            current_count = completed
                        # Signal: download starting (with report name)
                        self.progress_callback(current_count, total, report_name)
                    except:
                        pass
                
                # Generate filename (thread-safe)
                base_name = safe_filename(report_name)
                
                with completed_lock:
                    if base_name in used_filenames:
                        used_filenames[base_name] += 1
                        filename = f"{base_name}_{used_filenames[base_name]}.csv"
                    else:
                        used_filenames[base_name] = 1
                        filename = f"{base_name}.csv"
                
                csv_path = tmp_dir / filename
                
                # Retry logic with exponential backoff
                last_error = None
                for attempt in range(retry_attempts):
                    # Check cancellation before each attempt
                    if cancel_event and cancel_event.is_set():
                        return ("cancelled", report, None)
                    
                    try:
                        # ‚úÖ IMPROVED: Adaptive timeout based on total export size
                        timeout = 180 if total > 5000 else 120
                        
                        csv_content = self.export_report_csv(report_id, timeout=timeout)
                        
                        if not csv_content or len(csv_content.strip()) == 0:
                            raise Exception("Empty response received")
                        
                        first_line = csv_content.split('\n')[0] if csv_content else ""
                        if 'Error' in first_line and len(csv_content) < 500:
                            raise Exception(f"Salesforce error: {first_line[:100]}")
                        
                        csv_path.write_text(csv_content, encoding="utf-8")
                        
                        # Success! Increment counter FIRST
                        with completed_lock:
                            completed += 1
                            current_count = completed
                        
                        # ‚úÖ NOTIFY: Report completed successfully
                        if self.progress_callback:
                            try:
                                self.progress_callback(current_count, total)
                            except:
                                pass
                        
                        return ("success", report, filename)
                        
                    except Exception as e:
                        last_error = str(e)
                        if attempt < retry_attempts - 1:
                            # ‚úÖ IMPROVED: Exponential backoff with jitter
                            wait_time = (2 ** attempt) + (attempt * 0.5)  # 1s, 2.5s, 5s
                            time.sleep(wait_time)
                            continue
                        else:
                            # All retries failed
                            break
                
                # Failed after all retries
                error_content = (
                    f"# Failed to export report after {retry_attempts} attempts\n"
                    f"# Report Name: {report_name}\n"
                    f"# Report ID: {report_id}\n"
                    f"# Report Type: {report_type}\n"
                    f"# Error: {last_error}\n"
                )
                csv_path.write_text(error_content, encoding="utf-8")

                # Increment counter FIRST
                with completed_lock:
                    completed += 1
                    current_count = completed

                # ‚úÖ NOTIFY: Report failed (but counted as completed)
                if self.progress_callback:
                    try:
                        self.progress_callback(current_count, total)
                    except:
                        pass

                return ("failed", report, last_error)
            
            # ===== STEP 3: Export reports concurrently =====
            print(f"üöÄ Starting concurrent export with {max_workers} workers...")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all tasks
                future_to_report = {
                    executor.submit(export_single_report, report): report 
                    for report in reports
                }
                
                # ‚úÖ NEW: Track progress milestones
                last_milestone = 0
                milestone_interval = max(100, total // 20)  # Log every 5%
                
                # Process completed tasks
                for future in as_completed(future_to_report):
                    # Check for cancellation
                    if cancel_event and cancel_event.is_set():
                        print("‚ö†Ô∏è Cancellation detected, stopping remaining downloads...")
                        # Cancel remaining futures
                        for f in future_to_report:
                            f.cancel()
                        break
                    
                    try:
                        status, report, data = future.result()
                        
                        if status == "success":
                            successful.append(report.get("name"))
                        elif status == "failed":
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": data
                            })
                        elif status == "cancelled":
                            # Don't count as failed, just stopped
                            pass
                        
                        # ‚úÖ NEW: Log progress milestones
                        if completed - last_milestone >= milestone_interval:
                            success_rate = (len(successful) / completed * 100) if completed > 0 else 0
                            print(f"üìä Progress: {completed}/{total} ({completed/total*100:.1f}%) - Success rate: {success_rate:.1f}%")
                            last_milestone = completed
                        
                        # Update progress callback
                        if self.progress_callback:
                            try:
                                self.progress_callback(completed, total)
                            except Exception:
                                pass
                                
                    except Exception as e:
                        # Future itself failed
                        report = future_to_report.get(future)
                        if report:
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": str(e)
                            })
                            print(f"‚ö†Ô∏è Future error for {report.get('name')}: {str(e)[:100]}")
            
            # Check if cancelled
            was_cancelled = cancel_event and cancel_event.is_set()
            
            # ===== STEP 4: Create ZIP file =====
            print(f"üì¶ Creating ZIP file with {completed} reports...")
            
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                # ‚úÖ OPTIMIZED: Write files in sorted order for consistent ZIP structure
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(
                    total, 
                    successful, 
                    failed, 
                    "Selected Reports" + (" (CANCELLED)" if was_cancelled else "")
                )
                zf.writestr("_EXPORT_SUMMARY.txt", summary)
            
            # ‚úÖ NEW: Final statistics
            success_rate = (len(successful) / total * 100) if total > 0 else 0
            print(f"‚úÖ Export complete: {len(successful)}/{total} successful ({success_rate:.1f}%)")
            if failed:
                print(f"‚ö†Ô∏è Failed: {len(failed)} reports")
            
            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports",
                "api_version": self.api_version,
                "cancelled": was_cancelled,
                "completed": completed
            }
        
        finally:
            # ‚úÖ IMPROVED: Better cleanup with error handling
            try:
                shutil.rmtree(tmp_dir)
                print(f"üßπ Cleaned up temporary files")
            except Exception as e:
                print(f"‚ö†Ô∏è Error cleaning temp directory: {str(e)[:100]}")


    def _get_folder_name(self, folder_id: str) -> str:
        """Get the name of a folder by its ID"""
        try:
            url = f"{self.instance_url}/services/data/{self.api_version}/sobjects/Folder/{folder_id}"
            response = retry_request(url, headers=self.api_headers, timeout=30)
            data = response.json()
            return data.get("Name", folder_id)
        except:
            return folder_id

    def _create_summary(
        self,
        total: int,
        successful: List[str],
        failed: List[Dict[str, Any]],
        folder_name: str = "Unknown"
    ) -> str:
        """Create a summary text file for the export."""
        lines = [
            "SALESFORCE REPORT EXPORT SUMMARY",
            "=" * 40,
            f"Export Date: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"Instance: {self.instance_url}",
            f"API Version: {self.api_version}",
            f"Folder: {folder_name}",
            "",
            f"Total Reports: {total}",
            f"Successful: {len(successful)}",
            f"Failed: {len(failed)}",
            "",
        ]
        
        if failed:
            lines.append("FAILED REPORTS:")
            lines.append("-" * 40)
            for f in failed:
                lines.append(f"‚Ä¢ {f.get('name')} ({f.get('type')})")
                lines.append(f"  ID: {f.get('id')}")
                lines.append(f"  Error: {f.get('error')}")
                lines.append("")
        
        return "\n".join(lines)
    
    