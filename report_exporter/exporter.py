# exporter.py - Part 1: Helper Functions and Setup
# Salesforce Report Exporter with DYNAMIC API version detection

import time
import tempfile
import zipfile
import shutil
from pathlib import Path
import requests
from typing import Callable, Optional, List, Dict, Any
import threading


def get_org_api_version(instance_url: str, session_id: str = None) -> str:
    """
    Fetch the latest API version supported by the Salesforce org.
    This endpoint doesn't require authentication.
    
    Args:
        instance_url: The Salesforce instance URL
        session_id: Optional session ID (not required for this call)
        
    Returns:
        Latest API version string (e.g., "v61.0")
    """
    try:
        url = f"{instance_url.rstrip('/')}/services/data/"
        response = requests.get(url, timeout=15)
        
        if response.status_code == 200:
            versions = response.json()
            if versions and len(versions) > 0:
                # Get the latest (last) version
                latest = versions[-1]
                version = latest.get("version", "58.0")
                return f"v{version}"
    except Exception:
        pass
    
    # Fallback to a safe default
    return "v58.0"


def retry_request(
    url: str,
    headers: dict = None,
    cookies: dict = None,
    max_retries: int = 3,
    timeout: int = 60,
    allow_redirects: bool = True,
    backoff_factor: float = 2.0  # ‚úÖ NEW: Configurable backoff
) -> requests.Response:
    """
    Make HTTP GET request with exponential backoff retry logic.
    
    ‚úÖ OPTIMIZED: Better rate limit handling for large exports.
    """
    backoff = 1
    last_error = None
    headers = headers or {}
    cookies = cookies or {}

    for attempt in range(max_retries):
        try:
            response = requests.get(
                url,
                headers=headers,
                cookies=cookies,
                timeout=timeout,
                allow_redirects=allow_redirects
            )

            if response.status_code == 200:
                return response

            # ‚úÖ IMPROVED: Better rate limit handling
            if response.status_code == 429:  # Too Many Requests
                retry_after = response.headers.get('Retry-After')
                if retry_after:
                    try:
                        backoff = int(retry_after)
                    except ValueError:
                        backoff = min(backoff * backoff_factor, 120)  # Cap at 2 minutes
                else:
                    backoff = min(backoff * backoff_factor, 120)
                
                print(f"‚ö†Ô∏è Rate limited, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue

            # ‚úÖ IMPROVED: Better handling of server errors
            if response.status_code in (500, 502, 503, 504):
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Server error {response.status_code}, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue

            response.raise_for_status()

        except requests.Timeout as e:
            last_error = e
            if attempt < max_retries - 1:
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Timeout, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue
            raise
        except requests.RequestException as e:
            last_error = e
            if attempt < max_retries - 1:
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Request error, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue
            raise

    raise Exception(f"Request failed after {max_retries} retries: {last_error}")


def safe_filename(name: str, max_length: int = 100) -> str:
    """Sanitize filename by removing invalid characters."""
    if not name:
        return "unnamed_report"
    safe = "".join(c if c.isalnum() or c in " ._-" else "_" for c in name)
    while "__" in safe:
        safe = safe.replace("__", "_")
    safe = safe.strip("_ ")
    return safe[:max_length] if safe else "unnamed_report"


def clean_csv_footer(csv_content: str) -> str:
    """
    Remove Salesforce's metadata footer from CSV content.
    
    The footer typically contains:
    - Report name
    - Copyright notice
    - Confidential information warning
    - Generated by information
    
    Args:
        csv_content: Raw CSV content from Salesforce
        
    Returns:
        Cleaned CSV content without footer
    """
    lines = csv_content.split('\n')
    
    cleaned_lines = []
    footer_started = False
    blank_line_count = 0
    
    for i, line in enumerate(lines):
        stripped = line.strip()
        
        if not footer_started:
            if not stripped:
                blank_line_count += 1
                cleaned_lines.append(line)
            else:
                blank_line_count = 0
                
                footer_indicators = [
                    'Copyright (c)',
                    'Confidential Information',
                    'Generated By:',
                    '¬© Copyright',
                    'Do Not Distribute'
                ]
                
                if any(indicator in line for indicator in footer_indicators):
                    footer_started = True
                    while cleaned_lines and not cleaned_lines[-1].strip():
                        cleaned_lines.pop()
                elif ',' not in stripped and len(stripped) > 0:
                    next_lines = lines[i+1:min(i+5, len(lines))]
                    next_text = ' '.join(next_lines)
                    if any(indicator in next_text for indicator in footer_indicators):
                        footer_started = True
                        while cleaned_lines and not cleaned_lines[-1].strip():
                            cleaned_lines.pop()
                    else:
                        cleaned_lines.append(line)
                else:
                    cleaned_lines.append(line)
    
    result = '\n'.join(cleaned_lines)
    result = result.rstrip('\n')
    
    return result




# exporter.py - Part 2: SalesforceReportExporter Class
# This continues from Part 1 (helper functions)

class SalesforceReportExporter:
    """
    Export Salesforce reports to formatted Excel files and package them into a ZIP.
    
    Uses THREE methods:
    1. REST API to get list of reports (with metadata)
    2. Analytics API to export reports with full formatting (groupings, summaries)
    3. UI Export URL to download CSV (for CSV format exports only)
    
    API version is detected dynamically from the org.
    """

    def __init__(
        self,
        session_id: str,
        instance_url: str,
        api_version: str = None,
        progress_callback: Optional[Callable[[int, int], None]] = None
    ):
        """
        Initialize the Salesforce Report Exporter.
        
        Args:
            session_id: Salesforce session ID (from login)
            instance_url: Salesforce instance URL (e.g., https://na1.salesforce.com)
            api_version: Optional API version (e.g., "61.0" or "v61.0"). 
                        If None, will auto-detect latest version.
            progress_callback: Optional callback for progress updates
        """
        self.session_id = session_id
        self.instance_url = instance_url.rstrip('/')
        self.progress_callback = progress_callback
        
        # ===== API VERSION DETECTION =====
        # Get API version dynamically if not provided
        if api_version:
            self.api_version = api_version if api_version.startswith('v') else f"v{api_version}"
        else:
            self.api_version = get_org_api_version(self.instance_url)
        
        print(f"üì° Salesforce API Version: {self.api_version}")
        
        # ===== VERSION CHECK FOR EXCEL EXPORT =====
        # Check if API version supports native Excel download (requires v39.0+)
        self.excel_export_supported = self._check_api_version_for_excel()
        
        if self.excel_export_supported:
            print(f"   ‚úÖ Native Excel export: SUPPORTED")
        else:
            print(f"   ‚ö†Ô∏è  Native Excel export: NOT SUPPORTED (version < v39.0)")
            print(f"   ‚ÑπÔ∏è  Excel exports will fallback to CSV format")
        
        # ===== BUILD API ENDPOINTS =====
        # Build endpoints with dynamic version
        self.reports_list_endpoint = f"/services/data/{self.api_version}/analytics/reports"
        self.folders_list_endpoint = f"/services/data/{self.api_version}/folders"
        
        # ===== SETUP REQUEST HEADERS =====
        # Headers for REST API calls (list reports, get metadata)
        self.api_headers = {
            "Authorization": f"Bearer {self.session_id}",
            "Accept": "application/json"
        }
        
        # Headers for native Excel download
        self.excel_headers = {
            "Authorization": f"Bearer {self.session_id}",
            "Accept": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        }
        
        # Cookies for UI export (CSV download fallback)
        self.export_cookies = {
            "sid": self.session_id
        }
        
        print(f"   üîó Reports endpoint: {self.reports_list_endpoint}")
        print(f"   üîó Instance: {self.instance_url}")
    
    
    def _query_with_pagination(
        self,
        base_query: str,
        batch_size: int = 2000,
        progress_callback: Optional[Callable[[int, int], None]] = None,
        cancel_event: Optional[Any] = None  # ‚úÖ NEW: Cancellation support
    ) -> List[Dict[str, Any]]:
        """
        Execute SOQL query with automatic pagination.
        Handles Salesforce's 2,000 row limit per query.
        
        ‚úÖ OPTIMIZED: Cancellation support + better timeout handling for large queries.
        
        Args:
            base_query: Base SOQL query (without LIMIT/OFFSET)
            batch_size: Records per batch (default 2000, Salesforce limit)
            progress_callback: Optional callback(fetched, estimated_total)
            cancel_event: Optional threading.Event to check for cancellation
            
        Returns:
            List of all records combined from all pages
        """
        all_records = []
        offset = 0
        has_more = True
        estimated_total = None
        consecutive_errors = 0  # ‚úÖ NEW: Track consecutive errors
        max_consecutive_errors = 3  # ‚úÖ NEW: Fail after 3 consecutive errors
        
        while has_more:
            # ‚úÖ NEW: Check for cancellation
            if cancel_event and cancel_event.is_set():
                print(f"‚ö†Ô∏è Query cancelled at offset {offset}")
                break
            
            # Build paginated query
            paginated_query = f"{base_query} LIMIT {batch_size} OFFSET {offset}"
            
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": paginated_query}
            
            try:
                # ‚úÖ IMPROVED: Longer timeout for large queries
                timeout = 90 if len(all_records) > 5000 else 60
                
                response = requests.get(
                    query_url,
                    headers=self.api_headers,
                    params=params,
                    timeout=timeout
                )
                response.raise_for_status()
                
                data = response.json()
                records = data.get("records", [])
                
                if not records:
                    has_more = False
                    break
                
                all_records.extend(records)
                offset += len(records)
                
                # ‚úÖ RESET: Reset error counter on success
                consecutive_errors = 0
                
                # Update progress if callback provided
                if progress_callback and estimated_total is None:
                    # Estimate total based on first batch
                    if len(records) == batch_size:
                        estimated_total = batch_size * 10  # Rough estimate
                    else:
                        estimated_total = len(records)
                
                if progress_callback:
                    progress_callback(len(all_records), estimated_total or len(all_records))
                
                # If we got fewer records than batch_size, we're done
                if len(records) < batch_size:
                    has_more = False
                
                # ‚úÖ NEW: Small delay to avoid rate limiting on large queries
                if has_more and len(all_records) > 0 and len(all_records) % 10000 == 0:
                    print(f"üìä Fetched {len(all_records)} records, brief pause to avoid rate limits...")
                    time.sleep(1)
                
            except requests.Timeout as e:
                consecutive_errors += 1
                print(f"‚ö†Ô∏è Query timeout at offset {offset} (attempt {consecutive_errors}/{max_consecutive_errors})")
                
                if consecutive_errors >= max_consecutive_errors:
                    print(f"‚ùå Too many consecutive errors, stopping pagination at {len(all_records)} records")
                    has_more = False
                else:
                    # Wait before retry
                    time.sleep(2 * consecutive_errors)
                    continue
                    
            except requests.RequestException as e:
                consecutive_errors += 1
                print(f"‚ö†Ô∏è Query error at offset {offset}: {str(e)[:100]} (attempt {consecutive_errors}/{max_consecutive_errors})")
                
                if consecutive_errors >= max_consecutive_errors:
                    print(f"‚ùå Too many consecutive errors, stopping pagination at {len(all_records)} records")
                    has_more = False
                else:
                    # Wait before retry
                    time.sleep(2 * consecutive_errors)
                    continue
        
        return all_records


    def _is_csv_format_error(self, error_message: str, report_format: str) -> bool:
        """
        Detect if CSV export failed due to unsupported report format.
        
        Joined reports are NEVER supported in CSV.
        Matrix reports sometimes fail in CSV.
        
        Args:
            error_message: Error message from CSV export attempt
            report_format: Report format (TABULAR, SUMMARY, MATRIX, JOINED, MultiBlock)
            
        Returns:
            True if error is due to format incompatibility
        """
        error_lower = error_message.lower()
        
        # Known format-related error patterns from Salesforce
        format_error_patterns = [
            "joined report",
            "join report",
            "not supported",
            "unsupported format",
            "invalid report type",
            "matrix report",
            "cannot export",
            "format not supported",
            "multiblock"  # ‚úÖ NEW: MultiBlock is Salesforce's internal name for Joined
        ]
        
        # Check if error message contains format-related keywords
        has_format_error = any(pattern in error_lower for pattern in format_error_patterns)
        
        # ‚úÖ IMPROVED: Recognize both "JOINED" and "MultiBlock" formats
        is_joined = report_format in ("JOINED", "MultiBlock")
        
        # Matrix reports often fail in CSV (especially with complex groupings)
        is_matrix = report_format == "MATRIX"
        
        # ‚úÖ NEW: Check for session errors (these should NOT trigger Excel fallback)
        is_session_error = any(err in error_lower for err in [
            "session expired",
            "invalid session",
            "authentication",
            "login",
            "unauthorized"
        ])
        
        # Return True if:
        # 1. Error message indicates format issue, OR
        # 2. Report is Joined/MultiBlock (always incompatible), OR
        # 3. Report is Matrix AND error mentions format
        # BUT NOT if it's a session error (those need to be handled differently)
        
        if is_session_error:
            return False  # Don't fallback to Excel if session is the problem
        
        return has_format_error or is_joined or (is_matrix and has_format_error)
        

    

    def list_all_report_folders(self) -> List[Dict[str, Any]]:
        """
        Fetch list of all report folders in the org that the user has access to.
        Returns list of folder metadata (id, name, type, etc.)
        Filters out system/automated folders.
        """
        try:
            # Query for Report folders that user has access to
            query = """
                SELECT Id, Name, Type, DeveloperName, AccessType 
                FROM Folder 
                WHERE Type = 'Report' 
                ORDER BY Name
            """
            
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": query}
            
            response = requests.get(
                query_url, 
                headers=self.api_headers, 
                params=params, 
                timeout=30
            )
            response.raise_for_status()
            
            data = response.json()
            folders = data.get("records", [])
            
            # Clean up the response - convert to simple dict
            cleaned_folders = []
            for folder in folders:
                cleaned_folders.append({
                    "id": folder.get("Id"),
                    "name": folder.get("Name"),
                    "type": folder.get("Type"),
                    "developerName": folder.get("DeveloperName"),
                    "accessType": folder.get("AccessType")
                })
            
            return cleaned_folders
            
        except Exception as e:
            raise Exception(f"Failed to fetch report folders: {str(e)}")

    def list_all_reports(self, folder_id: str = None) -> List[Dict[str, Any]]:
        """
        Fetch list of all available reports using REST API or SOQL query.
        
        Args:
            folder_id: Optional folder ID to filter reports. If None, returns all reports.
            
        Returns:
            List of report metadata (id, name, format, folder, etc.)
        """
        # If folder_id is specified, use SOQL query for accurate filtering
        if folder_id:
            return self._list_reports_by_soql(folder_id)
        
        # Otherwise use the standard REST API endpoint
        url = f"{self.instance_url}{self.reports_list_endpoint}"
        response = retry_request(url, headers=self.api_headers, timeout=60)
        
        data = response.json()
        
        if isinstance(data, list):
            reports = data
        elif isinstance(data, dict):
            reports = data.get("reports", data.get("records", []))
        else:
            reports = []
        
        return reports


    def _list_reports_by_soql(self, folder_id: str) -> List[Dict[str, Any]]:
        """
        Use SOQL query with pagination to get reports by folder ID.
        Now handles unlimited reports per folder.
        
        Args:
            folder_id: The Salesforce folder ID to query
            
        Returns:
            List of report metadata in the same format as list_reports()
        """
        try:
            # SOQL query to get reports in specific folder
            base_query = f"""
                SELECT Id, Name, DeveloperName, FolderName, Format, CreatedDate, LastModifiedDate
                FROM Report 
                WHERE OwnerId = '{folder_id}'
                ORDER BY Name
            """
            
            # Use pagination helper
            records = self._query_with_pagination(base_query.strip())
            
            # Convert SOQL results to match REST API format
            reports = []
            for record in records:
                reports.append({
                    "id": record.get("Id"),
                    "name": record.get("Name"),
                    "developerName": record.get("DeveloperName"),
                    "folderName": record.get("FolderName"),
                    "reportFormat": record.get("Format", "TABULAR"),
                    "lastModifiedDate": record.get("LastModifiedDate"),
                    "createdDate": record.get("CreatedDate")
                })
            
            return reports
            
        except Exception as e:
            print(f"Error querying reports by folder: {str(e)}")
            return []


    def search_by_keyword(self, keyword: str, cancel_event=None) -> Dict[str, Any]:
        """
        Search folders AND reports by keyword, then organize results.
        
        ‚úÖ NEW: Creates a virtual "Unified Public Folder" for orphaned reports
        that match the search but whose folder wasn't found.
        """
        try:
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # Escape keyword for SOQL (prevent injection)
            keyword_escaped = keyword.replace("'", "\\'").replace("%", "\\%")
            
            # ===== STEP 1: Search folders by name =====
            print(f"üîç Step 1: Searching folders matching '{keyword}'...")
            
            folders_query = f"""
                SELECT Id, Name, Type, DeveloperName, AccessType 
                FROM Folder 
                WHERE Type = 'Report' 
                AND (
                    Name LIKE '%{keyword_escaped}%'
                    OR DeveloperName LIKE '%{keyword_escaped}%'
                )
                ORDER BY Name
            """
            
            matching_folders = self._execute_soql_query(folders_query)
            folder_ids_from_name_match = {f.get("Id") for f in matching_folders}
            
            print(f"‚úÖ Found {len(matching_folders)} folders matching keyword")
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 2: Search reports by name (WITH PAGINATION) =====
            print(f"üîç Step 2: Searching reports matching '{keyword}'...")
            
            reports_query = f"""
                SELECT Id, Name, DeveloperName, FolderName, Format, 
                    CreatedDate, LastModifiedDate, OwnerId
                FROM Report 
                WHERE Name LIKE '%{keyword_escaped}%' OR FolderName LIKE '%{keyword_escaped}%'
                ORDER BY Name
            """
            
            # ‚úÖ OPTIMIZED: Use pagination with cancellation support
            matching_reports = self._query_with_pagination(
                reports_query.strip(),
                batch_size=2000,
                cancel_event=cancel_event
            )
            
            print(f"‚úÖ Found {len(matching_reports)} reports matching keyword")
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 3: Get folder IDs from matching reports =====
            folder_ids_from_reports = {r.get("OwnerId") for r in matching_reports if r.get("OwnerId")}
            
            # ===== STEP 4: Fetch folders that contain matching reports =====
            additional_folder_ids = folder_ids_from_reports - folder_ids_from_name_match
            
            additional_folders = []
            if additional_folder_ids:
                print(f"üîç Step 3: Fetching {len(additional_folder_ids)} additional folders...")
                
                # ‚úÖ OPTIMIZED: Process in smaller chunks to avoid query string limits
                folder_ids_list = list(additional_folder_ids)
                chunk_size = 50  # ‚úÖ REDUCED: Smaller chunks for stability
                
                for i in range(0, len(folder_ids_list), chunk_size):
                    # Check cancellation
                    if cancel_event and cancel_event.is_set():
                        return {"folders": [], "reports_by_folder": {}}
                    
                    chunk = folder_ids_list[i:i + chunk_size]
                    ids_str = ",".join([f"'{fid}'" for fid in chunk])
                    
                    folders_query = f"""
                        SELECT Id, Name, Type, DeveloperName, AccessType 
                        FROM Folder 
                        WHERE Id IN ({ids_str})
                    """
                    
                    try:
                        chunk_folders = self._execute_soql_query(folders_query)
                        additional_folders.extend(chunk_folders)
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error fetching folder chunk: {str(e)[:100]}")
                        # Continue with other chunks
                        continue
            
            print(f"‚úÖ Fetched {len(additional_folders)} additional folders")
            
            # ===== STEP 5: Identify orphaned reports =====
            # These are reports that matched the search but whose OwnerId
            # doesn't correspond to any folder we successfully fetched
            
            all_fetched_folder_ids = folder_ids_from_name_match | {f.get("Id") for f in additional_folders}
            
            orphaned_reports = []
            valid_reports = []
            
            for report in matching_reports:
                owner_id = report.get("OwnerId")
                
                if not owner_id or owner_id not in all_fetched_folder_ids:
                    # This report's folder wasn't found - it's orphaned
                    orphaned_reports.append(report)
                else:
                    # This report has a valid folder
                    valid_reports.append(report)
            
            print(f"üìä Report classification:")
            print(f"  ‚úÖ Valid reports (with folders): {len(valid_reports)}")
            print(f"  ‚ö†Ô∏è Orphaned reports (no folder found): {len(orphaned_reports)}")
            
            # ===== STEP 5B: Create virtual "Unified Public Folder" if needed =====
            virtual_folder_id = None
            virtual_folder = None
            
            if orphaned_reports:
                # Create a virtual folder to hold orphaned reports
                virtual_folder_id = "VIRTUAL_UNIFIED_PUBLIC_FOLDER"
                
                virtual_folder = {
                    "Id": virtual_folder_id,
                    "Name": "üìÅ Unified Public Folder (Search Results)",
                    "Type": "Report",
                    "DeveloperName": "UnifiedPublicFolder",
                    "AccessType": "Public"
                }
                
                print(f"üÜï Created virtual folder for {len(orphaned_reports)} orphaned reports")
            
            # ===== STEP 6: Combine all folders (virtual first, then real folders) =====
            all_folders = []
            
            # ‚úÖ Add virtual folder at TOP if it exists
            if virtual_folder:
                all_folders.append(virtual_folder)
            
            # Add real folders (name-matched + additional)
            all_folders.extend(matching_folders)
            all_folders.extend(additional_folders)
            
            # ===== STEP 7: Group reports by folder (MEMORY EFFICIENT) =====
            print(f"üìä Grouping reports by folder...")
            
            reports_by_folder = {}
            
            # ‚úÖ Add orphaned reports to virtual folder FIRST
            if virtual_folder_id and orphaned_reports:
                reports_by_folder[virtual_folder_id] = []
                
                for report in orphaned_reports:
                    reports_by_folder[virtual_folder_id].append({
                        "id": report.get("Id"),
                        "name": report.get("Name"),
                        "developerName": report.get("DeveloperName"),
                        "folderName": report.get("FolderName") or "Unified Public Folder",
                        "reportFormat": report.get("Format", "TABULAR"),
                        "lastModifiedDate": report.get("LastModifiedDate"),
                        "createdDate": report.get("CreatedDate")
                    })
                
                print(f"  ‚úÖ Virtual folder: {len(orphaned_reports)} orphaned reports")
            
            # Group valid reports by their actual folders
            for report in valid_reports:
                folder_id = report.get("OwnerId")
                if folder_id:
                    if folder_id not in reports_by_folder:
                        reports_by_folder[folder_id] = []
                    
                    reports_by_folder[folder_id].append({
                        "id": report.get("Id"),
                        "name": report.get("Name"),
                        "developerName": report.get("DeveloperName"),
                        "folderName": report.get("FolderName"),
                        "reportFormat": report.get("Format", "TABULAR"),
                        "lastModifiedDate": report.get("LastModifiedDate"),
                        "createdDate": report.get("CreatedDate")
                    })
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 8: For folders matched by name, get ALL their reports (IN CHUNKS) =====
            print(f"üîç Step 4: Fetching all reports from {len(matching_folders)} matched folders...")
            
            for idx, folder in enumerate(matching_folders):
                folder_id = folder.get("Id")
                
                # Check cancellation
                if cancel_event and cancel_event.is_set():
                    return {"folders": [], "reports_by_folder": {}}
                
                # If this folder doesn't have any reports yet (from keyword search),
                # fetch ALL reports in this folder
                if folder_id not in reports_by_folder:
                    reports_query = f"""
                        SELECT Id, Name, DeveloperName, FolderName, Format, 
                            CreatedDate, LastModifiedDate
                        FROM Report 
                        WHERE OwnerId = '{folder_id}'
                        ORDER BY Name
                    """
                    
                    try:
                        # ‚úÖ OPTIMIZED: Use pagination with cancellation
                        folder_reports = self._query_with_pagination(
                            reports_query.strip(),
                            batch_size=2000,
                            cancel_event=cancel_event
                        )
                        
                        if folder_reports:
                            reports_by_folder[folder_id] = [
                                {
                                    "id": r.get("Id"),
                                    "name": r.get("Name"),
                                    "developerName": r.get("DeveloperName"),
                                    "folderName": r.get("FolderName"),
                                    "reportFormat": r.get("Format", "TABULAR"),
                                    "lastModifiedDate": r.get("LastModifiedDate"),
                                    "createdDate": r.get("CreatedDate")
                                }
                                for r in folder_reports
                            ]
                            
                            print(f"  ‚úÖ Folder {idx + 1}/{len(matching_folders)}: {len(folder_reports)} reports")
                        
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è Error fetching reports from folder {folder.get('Name')}: {str(e)[:100]}")
                        # Continue with other folders
                        continue
            
            # ===== STEP 9: Clean up folder metadata =====
            cleaned_folders = []
            for folder in all_folders:
                cleaned_folders.append({
                    "id": folder.get("Id"),
                    "name": folder.get("Name"),
                    "type": folder.get("Type"),
                    "developerName": folder.get("DeveloperName"),
                    "accessType": folder.get("AccessType")
                })
            
            # ===== FINAL: Calculate statistics =====
            total_reports = sum(len(reports) for reports in reports_by_folder.values())
            print(f"‚úÖ Search complete: {len(cleaned_folders)} folders, {total_reports} total reports")
            
            # ‚úÖ Log virtual folder stats if present
            if virtual_folder_id and virtual_folder_id in reports_by_folder:
                virtual_count = len(reports_by_folder[virtual_folder_id])
                print(f"  üìÅ Virtual folder contains: {virtual_count} orphaned reports")
            
            return {
                "folders": cleaned_folders,
                "reports_by_folder": reports_by_folder
            }
            
        except Exception as e:
            print(f"‚ùå Search error: {str(e)}")
            raise Exception(f"Search failed: {str(e)}")




  
    def _execute_soql_query(self, query: str) -> List[Dict]:
        """
        Helper method to execute a SOQL query and return records.
        
        Args:
            query: SOQL query string
            
        Returns:
            List of record dictionaries
            
        Raises:
            Exception: If query fails
        """
        try:
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": query.strip()}
            
            response = requests.get(
                query_url,
                headers=self.api_headers,
                params=params,
                timeout=30
            )
            response.raise_for_status()
            
            data = response.json()
            return data.get("records", [])
            
        except requests.RequestException as e:
            raise Exception(f"SOQL query failed: {str(e)}")


    def export_report_csv(self, report_id: str, timeout: int = 120) -> str:
        """
        Export a single report as CSV using the UI export URL method.
        
        ‚úÖ IMPROVED: Configurable timeout (default 120s, can be increased for large reports).
        
        This is the "screen scraping" approach that:
        - Bypasses the 2000 row API limit
        - Returns actual CSV content
        - Works with Lightning and Classic
        - Automatically removes Salesforce metadata footer
        
        Args:
            report_id: Salesforce report ID
            timeout: Request timeout in seconds (default 120)
        """
        # Build the export URL - mimics clicking "Export" in the UI
        export_url = (
            f"{self.instance_url}/{report_id}"
            f"?isdtp=p1&export=1&enc=UTF-8&xf=csv"
        )
        
        # ‚úÖ IMPROVED: Use retry_request with configurable timeout
        response = retry_request(
            export_url,
            cookies=self.export_cookies,
            timeout=timeout,
            allow_redirects=True,
            max_retries=3  # ‚úÖ NEW: Explicitly set retries
        )
        
        content = response.text
        
        # Check if we got HTML instead of CSV
        if content.strip().startswith('<!DOCTYPE') or content.strip().startswith('<html'):
            if 'login.salesforce.com' in content or 'ec=302' in content:
                raise Exception("Session expired or invalid. Please re-login.")
            elif 'You do not have access' in content:
                raise Exception("Access denied to this report.")
            else:
                raise Exception("Received HTML instead of CSV. Report may not be exportable.")
        
        # Clean the CSV content (remove footer)
        cleaned_content = clean_csv_footer(content)
        
        return cleaned_content


    def export_report_excel_native(self, report_id: str, timeout: int = 180) -> bytes:
        """
        Download report as formatted Excel using Salesforce's NATIVE exporter.
        
        This is the EXACT SAME Excel file users get from the Salesforce UI.
        Uses the official Analytics API Excel download endpoint.
        
        ‚úÖ Supports ALL report types:
        - Tabular
        - Summary (with groupings, subtotals, grand totals)
        - Matrix (cross-tab with row/column groupings)
        - Joined (multi-block reports)
        
        ‚úÖ Preserves ALL formatting:
        - Row groupings and indentation
        - Summary rows (subtotals at each grouping level)
        - Grand totals
        - Excel formatting (colors, fonts, borders, alignment)
        - Merged cells (for matrix reports)
        - Column headers and data types
        
        Args:
            report_id: Salesforce report ID (15 or 18 character)
            timeout: Request timeout in seconds (default 180)
            
        Returns:
            bytes: Excel file content (.xlsx format)
            
        Raises:
            Exception: If download fails or API version < 39.0
            
        Reference:
            Salesforce Analytics API - Download Excel:
            https://developer.salesforce.com/docs/atlas.en-us.api_analytics.meta/api_analytics/sforce_analytics_rest_api_download_excel.htm
        """
        try:
            # Build the native Excel download URL
            download_url = (
                f"{self.instance_url}/services/data/{self.api_version}"
                f"/analytics/reports/{report_id}"
            )
            
            # Critical: Set Accept header to get Excel format
            # This tells Salesforce to return .xlsx instead of JSON
            excel_headers = {
                "Authorization": f"Bearer {self.session_id}",
                "Accept": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            }
            
            print(f"üì• Downloading native Excel: {report_id}")
            print(f"   URL: {download_url}")
            print(f"   Accept: Excel format (.xlsx)")
            
            # Make request with retry logic
            response = retry_request(
                download_url,
                headers=excel_headers,
                timeout=timeout,
                max_retries=3,
                backoff_factor=2.0
            )
            
            # Validate response
            if not response.content:
                raise Exception("Empty response from Salesforce Excel endpoint")
            
            # Check if we got Excel (should start with PK zip signature)
            if not response.content.startswith(b'PK'):
                # We got something else (HTML error page, JSON error, etc.)
                error_text = response.content[:500].decode('utf-8', errors='ignore')
                
                if 'login.salesforce.com' in error_text or 'ec=302' in error_text:
                    raise Exception("Session expired or invalid. Please re-login.")
                elif 'You do not have access' in error_text:
                    raise Exception("Access denied to this report.")
                elif 'Report not found' in error_text or '404' in error_text:
                    raise Exception("Report not found or deleted.")
                else:
                    raise Exception(f"Received non-Excel response: {error_text[:100]}")
            
            excel_bytes = response.content
            size_kb = len(excel_bytes) / 1024
            
            print(f"   ‚úÖ Downloaded: {size_kb:.1f} KB")
            
            return excel_bytes
            
        except Exception as e:
            error_msg = str(e)
            print(f"   ‚ùå Native Excel download failed: {error_msg[:100]}")
            raise Exception(f"Native Excel download failed: {error_msg}")


    def _check_api_version_for_excel(self) -> bool:
        """
        Check if current API version supports native Excel download.
        
        Native Excel download requires API version 39.0 or higher.
        
        Returns:
            bool: True if version is sufficient, False otherwise
        """
        try:
            # Extract version number (e.g., "v61.0" -> 61.0)
            version_str = self.api_version.replace('v', '').replace('V', '')
            version_float = float(version_str)
            
            # Excel download available since v39.0
            min_version = 39.0
            
            if version_float >= min_version:
                return True
            else:
                print(f"‚ö†Ô∏è API version {self.api_version} is below minimum {min_version} for Excel download")
                return False
                
        except Exception as e:
            print(f"‚ö†Ô∏è Could not parse API version: {self.api_version} - {str(e)}")
            # Assume it's recent enough
            return True


    def export_selected_reports_to_zip_concurrent_excel(
        self,
        output_zip_path: str,
        report_ids: List[str],
        max_workers: int = 10,
        cancel_event: Optional[Any] = None,
        retry_attempts: int = 3,
        reports_metadata: Optional[Dict[str, Dict]] = None
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to NATIVE Excel format using CONCURRENT downloads.
        
        ‚úÖ Uses Salesforce's NATIVE Excel exporter (same as UI download)
        ‚úÖ Supports ALL report types: Tabular, Summary, Matrix, Joined
        ‚úÖ Preserves EXACT formatting from Salesforce UI
        ‚úÖ Falls back to CSV if native Excel fails
        
        Args:
            output_zip_path: Path where ZIP file will be saved
            report_ids: List of report IDs to export
            max_workers: Number of parallel downloads (default 10)
            cancel_event: Threading event to signal cancellation
            retry_attempts: Number of retry attempts for failed reports
            reports_metadata: Optional dict of {report_id: {name, format}} to skip metadata fetch
            
        Returns:
            Dictionary with export results:
            {
                "zip": output_zip_path,
                "total": total_reports,
                "successful": ["Report 1", "Report 2", ...],
                "failed": [{"id": "...", "name": "...", "error": "..."}, ...],
                "cancelled": bool,
                "completed": count,
                "method_used": {"excel": count, "csv_fallback": count}
            }
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_excel_"))
        
        try:
            # ===== STEP 1: Get/validate metadata =====
            print(f"üìä Preparing to export {len(report_ids)} reports as Excel...")
            
            if reports_metadata:
                print("‚ö° Using cached metadata (skipping API calls)")
                reports = []
                for report_id in report_ids:
                    if report_id in reports_metadata:
                        reports.append(reports_metadata[report_id])
                    else:
                        reports.append({
                            "id": report_id,
                            "name": report_id,
                            "reportFormat": "TABULAR"
                        })
            else:
                print("üìã Fetching report metadata...")
                if not report_ids:
                    reports = []
                else:
                    chunk_size = 50
                    reports = []
                    
                    for i in range(0, len(report_ids), chunk_size):
                        if cancel_event and cancel_event.is_set():
                            raise Exception("Export cancelled by user")
                        
                        chunk_ids = report_ids[i:i + chunk_size]
                        ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                        
                        base_query = f"""
                            SELECT Id, Name, Format 
                            FROM Report 
                            WHERE Id IN ({ids_formatted})
                        """
                        
                        try:
                            chunk_records = self._query_with_pagination(
                                base_query.strip(), 
                                batch_size=2000,
                                cancel_event=cancel_event
                            )
                            
                            for record in chunk_records:
                                reports.append({
                                    "id": record.get("Id"),
                                    "name": record.get("Name"),
                                    "reportFormat": record.get("Format", "TABULAR")
                                })
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error fetching report chunk {i//chunk_size + 1}: {str(e)[:100]}")
                            for rid in chunk_ids:
                                reports.append({
                                    "id": rid,
                                    "name": rid,
                                    "reportFormat": "TABULAR"
                                })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []
            used_filenames: Dict[str, int] = {}
            
            # Track export methods used
            export_methods = {
                "excel": 0,        # Native Excel download
                "csv_fallback": 0  # CSV fallback when Excel fails
            }
            
            # Thread-safe counters
            completed_lock = threading.Lock()
            
            # Adaptive worker count
            if total > 5000:
                max_workers = min(max_workers, 8)
                print(f"‚öôÔ∏è Large export detected ({total} reports), using {max_workers} workers")
            elif total > 1000:
                max_workers = min(max_workers, 10)
                print(f"‚öôÔ∏è Using {max_workers} workers for {total} reports")
            
            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports (Excel)",
                    "api_version": self.api_version,
                    "cancelled": False,
                    "completed": 0,
                    "method_used": export_methods
                }
            
            # ===== STEP 2: Define worker function =====
            def export_single_report_excel(report: Dict) -> tuple:
                """Export a single report - tries Excel, falls back to CSV"""
                nonlocal completed
                
                if cancel_event and cancel_event.is_set():
                    return ("cancelled", report, None, None)
                
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")
                
                # Notify: Starting download
                if self.progress_callback:
                    try:
                        with completed_lock:
                            current_count = completed
                        self.progress_callback(current_count, total, report_name)
                    except:
                        pass
                
                # Generate filename (thread-safe)
                base_name = safe_filename(report_name)
                
                with completed_lock:
                    if base_name in used_filenames:
                        used_filenames[base_name] += 1
                        file_number = used_filenames[base_name]
                    else:
                        used_filenames[base_name] = 1
                        file_number = 1
                
                # Try native Excel first, fallback to CSV if it fails
                export_method = None
                file_content = None
                file_extension = None
                last_error = None
                
                # ===== ATTEMPT 1: Native Excel Download =====
                if self.excel_export_supported:
                    for attempt in range(retry_attempts):
                        if cancel_event and cancel_event.is_set():
                            return ("cancelled", report, None, None)
                        
                        try:
                            timeout = 180 if total > 5000 else 120
                            
                            print(f"üì• [{attempt + 1}/{retry_attempts}] Trying Excel: {report_name}")
                            
                            excel_content = self.export_report_excel_native(report_id, timeout=timeout)
                            
                            if not excel_content:
                                raise Exception("Empty Excel response")
                            
                            # Success!
                            file_content = excel_content
                            file_extension = ".xlsx"
                            export_method = "excel"
                            print(f"   ‚úÖ Excel downloaded: {len(excel_content)} bytes")
                            break
                            
                        except Exception as e:
                            last_error = str(e)
                            
                            if "not found" in last_error.lower() or "deleted" in last_error.lower():
                                print(f"   ‚ö†Ô∏è Report not found, skipping retries")
                                break
                            
                            if attempt < retry_attempts - 1:
                                wait_time = (2 ** attempt) + (attempt * 0.5)
                                print(f"   ‚ö†Ô∏è Excel failed, retry in {wait_time}s: {last_error[:50]}")
                                time.sleep(wait_time)
                                continue
                            else:
                                print(f"   ‚ùå Excel failed after {retry_attempts} attempts")
                                break
                
                # ===== ATTEMPT 2: CSV Fallback =====
                if file_content is None:
                    print(f"üìÑ Falling back to CSV: {report_name}")
                    
                    for attempt in range(retry_attempts):
                        if cancel_event and cancel_event.is_set():
                            return ("cancelled", report, None, None)
                        
                        try:
                            timeout = 180 if total > 5000 else 120
                            
                            csv_content = self.export_report_csv(report_id, timeout=timeout)
                            
                            if not csv_content or len(csv_content.strip()) == 0:
                                raise Exception("Empty CSV response")
                            
                            # Success!
                            file_content = csv_content.encode('utf-8')
                            file_extension = ".csv"
                            export_method = "csv_fallback"
                            print(f"   ‚úÖ CSV downloaded: {len(csv_content)} bytes")
                            break
                            
                        except Exception as e:
                            last_error = str(e)
                            
                            if attempt < retry_attempts - 1:
                                wait_time = (2 ** attempt) + (attempt * 0.5)
                                print(f"   ‚ö†Ô∏è CSV failed, retry in {wait_time}s: {last_error[:50]}")
                                time.sleep(wait_time)
                                continue
                            else:
                                print(f"   ‚ùå CSV failed after {retry_attempts} attempts")
                                break
                
                # ===== SAVE FILE OR RECORD FAILURE =====
                if file_content:
                    # Build filename with correct extension
                    if file_number > 1:
                        filename = f"{base_name}_{file_number}{file_extension}"
                    else:
                        filename = f"{base_name}{file_extension}"
                    
                    file_path = tmp_dir / filename
                    
                    # Write file
                    if file_extension == ".xlsx":
                        file_path.write_bytes(file_content)
                    else:
                        file_path.write_bytes(file_content)
                    
                    # Update counter
                    with completed_lock:
                        completed += 1
                        current_count = completed
                        export_methods[export_method] += 1
                    
                    # Notify: Report completed
                    if self.progress_callback:
                        try:
                            self.progress_callback(current_count, total)
                        except:
                            pass
                    
                    return ("success", report, filename, export_method)
                
                else:
                    # Both Excel and CSV failed - create error file
                    error_content = (
                        f"# Failed to export report after {retry_attempts} attempts\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"#\n"
                        f"# Excel download error: {last_error}\n"
                        f"#\n"
                        f"# This report could not be downloaded in any format.\n"
                        f"# Please try downloading it manually from Salesforce UI.\n"
                    )
                    
                    if file_number > 1:
                        error_filename = f"{base_name}_{file_number}_ERROR.txt"
                    else:
                        error_filename = f"{base_name}_ERROR.txt"
                    
                    error_path = tmp_dir / error_filename
                    error_path.write_text(error_content, encoding="utf-8")
                    
                    # Update counter
                    with completed_lock:
                        completed += 1
                        current_count = completed
                    
                    # Notify: Report failed
                    if self.progress_callback:
                        try:
                            self.progress_callback(current_count, total)
                        except:
                            pass
                    
                    return ("failed", report, last_error, None)
            
            # ===== STEP 3: Export reports concurrently =====
            print(f"üöÄ Starting concurrent export with {max_workers} workers...")
            print(f"üì• Export method: Native Salesforce Excel (same as UI)")
            print(f"üîÑ Fallback: CSV if Excel fails")
            print(f"‚úÖ Supports: Tabular, Summary, Matrix, Joined (ALL types)")
            # Log to activity window
            if hasattr(self, 'update_queue'):
                self.update_queue.put(("log", f"üöÄ Starting concurrent export of {len(report_ids)} reports..."))
                self.update_queue.put(("log", f"üìÑ CSV format: Fast export for most reports"))
                self.update_queue.put(("log", f"üìä Excel fallback: Automatic for Joined/Matrix reports"))
                self.update_queue.put(("log", f"‚ö° Method: Concurrent downloads ({max_workers} workers)"))
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all tasks
                future_to_report = {
                    executor.submit(export_single_report_excel, report): report 
                    for report in reports
                }
                
                # Track progress milestones
                last_milestone = 0
                milestone_interval = max(100, total // 20)
                
                # Process completed tasks
                for future in as_completed(future_to_report):
                    if cancel_event and cancel_event.is_set():
                        print("‚ö†Ô∏è Cancellation detected, stopping remaining downloads...")
                        for f in future_to_report:
                            f.cancel()
                        break
                    
                    try:
                        status, report, data, method = future.result()
                        
                        if status == "success":
                            successful.append(report.get("name"))
                        elif status == "failed":
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": data
                            })
                        elif status == "cancelled":
                            pass
                        
                        # Log progress milestones
                        if completed - last_milestone >= milestone_interval:
                            success_rate = (len(successful) / completed * 100) if completed > 0 else 0
                            excel_count = export_methods["excel"]
                            csv_count = export_methods["csv_fallback"]
                            print(f"üìä Progress: {completed}/{total} ({completed/total*100:.1f}%) - Success: {success_rate:.1f}%")
                            print(f"   üì• Excel: {excel_count} | üìÑ CSV fallback: {csv_count}")
                            last_milestone = completed
                        
                        # Update progress callback
                        if self.progress_callback:
                            try:
                                self.progress_callback(completed, total)
                            except Exception:
                                pass
                                
                    except Exception as e:
                        report = future_to_report.get(future)
                        if report:
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": str(e)
                            })
                            print(f"‚ö†Ô∏è Future error for {report.get('name')}: {str(e)[:100]}")
            
            # Check if cancelled
            was_cancelled = cancel_event and cancel_event.is_set()
            
            # ===== STEP 4: Create ZIP file =====
            print(f"üì¶ Creating ZIP file with {completed} reports...")
            
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                # Write files in sorted order
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                # Create enhanced summary
                summary = self._create_summary(
                    total, 
                    successful, 
                    failed, 
                    "Selected Reports (Native Excel)" + (" (CANCELLED)" if was_cancelled else "")
                )
                
                # Add export method statistics
                summary += "\n\n"
                summary += "=" * 50 + "\n"
                summary += "EXPORT METHOD STATISTICS\n"
                summary += "=" * 50 + "\n"
                summary += f"Native Excel Downloads: {export_methods['excel']}\n"
                summary += f"CSV Fallbacks: {export_methods['csv_fallback']}\n"
                summary += f"Failed: {len(failed)}\n"
                summary += "\n"
                summary += "=" * 50 + "\n"
                summary += "EXCEL EXPORT FORMAT INFORMATION\n"
                summary += "=" * 50 + "\n"
                summary += "Export Method: Salesforce Native Excel Download\n"
                summary += "API Endpoint: /services/data/vXX.X/analytics/reports/<ID>\n"
                summary += "Accept Header: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n"
                summary += "\n"
                summary += "‚úÖ This export uses Salesforce's NATIVE Excel exporter:\n"
                summary += "  ‚Ä¢ Same Excel file as UI 'Export ‚Üí Formatted Report'\n"
                summary += "  ‚Ä¢ Preserves ALL formatting (groupings, subtotals, colors)\n"
                summary += "  ‚Ä¢ Supports ALL report types (Tabular, Summary, Matrix, Joined)\n"
                summary += "  ‚Ä¢ True .xlsx format (no security warnings)\n"
                summary += "\n"
                summary += "üìÑ CSV Fallbacks:\n"
                summary += f"  ‚Ä¢ {export_methods['csv_fallback']} reports exported as CSV\n"
                summary += "  ‚Ä¢ CSV is used when native Excel download fails\n"
                summary += "  ‚Ä¢ CSV preserves data but not formatting\n"
                summary += "\n"
                summary += "Reference:\n"
                summary += "https://developer.salesforce.com/docs/atlas.en-us.api_analytics.meta/api_analytics/sforce_analytics_rest_api_download_excel.htm\n"
                summary += "=" * 50 + "\n"
                
                zf.writestr("_EXPORT_SUMMARY.txt", summary)
            
            # Final statistics
            success_rate = (len(successful) / total * 100) if total > 0 else 0
            print(f"‚úÖ Export complete: {len(successful)}/{total} successful ({success_rate:.1f}%)")
            print(f"   üì• Native Excel: {export_methods['excel']}")
            print(f"   üìÑ CSV Fallback: {export_methods['csv_fallback']}")
            if failed:
                print(f"   ‚ùå Failed: {len(failed)} reports")
            
            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports (Native Excel)",
                "api_version": self.api_version,
                "cancelled": was_cancelled,
                "completed": completed,
                "method_used": export_methods
            }
        
        finally:
            # Cleanup temporary files
            try:
                shutil.rmtree(tmp_dir)
                print(f"üßπ Cleaned up temporary files")
            except Exception as e:
                print(f"‚ö†Ô∏è Error cleaning temp directory: {str(e)[:100]}")


    


    
    
    # exporter.py - Part 3: Export Methods
# This continues the SalesforceReportExporter class

    # Add these methods to the SalesforceReportExporter class

    def export_reports_by_folder_to_zip(
        self,
        output_zip_path: str,
        folder_id: str,
        delay_between_reports: float = 1.0
    ) -> Dict[str, Any]:
        """
        Export all reports from a specific folder to a ZIP file.
        
        Args:
            output_zip_path: Path where ZIP file will be saved
            folder_id: The Salesforce folder ID to export reports from
            delay_between_reports: Seconds to wait between exports (rate limiting)
            
        Returns:
            Dictionary with export results
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # Step 1: Get reports from this folder
            reports = self.list_reports(folder_id=folder_id)
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            # Get folder name
            folder_name = self._get_folder_name(folder_id)

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", f"No reports found in folder: {folder_name}")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": folder_name,
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, folder_name)
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": folder_name,
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass

    def export_all_reports_to_zip(
        self,
        output_zip_path: str,
        delay_between_reports: float = 0.5
    ) -> Dict[str, Any]:
        """
        Export ALL reports from ALL folders to a ZIP file.
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # Step 1: Get list of all reports
            reports = self.list_reports()
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found in this Salesforce org.")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "All Folders",
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, "All Folders")
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "All Folders",
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass
    
    # exporter.py - Part 4: Selected Reports Export & Helper Methods
# This completes the SalesforceReportExporter class

    # Add these final methods to the SalesforceReportExporter class

    def export_selected_reports_to_zip(
        self,
        output_zip_path: str,
        report_ids: List[str],
        delay_between_reports: float = 0.5
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to a ZIP file.
        Now uses direct SOQL to ensure system/automated reports are found.
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # --- IMPROVED: Direct SOQL lookup with pagination ---
            if not report_ids:
                reports = []
            else:
                # Split into chunks of 100 IDs (SOQL IN clause limit is ~1000 chars)
                chunk_size = 100
                reports = []
                
                for i in range(0, len(report_ids), chunk_size):
                    chunk_ids = report_ids[i:i + chunk_size]
                    ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                    
                    base_query = f"""
                        SELECT Id, Name, Format 
                        FROM Report 
                        WHERE Id IN ({ids_formatted})
                    """
                    
                    try:
                        # Use pagination helper (though with 100 IDs we won't need pagination)
                        chunk_records = self._query_with_pagination(base_query.strip(), batch_size=2000)
                        
                        for record in chunk_records:
                            reports.append({
                                "id": record.get("Id"),
                                "name": record.get("Name"),
                                "reportFormat": record.get("Format", "TABULAR")
                            })
                    except Exception as e:
                        print(f"Error fetching report chunk: {str(e)}")
                        # Fallback: create entries with just IDs
                        for rid in chunk_ids:
                            reports.append({
                                "id": rid,
                                "name": rid,
                                "reportFormat": "TABULAR"
                            })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports",
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each selected report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, "Selected Reports")
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports",
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass
    


    def export_selected_reports_to_zip_concurrent(
        self,
        output_zip_path: str,
        report_ids: List[str],
        max_workers: int = 10,
        cancel_event: Optional[Any] = None,
        retry_attempts: int = 3,
        reports_metadata: Optional[Dict[str, Dict]] = None
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to CSV format using CONCURRENT downloads.
        
        ‚úÖ Uses CSV export (fast, lightweight)
        ‚úÖ Automatic Excel fallback for Joined/Matrix reports
        
        Args:
            output_zip_path: Path where ZIP file will be saved
            report_ids: List of report IDs to export
            max_workers: Number of parallel downloads (default 10)
            cancel_event: Threading event to signal cancellation
            retry_attempts: Number of retry attempts for failed reports
            reports_metadata: Optional dict of {report_id: {name, format}} to skip metadata fetch
            
        Returns:
            Dictionary with export results:
            {
                "zip": output_zip_path,
                "total": total_reports,
                "successful": ["Report 1", "Report 2", ...],
                "failed": [{"id": "...", "name": "...", "error": "..."}, ...],
                "cancelled": bool,
                "completed": count,
                "method_used": {"csv": count, "excel_fallback": count}
            }
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_csv_"))
        
        try:
            # ===== STEP 1: Get/validate metadata =====
            print(f"üìä Preparing to export {len(report_ids)} reports as CSV...")
            
            if reports_metadata:
                print("‚ö° Using cached metadata (skipping API calls)")
                reports = []
                for report_id in report_ids:
                    if report_id in reports_metadata:
                        reports.append(reports_metadata[report_id])
                    else:
                        reports.append({
                            "id": report_id,
                            "name": report_id,
                            "reportFormat": "TABULAR"
                        })
            else:
                print("üìã Fetching report metadata...")
                if not report_ids:
                    reports = []
                else:
                    chunk_size = 50
                    reports = []
                    
                    for i in range(0, len(report_ids), chunk_size):
                        if cancel_event and cancel_event.is_set():
                            raise Exception("Export cancelled by user")
                        
                        chunk_ids = report_ids[i:i + chunk_size]
                        ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                        
                        base_query = f"""
                            SELECT Id, Name, Format 
                            FROM Report 
                            WHERE Id IN ({ids_formatted})
                        """
                        
                        try:
                            chunk_records = self._query_with_pagination(
                                base_query.strip(), 
                                batch_size=2000,
                                cancel_event=cancel_event
                            )
                            
                            for record in chunk_records:
                                reports.append({
                                    "id": record.get("Id"),
                                    "name": record.get("Name"),
                                    "reportFormat": record.get("Format", "TABULAR")
                                })
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error fetching report chunk {i//chunk_size + 1}: {str(e)[:100]}")
                            for rid in chunk_ids:
                                reports.append({
                                    "id": rid,
                                    "name": rid,
                                    "reportFormat": "TABULAR"
                                })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []
            used_filenames: Dict[str, int] = {}
            
            # Track export methods used
            export_methods = {
                "csv": 0,              # CSV succeeded
                "excel_fallback": 0    # CSV failed ‚Üí Excel fallback
            }
            
            # Thread-safe counters
            completed_lock = threading.Lock()
            
            # Adaptive worker count
            if total > 5000:
                max_workers = min(max_workers, 8)
                print(f"‚öôÔ∏è Large export detected ({total} reports), using {max_workers} workers")
            elif total > 1000:
                max_workers = min(max_workers, 10)
                print(f"‚öôÔ∏è Using {max_workers} workers for {total} reports")
            
            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports (CSV)",
                    "api_version": self.api_version,
                    "cancelled": False,
                    "completed": 0,
                    "method_used": export_methods
                }
            
            # ===== STEP 2: Define worker function =====
            def export_single_report(report: Dict) -> tuple:
                """Export a single report - tries CSV, falls back to Excel if format unsupported"""
                nonlocal completed
                
                if cancel_event and cancel_event.is_set():
                    return ("cancelled", report, None)
                
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")
                
                # Notify: Starting download
                if self.progress_callback:
                    try:
                        with completed_lock:
                            current_count = completed
                        self.progress_callback(current_count, total, report_name)
                    except:
                        pass
                
                # Generate filename (thread-safe)
                base_name = safe_filename(report_name)
                
                with completed_lock:
                    if base_name in used_filenames:
                        used_filenames[base_name] += 1
                        file_number = used_filenames[base_name]
                    else:
                        used_filenames[base_name] = 1
                        file_number = 1
                
                # ===== NEW: CSV Export with Excel Fallback =====
                export_method = None
                file_content = None
                file_extension = None
                last_error = None
                
                # ‚úÖ IMPROVED: Pre-check if report type is known to be incompatible with CSV
                should_skip_csv = report_type in ("JOINED", "MultiBlock")
                
                if should_skip_csv:
                    print(f"‚ö†Ô∏è {report_type} report detected: {report_name}")
                    print(f"   Skipping CSV, using Excel directly")
                    # Don't even try CSV for Joined/MultiBlock reports
                    last_error = f"{report_type} reports are not supported in CSV format"
                else:
                    # ATTEMPT 1: Try CSV first (user's choice)
                    for attempt in range(retry_attempts):
                        if cancel_event and cancel_event.is_set():
                            return ("cancelled", report, None)
                        
                        try:
                            timeout = 180 if total > 5000 else 120
                            
                            print(f"üìÑ [{attempt + 1}/{retry_attempts}] Trying CSV: {report_name}")
                            
                            csv_content = self.export_report_csv(report_id, timeout=timeout)
                            
                            if not csv_content or len(csv_content.strip()) == 0:
                                raise Exception("Empty CSV response")
                            
                            # Check if it's an error page (HTML instead of CSV)
                            if csv_content.strip().startswith('<!DOCTYPE') or csv_content.strip().startswith('<html'):
                                raise Exception("Received HTML error page instead of CSV")
                            
                            # Success!
                            file_content = csv_content.encode('utf-8')
                            file_extension = ".csv"
                            export_method = "csv"
                            print(f"   ‚úÖ CSV downloaded: {len(csv_content)} bytes")
                            break
                            
                        except Exception as e:
                            last_error = str(e)
                            
                            # Check if this is a format incompatibility error
                            is_format_error = self._is_csv_format_error(last_error, report_type)
                            
                            if is_format_error:
                                print(f"   ‚ö†Ô∏è CSV format not supported for {report_type} report")
                                break  # Don't retry CSV, go straight to Excel fallback
                            
                            # Not a format error - might be temporary issue
                            if attempt < retry_attempts - 1:
                                wait_time = (2 ** attempt) + (attempt * 0.5)
                                print(f"   ‚ö†Ô∏è CSV failed, retry in {wait_time}s: {last_error[:50]}")
                                time.sleep(wait_time)
                                continue
                            else:
                                print(f"   ‚ùå CSV failed after {retry_attempts} attempts")
                                break
                
                # ATTEMPT 2: Excel Fallback (if CSV failed due to format)
                if file_content is None:
                    # ‚úÖ IMPROVED: Better decision on whether to try Excel
                    should_try_excel = (
                        last_error and 
                        (self._is_csv_format_error(last_error, report_type) or should_skip_csv)
                    )
                    
                    if should_try_excel:
                        print(f"üìä CSV‚ÜíExcel fallback: {report_name} ({report_type})")
                        
                        for attempt in range(retry_attempts):
                            if cancel_event and cancel_event.is_set():
                                return ("cancelled", report, None)
                            
                            try:
                                timeout = 180 if total > 5000 else 120
                                
                                print(f"üìä [{attempt + 1}/{retry_attempts}] Trying Excel fallback: {report_name}")
                                
                                excel_content = self.export_report_excel_native(report_id, timeout=timeout)
                                
                                if not excel_content:
                                    raise Exception("Empty Excel response")
                                
                                # Success!
                                file_content = excel_content
                                file_extension = ".xlsx"
                                export_method = "excel_fallback"
                                print(f"   ‚úÖ Excel fallback successful: {len(excel_content)} bytes")
                                break
                                
                            except Exception as e:
                                excel_error = str(e)
                                
                                if attempt < retry_attempts - 1:
                                    wait_time = (2 ** attempt) + (attempt * 0.5)
                                    print(f"   ‚ö†Ô∏è Excel fallback failed, retry in {wait_time}s: {excel_error[:50]}")
                                    time.sleep(wait_time)
                                    continue
                                else:
                                    print(f"   ‚ùå Excel fallback failed after {retry_attempts} attempts")
                                    last_error = f"CSV failed ({last_error[:100]}), Excel fallback also failed ({excel_error[:100]})"
                                    break
                
                # ===== SAVE FILE OR RECORD FAILURE =====
                if file_content:
                    # Build filename with correct extension
                    if file_number > 1:
                        filename = f"{base_name}_{file_number}{file_extension}"
                    else:
                        filename = f"{base_name}{file_extension}"
                    
                    file_path = tmp_dir / filename
                    
                    # Write file
                    if file_extension == ".xlsx":
                        file_path.write_bytes(file_content)
                    else:
                        file_path.write_bytes(file_content)
                    
                    # Update counter
                    with completed_lock:
                        completed += 1
                        current_count = completed
                        export_methods[export_method] += 1
                    
                    # Notify: Report completed
                    if self.progress_callback:
                        try:
                            self.progress_callback(current_count, total)
                        except:
                            pass
                    
                    return ("success", report, filename, export_method)
                
                else:
                    # Both CSV and Excel (if attempted) failed - create error file
                    error_content = (
                        f"# Failed to export report after {retry_attempts} attempts\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"#\n"
                        f"# Error: {last_error}\n"
                        f"#\n"
                        f"# This report could not be downloaded in any format.\n"
                        f"# Please try downloading it manually from Salesforce UI.\n"
                    )
                    
                    if file_number > 1:
                        error_filename = f"{base_name}_{file_number}_ERROR.txt"
                    else:
                        error_filename = f"{base_name}_ERROR.txt"
                    
                    error_path = tmp_dir / error_filename
                    error_path.write_text(error_content, encoding="utf-8")
                    
                    # Update counter
                    with completed_lock:
                        completed += 1
                        current_count = completed
                    
                    # Notify: Report failed
                    if self.progress_callback:
                        try:
                            self.progress_callback(current_count, total)
                        except:
                            pass
                    
                    return ("failed", report, last_error, None)
            
            # ===== STEP 3: Export reports concurrently =====
            print(f"üöÄ Starting concurrent export with {max_workers} workers...")
            print(f"üìÑ Export method: CSV (fast, lightweight)")
            print(f"üìä Fallback: Excel for Joined/Matrix reports")
            print(f"‚úÖ Supports: All report types with automatic fallback")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all tasks
                future_to_report = {
                    executor.submit(export_single_report, report): report 
                    for report in reports
                }
                
                # Track progress milestones
                last_milestone = 0
                milestone_interval = max(100, total // 20)
                
                # Process completed tasks
                for future in as_completed(future_to_report):
                    if cancel_event and cancel_event.is_set():
                        print("‚ö†Ô∏è Cancellation detected, stopping remaining downloads...")
                        for f in future_to_report:
                            f.cancel()
                        break
                    
                    try:
                        status, report, data, method = future.result()
                        
                        if status == "success":
                            successful.append(report.get("name"))
                        elif status == "failed":
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": data
                            })
                        elif status == "cancelled":
                            pass
                        
                        # Log progress milestones
                        if completed - last_milestone >= milestone_interval:
                            success_rate = (len(successful) / completed * 100) if completed > 0 else 0
                            csv_count = export_methods["csv"]
                            excel_count = export_methods["excel_fallback"]
                            print(f"üìä Progress: {completed}/{total} ({completed/total*100:.1f}%) - Success: {success_rate:.1f}%")
                            print(f"   üìÑ CSV: {csv_count} | üìä Excel fallback: {excel_count}")
                            last_milestone = completed
                        
                        # Update progress callback
                        if self.progress_callback:
                            try:
                                self.progress_callback(completed, total)
                            except Exception:
                                pass
                                
                    except Exception as e:
                        report = future_to_report.get(future)
                        if report:
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": str(e)
                            })
                            print(f"‚ö†Ô∏è Future error for {report.get('name')}: {str(e)[:100]}")
            
            # Check if cancelled
            was_cancelled = cancel_event and cancel_event.is_set()
            
            # ===== STEP 4: Create ZIP file =====
            print(f"üì¶ Creating ZIP file with {completed} reports...")
            
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                # Write files in sorted order
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                # Create enhanced summary
                summary = self._create_summary(
                    total, 
                    successful, 
                    failed, 
                    "Selected Reports (CSV)" + (" (CANCELLED)" if was_cancelled else "")
                )
                
                # Add export method statistics
                summary += "\n\n"
                summary += "=" * 50 + "\n"
                summary += "EXPORT METHOD STATISTICS\n"
                summary += "=" * 50 + "\n"
                summary += f"CSV Exports: {export_methods['csv']}\n"
                summary += f"Excel Fallbacks: {export_methods['excel_fallback']}\n"
                summary += f"Failed: {len(failed)}\n"
                summary += "\n"
                summary += "=" * 50 + "\n"
                summary += "CSV EXPORT FORMAT INFORMATION\n"
                summary += "=" * 50 + "\n"
                summary += "Primary Method: CSV Export (fast, lightweight)\n"
                summary += "Fallback Method: Excel Export (for unsupported formats)\n"
                summary += "\n"
                summary += "‚úÖ CSV exports preserve data in simple format:\n"
                summary += "  ‚Ä¢ Fast export for Tabular/Summary reports\n"
                summary += "  ‚Ä¢ Easy to process in spreadsheets/databases\n"
                summary += "  ‚Ä¢ Lightweight file size\n"
                summary += "\n"
                summary += "üìä Excel Fallbacks (for unsupported formats):\n"
                summary += f"  ‚Ä¢ {export_methods['excel_fallback']} reports exported as Excel\n"
                summary += "  ‚Ä¢ Used when CSV format doesn't support:\n"
                summary += "    - Joined reports (multi-block)\n"
                summary += "    - Complex Matrix reports\n"
                summary += "  ‚Ä¢ Preserves ALL formatting (groupings, subtotals)\n"
                summary += "\n"
                summary += "Report Type Compatibility:\n"
                summary += "  ‚Ä¢ Tabular:  ‚úÖ CSV (default)\n"
                summary += "  ‚Ä¢ Summary:  ‚úÖ CSV (default)\n"
                summary += "  ‚Ä¢ Matrix:   ‚úÖ CSV (‚ö†Ô∏è Excel fallback if complex)\n"
                summary += "  ‚Ä¢ Joined:   ‚ùå CSV not supported ‚Üí Excel fallback\n"
                summary += "=" * 50 + "\n"
                
                zf.writestr("_EXPORT_SUMMARY.txt", summary)
            
            # Final statistics
            success_rate = (len(successful) / total * 100) if total > 0 else 0
            print(f"‚úÖ Export complete: {len(successful)}/{total} successful ({success_rate:.1f}%)")
            print(f"   üìÑ CSV Exports: {export_methods['csv']}")
            print(f"   üìä Excel Fallbacks: {export_methods['excel_fallback']}")
            if failed:
                print(f"   ‚ùå Failed: {len(failed)} reports")
            
            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports (CSV)",
                "api_version": self.api_version,
                "cancelled": was_cancelled,
                "completed": completed,
                "method_used": export_methods
            }
        
        finally:
            # Cleanup temporary files
            try:
                shutil.rmtree(tmp_dir)
                print(f"üßπ Cleaned up temporary files")
            except Exception as e:
                print(f"‚ö†Ô∏è Error cleaning temp directory: {str(e)[:100]}")




    def _get_folder_name(self, folder_id: str) -> str:
        """Get the name of a folder by its ID"""
        try:
            url = f"{self.instance_url}/services/data/{self.api_version}/sobjects/Folder/{folder_id}"
            response = retry_request(url, headers=self.api_headers, timeout=30)
            data = response.json()
            return data.get("Name", folder_id)
        except:
            return folder_id

    def _create_summary(
        self,
        total: int,
        successful: List[str],
        failed: List[Dict[str, Any]],
        folder_name: str = "Unknown"
    ) -> str:
        """Create a summary text file for the export."""
        lines = [
            "SALESFORCE REPORT EXPORT SUMMARY",
            "=" * 40,
            f"Export Date: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"Instance: {self.instance_url}",
            f"API Version: {self.api_version}",
            f"Folder: {folder_name}",
            "",
            f"Total Reports: {total}",
            f"Successful: {len(successful)}",
            f"Failed: {len(failed)}",
            "",
        ]
        
        if failed:
            lines.append("FAILED REPORTS:")
            lines.append("-" * 40)
            for f in failed:
                lines.append(f"‚Ä¢ {f.get('name')} ({f.get('type')})")
                lines.append(f"  ID: {f.get('id')}")
                lines.append(f"  Error: {f.get('error')}")
                lines.append("")
        
        return "\n".join(lines)
    

# TEMPORARY TEST - Remove after verification
if __name__ == "__main__":
    # Test version check
    class MockExporter:
        def __init__(self, api_version):
            self.api_version = api_version
        
        def _check_api_version_for_excel(self):
            try:
                version_str = self.api_version.replace('v', '').replace('V', '')
                version_float = float(version_str)
                return version_float >= 39.0
            except:
                return True
    
    # Test cases
    test_versions = ["v61.0", "v39.0", "v38.0", "v25.0"]
    
    for version in test_versions:
        mock = MockExporter(version)
        supported = mock._check_api_version_for_excel()
        status = "‚úÖ SUPPORTED" if supported else "‚ùå NOT SUPPORTED"
        print(f"{version}: {status}")
    
    # Expected output:
    # v61.0: ‚úÖ SUPPORTED
    # v39.0: ‚úÖ SUPPORTED
    # v38.0: ‚ùå NOT SUPPORTED
    # v25.0: ‚ùå NOT SUPPORTED