# exporter.py - Part 1: Helper Functions and Setup
# Salesforce Report Exporter with DYNAMIC API version detection

import time
import tempfile
import zipfile
import shutil
from pathlib import Path
import requests
from typing import Callable, Optional, List, Dict, Any
import threading


def get_org_api_version(instance_url: str, session_id: str = None) -> str:
    """
    Fetch the latest API version supported by the Salesforce org.
    This endpoint doesn't require authentication.
    
    Args:
        instance_url: The Salesforce instance URL
        session_id: Optional session ID (not required for this call)
        
    Returns:
        Latest API version string (e.g., "v61.0")
    """
    try:
        url = f"{instance_url.rstrip('/')}/services/data/"
        response = requests.get(url, timeout=15)
        
        if response.status_code == 200:
            versions = response.json()
            if versions and len(versions) > 0:
                # Get the latest (last) version
                latest = versions[-1]
                version = latest.get("version", "58.0")
                return f"v{version}"
    except Exception:
        pass
    
    # Fallback to a safe default
    return "v58.0"


def retry_request(
    url: str,
    headers: dict = None,
    cookies: dict = None,
    max_retries: int = 3,
    timeout: int = 60,
    allow_redirects: bool = True,
    backoff_factor: float = 2.0  # ‚úÖ NEW: Configurable backoff
) -> requests.Response:
    """
    Make HTTP GET request with exponential backoff retry logic.
    
    ‚úÖ OPTIMIZED: Better rate limit handling for large exports.
    """
    backoff = 1
    last_error = None
    headers = headers or {}
    cookies = cookies or {}

    for attempt in range(max_retries):
        try:
            response = requests.get(
                url,
                headers=headers,
                cookies=cookies,
                timeout=timeout,
                allow_redirects=allow_redirects
            )

            if response.status_code == 200:
                return response

            # ‚úÖ IMPROVED: Better rate limit handling
            if response.status_code == 429:  # Too Many Requests
                retry_after = response.headers.get('Retry-After')
                if retry_after:
                    try:
                        backoff = int(retry_after)
                    except ValueError:
                        backoff = min(backoff * backoff_factor, 120)  # Cap at 2 minutes
                else:
                    backoff = min(backoff * backoff_factor, 120)
                
                print(f"‚ö†Ô∏è Rate limited, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue

            # ‚úÖ IMPROVED: Better handling of server errors
            if response.status_code in (500, 502, 503, 504):
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Server error {response.status_code}, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue

            response.raise_for_status()

        except requests.Timeout as e:
            last_error = e
            if attempt < max_retries - 1:
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Timeout, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue
            raise
        except requests.RequestException as e:
            last_error = e
            if attempt < max_retries - 1:
                backoff = min(backoff * backoff_factor, 60)
                print(f"‚ö†Ô∏è Request error, waiting {backoff}s before retry {attempt + 1}/{max_retries}")
                time.sleep(backoff)
                continue
            raise

    raise Exception(f"Request failed after {max_retries} retries: {last_error}")


def safe_filename(name: str, max_length: int = 100) -> str:
    """Sanitize filename by removing invalid characters."""
    if not name:
        return "unnamed_report"
    safe = "".join(c if c.isalnum() or c in " ._-" else "_" for c in name)
    while "__" in safe:
        safe = safe.replace("__", "_")
    safe = safe.strip("_ ")
    return safe[:max_length] if safe else "unnamed_report"


def clean_csv_footer(csv_content: str) -> str:
    """
    Remove Salesforce's metadata footer from CSV content.
    
    The footer typically contains:
    - Report name
    - Copyright notice
    - Confidential information warning
    - Generated by information
    
    Args:
        csv_content: Raw CSV content from Salesforce
        
    Returns:
        Cleaned CSV content without footer
    """
    lines = csv_content.split('\n')
    
    cleaned_lines = []
    footer_started = False
    blank_line_count = 0
    
    for i, line in enumerate(lines):
        stripped = line.strip()
        
        if not footer_started:
            if not stripped:
                blank_line_count += 1
                cleaned_lines.append(line)
            else:
                blank_line_count = 0
                
                footer_indicators = [
                    'Copyright (c)',
                    'Confidential Information',
                    'Generated By:',
                    '¬© Copyright',
                    'Do Not Distribute'
                ]
                
                if any(indicator in line for indicator in footer_indicators):
                    footer_started = True
                    while cleaned_lines and not cleaned_lines[-1].strip():
                        cleaned_lines.pop()
                elif ',' not in stripped and len(stripped) > 0:
                    next_lines = lines[i+1:min(i+5, len(lines))]
                    next_text = ' '.join(next_lines)
                    if any(indicator in next_text for indicator in footer_indicators):
                        footer_started = True
                        while cleaned_lines and not cleaned_lines[-1].strip():
                            cleaned_lines.pop()
                    else:
                        cleaned_lines.append(line)
                else:
                    cleaned_lines.append(line)
    
    result = '\n'.join(cleaned_lines)
    result = result.rstrip('\n')
    
    return result

# exporter.py - Part 2: SalesforceReportExporter Class
# This continues from Part 1 (helper functions)

class SalesforceReportExporter:
    """
    Export Salesforce reports to CSV files and package them into a ZIP.
    
    Uses TWO methods:
    1. REST API to get list of reports (with metadata)
    2. UI Export URL to download actual CSV (bypasses 2000 row limit)
    
    API version is detected dynamically from the org.
    """

    def __init__(
        self,
        session_id: str,
        instance_url: str,
        api_version: str = None,
        progress_callback: Optional[Callable[[int, int], None]] = None
    ):
        self.session_id = session_id
        self.instance_url = instance_url.rstrip('/')
        self.progress_callback = progress_callback
        
        # Get API version dynamically if not provided
        if api_version:
            self.api_version = api_version if api_version.startswith('v') else f"v{api_version}"
        else:
            self.api_version = get_org_api_version(self.instance_url)
        
        # Build endpoints with dynamic version
        self.reports_list_endpoint = f"/services/data/{self.api_version}/analytics/reports"
        self.folders_list_endpoint = f"/services/data/{self.api_version}/folders"
        
        # Headers for REST API calls (list reports)
        self.api_headers = {
            "Authorization": f"Bearer {self.session_id}",
            "Accept": "application/json"
        }
        
        # Cookies for UI export (CSV download)
        self.export_cookies = {
            "sid": self.session_id
        }
    
    def _query_with_pagination(
        self,
        base_query: str,
        batch_size: int = 2000,
        progress_callback: Optional[Callable[[int, int], None]] = None,
        cancel_event: Optional[Any] = None  # ‚úÖ NEW: Cancellation support
    ) -> List[Dict[str, Any]]:
        """
        Execute SOQL query with automatic pagination.
        Handles Salesforce's 2,000 row limit per query.
        
        ‚úÖ OPTIMIZED: Cancellation support + better timeout handling for large queries.
        
        Args:
            base_query: Base SOQL query (without LIMIT/OFFSET)
            batch_size: Records per batch (default 2000, Salesforce limit)
            progress_callback: Optional callback(fetched, estimated_total)
            cancel_event: Optional threading.Event to check for cancellation
            
        Returns:
            List of all records combined from all pages
        """
        all_records = []
        offset = 0
        has_more = True
        estimated_total = None
        consecutive_errors = 0  # ‚úÖ NEW: Track consecutive errors
        max_consecutive_errors = 3  # ‚úÖ NEW: Fail after 3 consecutive errors
        
        while has_more:
            # ‚úÖ NEW: Check for cancellation
            if cancel_event and cancel_event.is_set():
                print(f"‚ö†Ô∏è Query cancelled at offset {offset}")
                break
            
            # Build paginated query
            paginated_query = f"{base_query} LIMIT {batch_size} OFFSET {offset}"
            
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": paginated_query}
            
            try:
                # ‚úÖ IMPROVED: Longer timeout for large queries
                timeout = 90 if len(all_records) > 5000 else 60
                
                response = requests.get(
                    query_url,
                    headers=self.api_headers,
                    params=params,
                    timeout=timeout
                )
                response.raise_for_status()
                
                data = response.json()
                records = data.get("records", [])
                
                if not records:
                    has_more = False
                    break
                
                all_records.extend(records)
                offset += len(records)
                
                # ‚úÖ RESET: Reset error counter on success
                consecutive_errors = 0
                
                # Update progress if callback provided
                if progress_callback and estimated_total is None:
                    # Estimate total based on first batch
                    if len(records) == batch_size:
                        estimated_total = batch_size * 10  # Rough estimate
                    else:
                        estimated_total = len(records)
                
                if progress_callback:
                    progress_callback(len(all_records), estimated_total or len(all_records))
                
                # If we got fewer records than batch_size, we're done
                if len(records) < batch_size:
                    has_more = False
                
                # ‚úÖ NEW: Small delay to avoid rate limiting on large queries
                if has_more and len(all_records) > 0 and len(all_records) % 10000 == 0:
                    print(f"üìä Fetched {len(all_records)} records, brief pause to avoid rate limits...")
                    time.sleep(1)
                
            except requests.Timeout as e:
                consecutive_errors += 1
                print(f"‚ö†Ô∏è Query timeout at offset {offset} (attempt {consecutive_errors}/{max_consecutive_errors})")
                
                if consecutive_errors >= max_consecutive_errors:
                    print(f"‚ùå Too many consecutive errors, stopping pagination at {len(all_records)} records")
                    has_more = False
                else:
                    # Wait before retry
                    time.sleep(2 * consecutive_errors)
                    continue
                    
            except requests.RequestException as e:
                consecutive_errors += 1
                print(f"‚ö†Ô∏è Query error at offset {offset}: {str(e)[:100]} (attempt {consecutive_errors}/{max_consecutive_errors})")
                
                if consecutive_errors >= max_consecutive_errors:
                    print(f"‚ùå Too many consecutive errors, stopping pagination at {len(all_records)} records")
                    has_more = False
                else:
                    # Wait before retry
                    time.sleep(2 * consecutive_errors)
                    continue
        
        return all_records

    def list_all_report_folders(self) -> List[Dict[str, Any]]:
        """
        Fetch list of all report folders in the org that the user has access to.
        Returns list of folder metadata (id, name, type, etc.)
        Filters out system/automated folders.
        """
        try:
            # Query for Report folders that user has access to
            query = """
                SELECT Id, Name, Type, DeveloperName, AccessType 
                FROM Folder 
                WHERE Type = 'Report' 
                ORDER BY Name
            """
            
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": query}
            
            response = requests.get(
                query_url, 
                headers=self.api_headers, 
                params=params, 
                timeout=30
            )
            response.raise_for_status()
            
            data = response.json()
            folders = data.get("records", [])
            
            # Clean up the response - convert to simple dict
            cleaned_folders = []
            for folder in folders:
                cleaned_folders.append({
                    "id": folder.get("Id"),
                    "name": folder.get("Name"),
                    "type": folder.get("Type"),
                    "developerName": folder.get("DeveloperName"),
                    "accessType": folder.get("AccessType")
                })
            
            return cleaned_folders
            
        except Exception as e:
            raise Exception(f"Failed to fetch report folders: {str(e)}")

    def list_all_reports(self, folder_id: str = None) -> List[Dict[str, Any]]:
        """
        Fetch list of all available reports using REST API or SOQL query.
        
        Args:
            folder_id: Optional folder ID to filter reports. If None, returns all reports.
            
        Returns:
            List of report metadata (id, name, format, folder, etc.)
        """
        # If folder_id is specified, use SOQL query for accurate filtering
        if folder_id:
            return self._list_reports_by_soql(folder_id)
        
        # Otherwise use the standard REST API endpoint
        url = f"{self.instance_url}{self.reports_list_endpoint}"
        response = retry_request(url, headers=self.api_headers, timeout=60)
        
        data = response.json()
        
        if isinstance(data, list):
            reports = data
        elif isinstance(data, dict):
            reports = data.get("reports", data.get("records", []))
        else:
            reports = []
        
        return reports


    def _list_reports_by_soql(self, folder_id: str) -> List[Dict[str, Any]]:
        """
        Use SOQL query with pagination to get reports by folder ID.
        Now handles unlimited reports per folder.
        
        Args:
            folder_id: The Salesforce folder ID to query
            
        Returns:
            List of report metadata in the same format as list_reports()
        """
        try:
            # SOQL query to get reports in specific folder
            base_query = f"""
                SELECT Id, Name, DeveloperName, FolderName, Format, CreatedDate, LastModifiedDate
                FROM Report 
                WHERE OwnerId = '{folder_id}'
                ORDER BY Name
            """
            
            # Use pagination helper
            records = self._query_with_pagination(base_query.strip())
            
            # Convert SOQL results to match REST API format
            reports = []
            for record in records:
                reports.append({
                    "id": record.get("Id"),
                    "name": record.get("Name"),
                    "developerName": record.get("DeveloperName"),
                    "folderName": record.get("FolderName"),
                    "reportFormat": record.get("Format", "TABULAR"),
                    "lastModifiedDate": record.get("LastModifiedDate"),
                    "createdDate": record.get("CreatedDate")
                })
            
            return reports
            
        except Exception as e:
            print(f"Error querying reports by folder: {str(e)}")
            return []


    def search_by_keyword(self, keyword: str, cancel_event=None) -> Dict[str, Any]:
        """
        Search folders AND reports by keyword, then organize results.
        
        ‚úÖ NEW: Creates a virtual "Unified Public Folder" for orphaned reports
        that match the search but whose folder wasn't found.
        """
        try:
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # Escape keyword for SOQL (prevent injection)
            keyword_escaped = keyword.replace("'", "\\'").replace("%", "\\%")
            
            # ===== STEP 1: Search folders by name =====
            print(f"üîç Step 1: Searching folders matching '{keyword}'...")
            
            folders_query = f"""
                SELECT Id, Name, Type, DeveloperName, AccessType 
                FROM Folder 
                WHERE Type = 'Report' 
                AND (
                    Name LIKE '%{keyword_escaped}%'
                    OR DeveloperName LIKE '%{keyword_escaped}%'
                )
                ORDER BY Name
            """
            
            matching_folders = self._execute_soql_query(folders_query)
            folder_ids_from_name_match = {f.get("Id") for f in matching_folders}
            
            print(f"‚úÖ Found {len(matching_folders)} folders matching keyword")
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 2: Search reports by name (WITH PAGINATION) =====
            print(f"üîç Step 2: Searching reports matching '{keyword}'...")
            
            reports_query = f"""
                SELECT Id, Name, DeveloperName, FolderName, Format, 
                    CreatedDate, LastModifiedDate, OwnerId
                FROM Report 
                WHERE Name LIKE '%{keyword_escaped}%' OR FolderName LIKE '%{keyword_escaped}%'
                ORDER BY Name
            """
            
            # ‚úÖ OPTIMIZED: Use pagination with cancellation support
            matching_reports = self._query_with_pagination(
                reports_query.strip(),
                batch_size=2000,
                cancel_event=cancel_event
            )
            
            print(f"‚úÖ Found {len(matching_reports)} reports matching keyword")
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 3: Get folder IDs from matching reports =====
            folder_ids_from_reports = {r.get("OwnerId") for r in matching_reports if r.get("OwnerId")}
            
            # ===== STEP 4: Fetch folders that contain matching reports =====
            additional_folder_ids = folder_ids_from_reports - folder_ids_from_name_match
            
            additional_folders = []
            if additional_folder_ids:
                print(f"üîç Step 3: Fetching {len(additional_folder_ids)} additional folders...")
                
                # ‚úÖ OPTIMIZED: Process in smaller chunks to avoid query string limits
                folder_ids_list = list(additional_folder_ids)
                chunk_size = 50  # ‚úÖ REDUCED: Smaller chunks for stability
                
                for i in range(0, len(folder_ids_list), chunk_size):
                    # Check cancellation
                    if cancel_event and cancel_event.is_set():
                        return {"folders": [], "reports_by_folder": {}}
                    
                    chunk = folder_ids_list[i:i + chunk_size]
                    ids_str = ",".join([f"'{fid}'" for fid in chunk])
                    
                    folders_query = f"""
                        SELECT Id, Name, Type, DeveloperName, AccessType 
                        FROM Folder 
                        WHERE Id IN ({ids_str})
                    """
                    
                    try:
                        chunk_folders = self._execute_soql_query(folders_query)
                        additional_folders.extend(chunk_folders)
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error fetching folder chunk: {str(e)[:100]}")
                        # Continue with other chunks
                        continue
            
            print(f"‚úÖ Fetched {len(additional_folders)} additional folders")
            
            # ===== STEP 5: Identify orphaned reports =====
            # These are reports that matched the search but whose OwnerId
            # doesn't correspond to any folder we successfully fetched
            
            all_fetched_folder_ids = folder_ids_from_name_match | {f.get("Id") for f in additional_folders}
            
            orphaned_reports = []
            valid_reports = []
            
            for report in matching_reports:
                owner_id = report.get("OwnerId")
                
                if not owner_id or owner_id not in all_fetched_folder_ids:
                    # This report's folder wasn't found - it's orphaned
                    orphaned_reports.append(report)
                else:
                    # This report has a valid folder
                    valid_reports.append(report)
            
            print(f"üìä Report classification:")
            print(f"  ‚úÖ Valid reports (with folders): {len(valid_reports)}")
            print(f"  ‚ö†Ô∏è Orphaned reports (no folder found): {len(orphaned_reports)}")
            
            # ===== STEP 5B: Create virtual "Unified Public Folder" if needed =====
            virtual_folder_id = None
            virtual_folder = None
            
            if orphaned_reports:
                # Create a virtual folder to hold orphaned reports
                virtual_folder_id = "VIRTUAL_UNIFIED_PUBLIC_FOLDER"
                
                virtual_folder = {
                    "Id": virtual_folder_id,
                    "Name": "üìÅ Unified Public Folder (Search Results)",
                    "Type": "Report",
                    "DeveloperName": "UnifiedPublicFolder",
                    "AccessType": "Public"
                }
                
                print(f"üÜï Created virtual folder for {len(orphaned_reports)} orphaned reports")
            
            # ===== STEP 6: Combine all folders (virtual first, then real folders) =====
            all_folders = []
            
            # ‚úÖ Add virtual folder at TOP if it exists
            if virtual_folder:
                all_folders.append(virtual_folder)
            
            # Add real folders (name-matched + additional)
            all_folders.extend(matching_folders)
            all_folders.extend(additional_folders)
            
            # ===== STEP 7: Group reports by folder (MEMORY EFFICIENT) =====
            print(f"üìä Grouping reports by folder...")
            
            reports_by_folder = {}
            
            # ‚úÖ Add orphaned reports to virtual folder FIRST
            if virtual_folder_id and orphaned_reports:
                reports_by_folder[virtual_folder_id] = []
                
                for report in orphaned_reports:
                    reports_by_folder[virtual_folder_id].append({
                        "id": report.get("Id"),
                        "name": report.get("Name"),
                        "developerName": report.get("DeveloperName"),
                        "folderName": report.get("FolderName") or "Unified Public Folder",
                        "reportFormat": report.get("Format", "TABULAR"),
                        "lastModifiedDate": report.get("LastModifiedDate"),
                        "createdDate": report.get("CreatedDate")
                    })
                
                print(f"  ‚úÖ Virtual folder: {len(orphaned_reports)} orphaned reports")
            
            # Group valid reports by their actual folders
            for report in valid_reports:
                folder_id = report.get("OwnerId")
                if folder_id:
                    if folder_id not in reports_by_folder:
                        reports_by_folder[folder_id] = []
                    
                    reports_by_folder[folder_id].append({
                        "id": report.get("Id"),
                        "name": report.get("Name"),
                        "developerName": report.get("DeveloperName"),
                        "folderName": report.get("FolderName"),
                        "reportFormat": report.get("Format", "TABULAR"),
                        "lastModifiedDate": report.get("LastModifiedDate"),
                        "createdDate": report.get("CreatedDate")
                    })
            
            # Check cancellation
            if cancel_event and cancel_event.is_set():
                return {"folders": [], "reports_by_folder": {}}
            
            # ===== STEP 8: For folders matched by name, get ALL their reports (IN CHUNKS) =====
            print(f"üîç Step 4: Fetching all reports from {len(matching_folders)} matched folders...")
            
            for idx, folder in enumerate(matching_folders):
                folder_id = folder.get("Id")
                
                # Check cancellation
                if cancel_event and cancel_event.is_set():
                    return {"folders": [], "reports_by_folder": {}}
                
                # If this folder doesn't have any reports yet (from keyword search),
                # fetch ALL reports in this folder
                if folder_id not in reports_by_folder:
                    reports_query = f"""
                        SELECT Id, Name, DeveloperName, FolderName, Format, 
                            CreatedDate, LastModifiedDate
                        FROM Report 
                        WHERE OwnerId = '{folder_id}'
                        ORDER BY Name
                    """
                    
                    try:
                        # ‚úÖ OPTIMIZED: Use pagination with cancellation
                        folder_reports = self._query_with_pagination(
                            reports_query.strip(),
                            batch_size=2000,
                            cancel_event=cancel_event
                        )
                        
                        if folder_reports:
                            reports_by_folder[folder_id] = [
                                {
                                    "id": r.get("Id"),
                                    "name": r.get("Name"),
                                    "developerName": r.get("DeveloperName"),
                                    "folderName": r.get("FolderName"),
                                    "reportFormat": r.get("Format", "TABULAR"),
                                    "lastModifiedDate": r.get("LastModifiedDate"),
                                    "createdDate": r.get("CreatedDate")
                                }
                                for r in folder_reports
                            ]
                            
                            print(f"  ‚úÖ Folder {idx + 1}/{len(matching_folders)}: {len(folder_reports)} reports")
                        
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è Error fetching reports from folder {folder.get('Name')}: {str(e)[:100]}")
                        # Continue with other folders
                        continue
            
            # ===== STEP 9: Clean up folder metadata =====
            cleaned_folders = []
            for folder in all_folders:
                cleaned_folders.append({
                    "id": folder.get("Id"),
                    "name": folder.get("Name"),
                    "type": folder.get("Type"),
                    "developerName": folder.get("DeveloperName"),
                    "accessType": folder.get("AccessType")
                })
            
            # ===== FINAL: Calculate statistics =====
            total_reports = sum(len(reports) for reports in reports_by_folder.values())
            print(f"‚úÖ Search complete: {len(cleaned_folders)} folders, {total_reports} total reports")
            
            # ‚úÖ Log virtual folder stats if present
            if virtual_folder_id and virtual_folder_id in reports_by_folder:
                virtual_count = len(reports_by_folder[virtual_folder_id])
                print(f"  üìÅ Virtual folder contains: {virtual_count} orphaned reports")
            
            return {
                "folders": cleaned_folders,
                "reports_by_folder": reports_by_folder
            }
            
        except Exception as e:
            print(f"‚ùå Search error: {str(e)}")
            raise Exception(f"Search failed: {str(e)}")




  
    def _execute_soql_query(self, query: str) -> List[Dict]:
        """
        Helper method to execute a SOQL query and return records.
        
        Args:
            query: SOQL query string
            
        Returns:
            List of record dictionaries
            
        Raises:
            Exception: If query fails
        """
        try:
            query_url = f"{self.instance_url}/services/data/{self.api_version}/query"
            params = {"q": query.strip()}
            
            response = requests.get(
                query_url,
                headers=self.api_headers,
                params=params,
                timeout=30
            )
            response.raise_for_status()
            
            data = response.json()
            return data.get("records", [])
            
        except requests.RequestException as e:
            raise Exception(f"SOQL query failed: {str(e)}")


    def export_report_csv(self, report_id: str, timeout: int = 120) -> str:
        """
        Export a single report as CSV using the UI export URL method.
        
        ‚úÖ IMPROVED: Configurable timeout (default 120s, can be increased for large reports).
        
        This is the "screen scraping" approach that:
        - Bypasses the 2000 row API limit
        - Returns actual CSV content
        - Works with Lightning and Classic
        - Automatically removes Salesforce metadata footer
        
        Args:
            report_id: Salesforce report ID
            timeout: Request timeout in seconds (default 120)
        """
        # Build the export URL - mimics clicking "Export" in the UI
        export_url = (
            f"{self.instance_url}/{report_id}"
            f"?isdtp=p1&export=1&enc=UTF-8&xf=csv"
        )
        
        # ‚úÖ IMPROVED: Use retry_request with configurable timeout
        response = retry_request(
            export_url,
            cookies=self.export_cookies,
            timeout=timeout,
            allow_redirects=True,
            max_retries=3  # ‚úÖ NEW: Explicitly set retries
        )
        
        content = response.text
        
        # Check if we got HTML instead of CSV
        if content.strip().startswith('<!DOCTYPE') or content.strip().startswith('<html'):
            if 'login.salesforce.com' in content or 'ec=302' in content:
                raise Exception("Session expired or invalid. Please re-login.")
            elif 'You do not have access' in content:
                raise Exception("Access denied to this report.")
            else:
                raise Exception("Received HTML instead of CSV. Report may not be exportable.")
        
        # Clean the CSV content (remove footer)
        cleaned_content = clean_csv_footer(content)
        
        return cleaned_content
    
    def _csv_to_excel(self, csv_content: str, output_path: str) -> bool:
        """
        Convert CSV content to Excel (.xlsx) file.
        
        ‚úÖ Thread-safe conversion with proper error handling
        
        Args:
            csv_content: Raw CSV string content
            output_path: Path where .xlsx file should be saved
            
        Returns:
            True if conversion successful, False otherwise
        """
        try:
            import csv
            from io import StringIO
            
            # Try to import openpyxl for Excel writing
            try:
                from openpyxl import Workbook
                from openpyxl.styles import Font, PatternFill, Alignment
                from openpyxl.utils import get_column_letter
            except ImportError:
                print("‚ùå openpyxl not installed. Install with: pip install openpyxl")
                return False
            
            # Parse CSV content
            csv_reader = csv.reader(StringIO(csv_content))
            rows = list(csv_reader)
            
            if not rows:
                print("‚ö†Ô∏è No data to convert")
                return False
            
            # Create Excel workbook
            wb = Workbook()
            ws = wb.active
            ws.title = "Report Data"
            
            # Write data to Excel
            for row_idx, row_data in enumerate(rows, start=1):
                for col_idx, cell_value in enumerate(row_data, start=1):
                    cell = ws.cell(row=row_idx, column=col_idx, value=cell_value)
                    
                    # Style header row (first row)
                    if row_idx == 1:
                        cell.font = Font(bold=True, color="FFFFFF")
                        cell.fill = PatternFill(start_color="1F6AA5", end_color="1F6AA5", fill_type="solid")
                        cell.alignment = Alignment(horizontal="center", vertical="center")
            
            # Auto-adjust column widths (with reasonable limits)
            for column in ws.columns:
                max_length = 0
                column_letter = get_column_letter(column[0].column)
                
                for cell in column:
                    try:
                        if cell.value:
                            cell_length = len(str(cell.value))
                            if cell_length > max_length:
                                max_length = cell_length
                    except:
                        pass
                
                # Set width with min 10, max 50 characters
                adjusted_width = min(max(max_length + 2, 10), 50)
                ws.column_dimensions[column_letter].width = adjusted_width
            
            # Freeze header row
            ws.freeze_panes = "A2"
            
            # Save Excel file
            wb.save(output_path)
            
            return True
            
        except Exception as e:
            print(f"‚ùå CSV to Excel conversion error: {str(e)}")
            import traceback
            traceback.print_exc()
            return False   
    
    def _export_report_excel(self, report_id: str, timeout: int = 120) -> tuple[bool, str]:
        """
        Export a single report as Excel (.xlsx) file.
        
        ‚úÖ First exports as CSV, then converts to Excel
        
        Args:
            report_id: Salesforce report ID
            timeout: Request timeout in seconds
            
        Returns:
            (success, content_or_error) tuple
        """
        try:
            # Step 1: Get CSV content (reuse existing method)
            csv_content = self.export_report_csv(report_id, timeout=timeout)
            
            if not csv_content or len(csv_content.strip()) == 0:
                return (False, "Empty CSV content received")
            
            # CSV content is valid, return it for conversion
            # Conversion will happen in the caller to avoid temp file handling here
            return (True, csv_content)
            
        except Exception as e:
            error_msg = str(e)
            return (False, error_msg)    
    
    def _validate_excel_dependencies(self) -> tuple[bool, str]:
        """
        Check if required Excel libraries are installed.
        
        ‚úÖ Validates openpyxl availability
        
        Returns:
            (is_valid, error_message) tuple
        """
        try:
            import openpyxl
            return (True, "")
        except ImportError:
            error_msg = (
                "Excel export requires 'openpyxl' library.\n\n"
                "Install with:\n"
                "  pip install openpyxl\n\n"
                "Then restart the application."
            )
            return (False, error_msg)

    def _create_excel_summary(
        self,
        total: int,
        successful: List[str],
        failed: List[Dict[str, Any]],
        folder_name: str = "Unknown"
    ) -> str:
        """
        Create a summary for Excel exports.
        
        ‚úÖ Similar to _create_summary but Excel-specific
        
        Args:
            total: Total number of reports
            successful: List of successful report names
            failed: List of failed report dictionaries
            folder_name: Name of the folder/export batch
            
        Returns:
            Summary text string
        """
        lines = [
            "SALESFORCE REPORT EXPORT SUMMARY (EXCEL FORMAT)",
            "=" * 50,
            f"Export Date: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"Instance: {self.instance_url}",
            f"API Version: {self.api_version}",
            f"Folder: {folder_name}",
            f"Format: Excel (.xlsx)",
            "",
            f"Total Reports: {total}",
            f"Successful: {len(successful)}",
            f"Failed: {len(failed)}",
            "",
        ]
        
        if successful:
            lines.append("SUCCESSFUL EXPORTS:")
            lines.append("-" * 50)
            for name in successful[:20]:  # Limit to first 20
                lines.append(f"‚úì {name}")
            if len(successful) > 20:
                lines.append(f"... and {len(successful) - 20} more")
            lines.append("")
        
        if failed:
            lines.append("FAILED EXPORTS:")
            lines.append("-" * 50)
            for f in failed:
                lines.append(f"‚úó {f.get('name')} ({f.get('type')})")
                lines.append(f"  ID: {f.get('id')}")
                lines.append(f"  Error: {f.get('error')}")
                lines.append("")
        
        lines.append("=" * 50)
        lines.append("")
        lines.append("NOTE: Excel files may be larger than CSV equivalents.")
        lines.append("For very large reports (10,000+ rows), CSV format is recommended.")
        
        return "\n".join(lines)   

    def export_selected_reports_to_zip_concurrent_excel(
        self,
        output_zip_path: str,
        report_ids: List[str],
        max_workers: int = 10,
        cancel_event: Optional[Any] = None,
        retry_attempts: int = 3,
        reports_metadata: Optional[Dict[str, Dict]] = None
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to Excel format (.xlsx) in a ZIP file using CONCURRENT downloads.
        
        ‚úÖ NEW METHOD: Excel-specific export with CSV ‚Üí XLSX conversion
        
        Similar to export_selected_reports_to_zip_concurrent but converts each CSV to Excel format.
        
        Args:
            output_zip_path: Path where ZIP file will be saved
            report_ids: List of report IDs to export
            max_workers: Number of parallel downloads (default 10)
            cancel_event: Threading event to signal cancellation
            retry_attempts: Number of retry attempts for failed reports
            reports_metadata: Optional dict of {report_id: {name, format}} to skip metadata fetch
            
        Returns:
            Dictionary with export results
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        # ‚úÖ First, validate Excel dependencies
        is_valid, error_msg = self._validate_excel_dependencies()
        if not is_valid:
            raise Exception(error_msg)
        
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_excel_"))
        
        try:
            # ===== STEP 1: Get/validate metadata =====
            print(f"üìä Preparing to export {len(report_ids)} reports as Excel...")
            
            # Use provided metadata if available, else fetch
            if reports_metadata:
                print("‚ö° Using cached metadata (skipping API calls)")
                reports = []
                for report_id in report_ids:
                    if report_id in reports_metadata:
                        reports.append(reports_metadata[report_id])
                    else:
                        reports.append({
                            "id": report_id,
                            "name": report_id,
                            "reportFormat": "TABULAR"
                        })
            else:
                print("üîç Fetching report metadata...")
                if not report_ids:
                    reports = []
                else:
                    chunk_size = 50
                    reports = []
                    
                    for i in range(0, len(report_ids), chunk_size):
                        if cancel_event and cancel_event.is_set():
                            raise Exception("Export cancelled by user")
                        
                        chunk_ids = report_ids[i:i + chunk_size]
                        ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                        
                        base_query = f"""
                            SELECT Id, Name, Format 
                            FROM Report 
                            WHERE Id IN ({ids_formatted})
                        """
                        
                        try:
                            chunk_records = self._query_with_pagination(
                                base_query.strip(), 
                                batch_size=2000,
                                cancel_event=cancel_event
                            )
                            
                            for record in chunk_records:
                                reports.append({
                                    "id": record.get("Id"),
                                    "name": record.get("Name"),
                                    "reportFormat": record.get("Format", "TABULAR")
                                })
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error fetching report chunk {i//chunk_size + 1}: {str(e)[:100]}")
                            for rid in chunk_ids:
                                reports.append({
                                    "id": rid,
                                    "name": rid,
                                    "reportFormat": "TABULAR"
                                })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []
            used_filenames: Dict[str, int] = {}
            
            # Thread-safe counters
            completed_lock = threading.Lock()
            
            # Adaptive worker count
            if total > 5000:
                max_workers = min(max_workers, 8)
                print(f"‚öôÔ∏è Large export detected ({total} reports), using {max_workers} workers")
            elif total > 1000:
                max_workers = min(max_workers, 10)
                print(f"‚öôÔ∏è Using {max_workers} workers for {total} reports")
            
            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports (Excel)",
                    "api_version": self.api_version,
                    "cancelled": False,
                    "completed": 0
                }
            
            # ===== STEP 2: Define worker function =====
            def export_single_report_excel(report: Dict) -> tuple:
                """Export a single report as Excel - runs in thread pool"""
                nonlocal completed
                
                if cancel_event and cancel_event.is_set():
                    return ("cancelled", report, None)
                
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")
                
                # Notify: Starting download
                if self.progress_callback:
                    try:
                        with completed_lock:
                            current_count = completed
                        self.progress_callback(current_count, total, report_name)
                    except:
                        pass
                
                # Generate filename (thread-safe)
                base_name = safe_filename(report_name)
                
                with completed_lock:
                    if base_name in used_filenames:
                        used_filenames[base_name] += 1
                        filename = f"{base_name}_{used_filenames[base_name]}.xlsx"
                    else:
                        used_filenames[base_name] = 1
                        filename = f"{base_name}.xlsx"
                
                excel_path = tmp_dir / filename
                
                # Retry logic with exponential backoff
                last_error = None
                for attempt in range(retry_attempts):
                    if cancel_event and cancel_event.is_set():
                        return ("cancelled", report, None)
                    
                    try:
                        # Adaptive timeout
                        timeout = 180 if total > 5000 else 120
                        
                        # ‚úÖ Step 1: Get CSV content
                        success, csv_content = self._export_report_excel(report_id, timeout=timeout)
                        
                        if not success:
                            raise Exception(csv_content)  # csv_content contains error message
                        
                        if not csv_content or len(csv_content.strip()) == 0:
                            raise Exception("Empty CSV content received")
                        
                        # ‚úÖ Step 2: Convert CSV to Excel
                        conversion_success = self._csv_to_excel(csv_content, str(excel_path))
                        
                        if not conversion_success:
                            raise Exception("Failed to convert CSV to Excel format")
                        
                        # Verify file was created
                        if not excel_path.exists():
                            raise Exception("Excel file was not created")
                        
                        # Success! Increment counter
                        with completed_lock:
                            completed += 1
                            current_count = completed
                        
                        # Notify: Report completed successfully
                        if self.progress_callback:
                            try:
                                self.progress_callback(current_count, total)
                            except:
                                pass
                        
                        return ("success", report, filename)
                        
                    except Exception as e:
                        last_error = str(e)
                        if attempt < retry_attempts - 1:
                            wait_time = (2 ** attempt) + (attempt * 0.5)
                            time.sleep(wait_time)
                            continue
                        else:
                            break
                
                # Failed after all retries
                error_content = (
                    f"# Failed to export report as Excel after {retry_attempts} attempts\n"
                    f"# Report Name: {report_name}\n"
                    f"# Report ID: {report_id}\n"
                    f"# Report Type: {report_type}\n"
                    f"# Error: {last_error}\n"
                )
                
                # Create error file (as .txt since Excel conversion failed)
                error_path = tmp_dir / f"{base_name}_ERROR.txt"
                error_path.write_text(error_content, encoding="utf-8")
                
                # Increment counter
                with completed_lock:
                    completed += 1
                    current_count = completed
                
                # Notify: Report failed
                if self.progress_callback:
                    try:
                        self.progress_callback(current_count, total)
                    except:
                        pass
                
                return ("failed", report, last_error)
            
            # ===== STEP 3: Export reports concurrently =====
            print(f"üöÄ Starting concurrent Excel export with {max_workers} workers...")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all tasks
                future_to_report = {
                    executor.submit(export_single_report_excel, report): report 
                    for report in reports
                }
                
                # Track progress milestones
                last_milestone = 0
                milestone_interval = max(100, total // 20)
                
                # Process completed tasks
                for future in as_completed(future_to_report):
                    if cancel_event and cancel_event.is_set():
                        print("‚ö†Ô∏è Cancellation detected, stopping remaining downloads...")
                        for f in future_to_report:
                            f.cancel()
                        break
                    
                    try:
                        status, report, data = future.result()
                        
                        if status == "success":
                            successful.append(report.get("name"))
                        elif status == "failed":
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": data
                            })
                        elif status == "cancelled":
                            pass
                        
                        # Log progress milestones
                        if completed - last_milestone >= milestone_interval:
                            success_rate = (len(successful) / completed * 100) if completed > 0 else 0
                            print(f"üìä Progress: {completed}/{total} ({completed/total*100:.1f}%) - Success rate: {success_rate:.1f}%")
                            last_milestone = completed
                        
                        # Update progress callback
                        if self.progress_callback:
                            try:
                                self.progress_callback(completed, total)
                            except Exception:
                                pass
                                
                    except Exception as e:
                        report = future_to_report.get(future)
                        if report:
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": str(e)
                            })
                            print(f"‚ö†Ô∏è Future error for {report.get('name')}: {str(e)[:100]}")
            
            # Check if cancelled
            was_cancelled = cancel_event and cancel_event.is_set()
            
            # ===== STEP 4: Create ZIP file =====
            print(f"üì¶ Creating ZIP file with {completed} Excel reports...")
            
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                # Write files in sorted order
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                # Create Excel-specific summary
                summary = self._create_excel_summary(
                    total, 
                    successful, 
                    failed, 
                    "Selected Reports (Excel)" + (" (CANCELLED)" if was_cancelled else "")
                )
                zf.writestr("_EXPORT_SUMMARY.txt", summary)
            
            # Final statistics
            success_rate = (len(successful) / total * 100) if total > 0 else 0
            print(f"‚úÖ Excel export complete: {len(successful)}/{total} successful ({success_rate:.1f}%)")
            if failed:
                print(f"‚ö†Ô∏è Failed: {len(failed)} reports")
            
            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports (Excel)",
                "api_version": self.api_version,
                "cancelled": was_cancelled,
                "completed": completed
            }
        
        finally:
            # Cleanup temporary files
            try:
                shutil.rmtree(tmp_dir)
                print(f"üßπ Cleaned up temporary files")
            except Exception as e:
                print(f"‚ö†Ô∏è Error cleaning temp directory: {str(e)[:100]}")    
    
    
    
    # exporter.py - Part 3: Export Methods
# This continues the SalesforceReportExporter class

    # Add these methods to the SalesforceReportExporter class

    def export_reports_by_folder_to_zip(
        self,
        output_zip_path: str,
        folder_id: str,
        delay_between_reports: float = 1.0
    ) -> Dict[str, Any]:
        """
        Export all reports from a specific folder to a ZIP file.
        
        Args:
            output_zip_path: Path where ZIP file will be saved
            folder_id: The Salesforce folder ID to export reports from
            delay_between_reports: Seconds to wait between exports (rate limiting)
            
        Returns:
            Dictionary with export results
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # Step 1: Get reports from this folder
            reports = self.list_reports(folder_id=folder_id)
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            # Get folder name
            folder_name = self._get_folder_name(folder_id)

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", f"No reports found in folder: {folder_name}")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": folder_name,
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, folder_name)
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": folder_name,
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass

    def export_all_reports_to_zip(
        self,
        output_zip_path: str,
        delay_between_reports: float = 0.5
    ) -> Dict[str, Any]:
        """
        Export ALL reports from ALL folders to a ZIP file.
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # Step 1: Get list of all reports
            reports = self.list_reports()
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found in this Salesforce org.")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "All Folders",
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, "All Folders")
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "All Folders",
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass
    
    # exporter.py - Part 4: Selected Reports Export & Helper Methods
# This completes the SalesforceReportExporter class

    # Add these final methods to the SalesforceReportExporter class

    def export_selected_reports_to_zip(
        self,
        output_zip_path: str,
        report_ids: List[str],
        delay_between_reports: float = 0.5
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to a ZIP file.
        Now uses direct SOQL to ensure system/automated reports are found.
        """
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))

        try:
            # --- IMPROVED: Direct SOQL lookup with pagination ---
            if not report_ids:
                reports = []
            else:
                # Split into chunks of 100 IDs (SOQL IN clause limit is ~1000 chars)
                chunk_size = 100
                reports = []
                
                for i in range(0, len(report_ids), chunk_size):
                    chunk_ids = report_ids[i:i + chunk_size]
                    ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                    
                    base_query = f"""
                        SELECT Id, Name, Format 
                        FROM Report 
                        WHERE Id IN ({ids_formatted})
                    """
                    
                    try:
                        # Use pagination helper (though with 100 IDs we won't need pagination)
                        chunk_records = self._query_with_pagination(base_query.strip(), batch_size=2000)
                        
                        for record in chunk_records:
                            reports.append({
                                "id": record.get("Id"),
                                "name": record.get("Name"),
                                "reportFormat": record.get("Format", "TABULAR")
                            })
                    except Exception as e:
                        print(f"Error fetching report chunk: {str(e)}")
                        # Fallback: create entries with just IDs
                        for rid in chunk_ids:
                            reports.append({
                                "id": rid,
                                "name": rid,
                                "reportFormat": "TABULAR"
                            })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []

            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports",
                    "api_version": self.api_version
                }

            used_filenames: Dict[str, int] = {}

            # Step 2: Export each selected report
            for report in reports:
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")

                base_name = safe_filename(report_name)
                if base_name in used_filenames:
                    used_filenames[base_name] += 1
                    filename = f"{base_name}_{used_filenames[base_name]}.csv"
                else:
                    used_filenames[base_name] = 1
                    filename = f"{base_name}.csv"

                csv_path = tmp_dir / filename

                try:
                    csv_content = self.export_report_csv(report_id)
                    
                    if not csv_content or len(csv_content.strip()) == 0:
                        raise Exception("Empty response received")
                    
                    first_line = csv_content.split('\n')[0] if csv_content else ""
                    if 'Error' in first_line and len(csv_content) < 500:
                        raise Exception(f"Salesforce error: {first_line[:100]}")
                    
                    csv_path.write_text(csv_content, encoding="utf-8")
                    successful.append(report_name)

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "id": report_id,
                        "name": report_name,
                        "type": report_type,
                        "error": error_msg
                    })
                    error_content = (
                        f"# Failed to export report\n"
                        f"# Report Name: {report_name}\n"
                        f"# Report ID: {report_id}\n"
                        f"# Report Type: {report_type}\n"
                        f"# Error: {error_msg}\n"
                    )
                    csv_path.write_text(error_content, encoding="utf-8")

                completed += 1
                
                if self.progress_callback:
                    try:
                        self.progress_callback(completed, total)
                    except Exception:
                        pass

                if delay_between_reports > 0 and completed < total:
                    time.sleep(delay_between_reports)

            # Step 3: Create ZIP file
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(total, successful, failed, "Selected Reports")
                zf.writestr("_EXPORT_SUMMARY.txt", summary)

            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports",
                "api_version": self.api_version
            }

        finally:
            try:
                shutil.rmtree(tmp_dir)
            except Exception:
                pass
    

    def export_selected_reports_to_zip_concurrent(
        self,
        output_zip_path: str,
        report_ids: List[str],
        max_workers: int = 10,  # ‚úÖ INCREASED: Default 10 workers for faster exports
        cancel_event: Optional[Any] = None,
        retry_attempts: int = 3,
        reports_metadata: Optional[Dict[str, Dict]] = None
    ) -> Dict[str, Any]:
        """
        Export specific selected reports to a ZIP file using CONCURRENT downloads.
        
        ‚úÖ OPTIMIZED: Better memory management + adaptive worker count for 10,000+ reports.
        
        Args:
            output_zip_path: Path where ZIP file will be saved
            report_ids: List of report IDs to export
            max_workers: Number of parallel downloads (default 10)
            cancel_event: Threading event to signal cancellation
            retry_attempts: Number of retry attempts for failed reports
            reports_metadata: Optional dict of {report_id: {name, format}} to skip metadata fetch
            
        Returns:
            Dictionary with export results
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        tmp_dir = Path(tempfile.mkdtemp(prefix="sf_reports_"))
        
        try:
            # ===== STEP 1: Get/validate metadata =====
            print(f"üìä Preparing to export {len(report_ids)} reports...")
            
            # ‚úÖ OPTIMIZED: Use provided metadata if available, else fetch
            if reports_metadata:
                print("‚ö° Using cached metadata (skipping API calls)")
                reports = []
                for report_id in report_ids:
                    if report_id in reports_metadata:
                        reports.append(reports_metadata[report_id])
                    else:
                        # Fallback: create basic entry
                        reports.append({
                            "id": report_id,
                            "name": report_id,
                            "reportFormat": "TABULAR"
                        })
            else:
                print("üîç Fetching report metadata...")
                # Fallback: Fetch metadata if not provided
                if not report_ids:
                    reports = []
                else:
                    # ‚úÖ OPTIMIZED: Smaller chunks for stability
                    chunk_size = 50  # Reduced from 100
                    reports = []
                    
                    for i in range(0, len(report_ids), chunk_size):
                        # Check for cancellation
                        if cancel_event and cancel_event.is_set():
                            raise Exception("Export cancelled by user")
                        
                        chunk_ids = report_ids[i:i + chunk_size]
                        ids_formatted = ",".join([f"'{rid}'" for rid in chunk_ids])
                        
                        base_query = f"""
                            SELECT Id, Name, Format 
                            FROM Report 
                            WHERE Id IN ({ids_formatted})
                        """
                        
                        try:
                            chunk_records = self._query_with_pagination(
                                base_query.strip(), 
                                batch_size=2000,
                                cancel_event=cancel_event
                            )
                            
                            for record in chunk_records:
                                reports.append({
                                    "id": record.get("Id"),
                                    "name": record.get("Name"),
                                    "reportFormat": record.get("Format", "TABULAR")
                                })
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error fetching report chunk {i//chunk_size + 1}: {str(e)[:100]}")
                            # Fallback: create entries with just IDs
                            for rid in chunk_ids:
                                reports.append({
                                    "id": rid,
                                    "name": rid,
                                    "reportFormat": "TABULAR"
                                })
            
            total = len(reports)
            completed = 0
            failed: List[Dict[str, Any]] = []
            successful: List[str] = []
            used_filenames: Dict[str, int] = {}
            
            # Thread-safe counters
            completed_lock = threading.Lock()
            
            # ‚úÖ NEW: Adaptive worker count based on total reports
            if total > 5000:
                max_workers = min(max_workers, 8)  # Reduce workers for very large exports
                print(f"‚öôÔ∏è Large export detected ({total} reports), using {max_workers} workers")
            elif total > 1000:
                max_workers = min(max_workers, 10)
                print(f"‚öôÔ∏è Using {max_workers} workers for {total} reports")
            
            if total == 0:
                with zipfile.ZipFile(output_zip_path, "w") as zf:
                    zf.writestr("_README.txt", "No reports found with the selected IDs")
                return {
                    "zip": output_zip_path,
                    "total": 0,
                    "failed": [],
                    "successful": [],
                    "folder_name": "Selected Reports",
                    "api_version": self.api_version,
                    "cancelled": False,
                    "completed": 0
                }
            
            # ===== STEP 2: Define worker function =====
            def export_single_report(report: Dict) -> tuple:
                """Export a single report - runs in thread pool"""
                nonlocal completed
                
                # Check for cancellation
                if cancel_event and cancel_event.is_set():
                    return ("cancelled", report, None)
                
                report_id = report.get("id")
                report_name = report.get("name") or report_id
                report_type = report.get("reportFormat", "TABULAR")
                
                # ‚úÖ NOTIFY: Starting download of this report
                if self.progress_callback:
                    try:
                        with completed_lock:
                            current_count = completed
                        # Signal: download starting (with report name)
                        self.progress_callback(current_count, total, report_name)
                    except:
                        pass
                
                # Generate filename (thread-safe)
                base_name = safe_filename(report_name)
                
                with completed_lock:
                    if base_name in used_filenames:
                        used_filenames[base_name] += 1
                        filename = f"{base_name}_{used_filenames[base_name]}.csv"
                    else:
                        used_filenames[base_name] = 1
                        filename = f"{base_name}.csv"
                
                csv_path = tmp_dir / filename
                
                # Retry logic with exponential backoff
                last_error = None
                for attempt in range(retry_attempts):
                    # Check cancellation before each attempt
                    if cancel_event and cancel_event.is_set():
                        return ("cancelled", report, None)
                    
                    try:
                        # ‚úÖ IMPROVED: Adaptive timeout based on total export size
                        timeout = 180 if total > 5000 else 120
                        
                        csv_content = self.export_report_csv(report_id, timeout=timeout)
                        
                        if not csv_content or len(csv_content.strip()) == 0:
                            raise Exception("Empty response received")
                        
                        first_line = csv_content.split('\n')[0] if csv_content else ""
                        if 'Error' in first_line and len(csv_content) < 500:
                            raise Exception(f"Salesforce error: {first_line[:100]}")
                        
                        csv_path.write_text(csv_content, encoding="utf-8")
                        
                        # Success! Increment counter FIRST
                        with completed_lock:
                            completed += 1
                            current_count = completed
                        
                        # ‚úÖ NOTIFY: Report completed successfully
                        if self.progress_callback:
                            try:
                                self.progress_callback(current_count, total)
                            except:
                                pass
                        
                        return ("success", report, filename)
                        
                    except Exception as e:
                        last_error = str(e)
                        if attempt < retry_attempts - 1:
                            # ‚úÖ IMPROVED: Exponential backoff with jitter
                            wait_time = (2 ** attempt) + (attempt * 0.5)  # 1s, 2.5s, 5s
                            time.sleep(wait_time)
                            continue
                        else:
                            # All retries failed
                            break
                
                # Failed after all retries
                error_content = (
                    f"# Failed to export report after {retry_attempts} attempts\n"
                    f"# Report Name: {report_name}\n"
                    f"# Report ID: {report_id}\n"
                    f"# Report Type: {report_type}\n"
                    f"# Error: {last_error}\n"
                )
                csv_path.write_text(error_content, encoding="utf-8")

                # Increment counter FIRST
                with completed_lock:
                    completed += 1
                    current_count = completed

                # ‚úÖ NOTIFY: Report failed (but counted as completed)
                if self.progress_callback:
                    try:
                        self.progress_callback(current_count, total)
                    except:
                        pass

                return ("failed", report, last_error)
            
            # ===== STEP 3: Export reports concurrently =====
            print(f"üöÄ Starting concurrent export with {max_workers} workers...")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all tasks
                future_to_report = {
                    executor.submit(export_single_report, report): report 
                    for report in reports
                }
                
                # ‚úÖ NEW: Track progress milestones
                last_milestone = 0
                milestone_interval = max(100, total // 20)  # Log every 5%
                
                # Process completed tasks
                for future in as_completed(future_to_report):
                    # Check for cancellation
                    if cancel_event and cancel_event.is_set():
                        print("‚ö†Ô∏è Cancellation detected, stopping remaining downloads...")
                        # Cancel remaining futures
                        for f in future_to_report:
                            f.cancel()
                        break
                    
                    try:
                        status, report, data = future.result()
                        
                        if status == "success":
                            successful.append(report.get("name"))
                        elif status == "failed":
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": data
                            })
                        elif status == "cancelled":
                            # Don't count as failed, just stopped
                            pass
                        
                        # ‚úÖ NEW: Log progress milestones
                        if completed - last_milestone >= milestone_interval:
                            success_rate = (len(successful) / completed * 100) if completed > 0 else 0
                            print(f"üìä Progress: {completed}/{total} ({completed/total*100:.1f}%) - Success rate: {success_rate:.1f}%")
                            last_milestone = completed
                        
                        # Update progress callback
                        if self.progress_callback:
                            try:
                                self.progress_callback(completed, total)
                            except Exception:
                                pass
                                
                    except Exception as e:
                        # Future itself failed
                        report = future_to_report.get(future)
                        if report:
                            failed.append({
                                "id": report.get("id"),
                                "name": report.get("name"),
                                "type": report.get("reportFormat", "TABULAR"),
                                "error": str(e)
                            })
                            print(f"‚ö†Ô∏è Future error for {report.get('name')}: {str(e)[:100]}")
            
            # Check if cancelled
            was_cancelled = cancel_event and cancel_event.is_set()
            
            # ===== STEP 4: Create ZIP file =====
            print(f"üì¶ Creating ZIP file with {completed} reports...")
            
            with zipfile.ZipFile(output_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                # ‚úÖ OPTIMIZED: Write files in sorted order for consistent ZIP structure
                for file_path in sorted(tmp_dir.iterdir()):
                    if file_path.is_file():
                        zf.write(file_path, arcname=file_path.name)
                
                summary = self._create_summary(
                    total, 
                    successful, 
                    failed, 
                    "Selected Reports" + (" (CANCELLED)" if was_cancelled else "")
                )
                zf.writestr("_EXPORT_SUMMARY.txt", summary)
            
            # ‚úÖ NEW: Final statistics
            success_rate = (len(successful) / total * 100) if total > 0 else 0
            print(f"‚úÖ Export complete: {len(successful)}/{total} successful ({success_rate:.1f}%)")
            if failed:
                print(f"‚ö†Ô∏è Failed: {len(failed)} reports")
            
            return {
                "zip": output_zip_path,
                "total": total,
                "failed": failed,
                "successful": successful,
                "folder_name": "Selected Reports",
                "api_version": self.api_version,
                "cancelled": was_cancelled,
                "completed": completed
            }
        
        finally:
            # ‚úÖ IMPROVED: Better cleanup with error handling
            try:
                shutil.rmtree(tmp_dir)
                print(f"üßπ Cleaned up temporary files")
            except Exception as e:
                print(f"‚ö†Ô∏è Error cleaning temp directory: {str(e)[:100]}")


    def _get_folder_name(self, folder_id: str) -> str:
        """Get the name of a folder by its ID"""
        try:
            url = f"{self.instance_url}/services/data/{self.api_version}/sobjects/Folder/{folder_id}"
            response = retry_request(url, headers=self.api_headers, timeout=30)
            data = response.json()
            return data.get("Name", folder_id)
        except:
            return folder_id

    def _create_summary(
        self,
        total: int,
        successful: List[str],
        failed: List[Dict[str, Any]],
        folder_name: str = "Unknown"
    ) -> str:
        """Create a summary text file for the export."""
        lines = [
            "SALESFORCE REPORT EXPORT SUMMARY",
            "=" * 40,
            f"Export Date: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"Instance: {self.instance_url}",
            f"API Version: {self.api_version}",
            f"Folder: {folder_name}",
            "",
            f"Total Reports: {total}",
            f"Successful: {len(successful)}",
            f"Failed: {len(failed)}",
            "",
        ]
        
        if failed:
            lines.append("FAILED REPORTS:")
            lines.append("-" * 40)
            for f in failed:
                lines.append(f"‚Ä¢ {f.get('name')} ({f.get('type')})")
                lines.append(f"  ID: {f.get('id')}")
                lines.append(f"  Error: {f.get('error')}")
                lines.append("")
        
        return "\n".join(lines)
    
    